{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from argparse import ArgumentParser\n",
    "from tensorboardX import SummaryWriter\n",
    "import sys\n",
    "from models.factorizer import setup_factorizer\n",
    "from data_loader.data_loader import setup_generator\n",
    "from utils.evaluate import evaluate_fm\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "torch.backends.cudnn.enabled = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def setup_args(parser=None):\n",
    "    \"\"\" Set up arguments for the Engine\n",
    "\n",
    "    return:\n",
    "        python dictionary\n",
    "    \"\"\"\n",
    "    if parser is None:\n",
    "        parser = ArgumentParser()\n",
    "    data = parser.add_argument_group('Data')\n",
    "    engine = parser.add_argument_group('Engine Arguments')\n",
    "    factorize = parser.add_argument_group('Factorizer Arguments')\n",
    "    matrix_factorize = parser.add_argument_group('MF Arguments')\n",
    "    regularize = parser.add_argument_group('Regularizer Arguments')\n",
    "    log = parser.add_argument_group('Tensorboard Arguments')\n",
    "\n",
    "    engine.add_argument('--alias', default='experiment',\n",
    "                        help='Name for the experiment')\n",
    "    engine.add_argument('--seed', default='42')\n",
    "\n",
    "    data.add_argument('--data-type', default='ml1m', help='type of the dataset')\n",
    "    data.add_argument('--data-path', default='./data/{data_type}/')\n",
    "    data.add_argument('--train_test-freq-bd', help='split the data freq-wise, bound of the user freq')\n",
    "    data.add_argument('--train-valid-freq-bd', help='split the data freq-wise, bound of the user freq')\n",
    "    data.add_argument('--batch-size-train', default=1)\n",
    "    data.add_argument('--batch-size-valid', default=1)\n",
    "    data.add_argument('--batch-size-test', default=1)\n",
    "    data.add_argument('--device-ids-test', default=[0], help='devices used for multi-processing evaluate')\n",
    "\n",
    "    regularize.add_argument('--max-steps', default=1e5)\n",
    "    regularize.add_argument('--max-epochs', default=50)\n",
    "    regularize.add_argument('--use-cuda', default=True)\n",
    "    regularize.add_argument('--device-id', default=0, help='Training Devices')\n",
    "\n",
    "    factorize.add_argument('--factorizer', default='fm', help='Type of the Factorization Model')\n",
    "    factorize.add_argument('--latent-dim', default=8)\n",
    "\n",
    "    type_opt = 'fm'\n",
    "    matrix_factorize.add_argument('--{}-optimizer'.format(type_opt), default='sgd')\n",
    "    matrix_factorize.add_argument('--{}-lr'.format(type_opt), default=1e-3)\n",
    "    matrix_factorize.add_argument('--{}-grad-clip'.format(type_opt), default=1)\n",
    "\n",
    "    log.add_argument('--log-interval', default=1)\n",
    "    log.add_argument('--tensorboard', default='./tmp/runs')\n",
    "    log.add_argument('--early_stop', default=None)\n",
    "    log.add_argument('--display_interval', default=100)\n",
    "    return parser\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Engine(object):\n",
    "    \"\"\"Engine wrapping the training & evaluation\n",
    "       of adpative regularized maxtirx factorization\n",
    "    \"\"\"\n",
    "    _global_writer = None  # Class variable for the global writer\n",
    "    _param_step = 0  # Class variable to track parameter steps\n",
    "\n",
    "\n",
    "    def __init__(self, opt):\n",
    "        self._opt = opt\n",
    "        self._opt['data_path'] = self._opt['data_path'].format(data_type=self._opt['data_type'])\n",
    "        self._sampler = setup_generator(opt)\n",
    "\n",
    "        self._opt['field_dims'] = self._sampler.field_dims\n",
    "\n",
    "        if Engine._global_writer is None:\n",
    "            Engine._global_writer = SummaryWriter(log_dir='{}/parameter_comparison'.format(self._opt['tensorboard']))\n",
    "        \n",
    "\n",
    "        self._opt['emb_save_path'] = self._opt['emb_save_path'].format(\n",
    "            factorizer=self._opt['factorizer'],\n",
    "            data_type=self._opt['data_type'],\n",
    "            alias=self._opt['alias'],\n",
    "            num_parameter='{num_parameter}'\n",
    "        )\n",
    "        if 'retrain_emb_param' in opt:\n",
    "            self.retrain = True\n",
    "            if opt['re_init']:\n",
    "                self._opt['alias'] += '_reinitTrue'\n",
    "            else:\n",
    "                self._opt['alias'] += '_reinitFalse'\n",
    "            self._opt['alias'] += '_retrain_emb_param{}'.format(opt['retrain_emb_param'])\n",
    "        else:\n",
    "            self.retrain = False\n",
    "            self.candidate_p = self._opt.get('candidate_p')\n",
    "        self._opt['eval_res_path'] = self._opt['eval_res_path'].format(\n",
    "            factorizer=self._opt['factorizer'],\n",
    "            data_type=self._opt['data_type'],\n",
    "            alias=self._opt['alias'],\n",
    "            epoch_idx='{epoch_idx}'\n",
    "        )\n",
    "        self._factorizer = setup_factorizer(opt)\n",
    "        self._opt['tensorboard'] = self._opt['tensorboard'].format(\n",
    "            factorizer=self._opt['factorizer'],\n",
    "            data_type=self._opt['data_type'],\n",
    "        )\n",
    "        self._writer = SummaryWriter(log_dir='{}/{}'.format(self._opt['tensorboard'], opt['alias']))\n",
    "        self._writer.add_text('option', str(opt), 0)\n",
    "        self._mode = None\n",
    "        self.early_stop = self._opt.get('early_stop')\n",
    "\n",
    "\n",
    "    @property\n",
    "    def mode(self):\n",
    "        return self._mode\n",
    "\n",
    "    @mode.setter\n",
    "    def mode(self, new_mode):\n",
    "        assert new_mode in ['complete', 'partial', None]  # training a complete trajectory or a partial trajctory\n",
    "        self._mode = new_mode\n",
    "\n",
    "    def save_pruned_embedding(self, param, step_idx):\n",
    "        max_candidate_p = max(self.candidate_p)\n",
    "        if max_candidate_p == 0:\n",
    "\n",
    "            print(\"Minimal target parameters achieved, stop pruning.\")\n",
    "            return 0\n",
    "\n",
    "        else:\n",
    "            if param <= max_candidate_p:\n",
    "                embedding = self._factorizer.model.get_embedding()\n",
    "                emb_save_path = self._opt['emb_save_path'].format(num_parameter=param)\n",
    "                emb_save_dir, _ = os.path.split(emb_save_path)\n",
    "                if not os.path.exists(emb_save_dir):\n",
    "                    os.makedirs(emb_save_dir)\n",
    "                np.save(emb_save_path, embedding)\n",
    "                max_idx = self.candidate_p.index(max(self.candidate_p))\n",
    "                self.candidate_p[max_idx] = 0\n",
    "                print(\"*\" * 80)\n",
    "                print(\"Reach the target parameter: {}, save embedding with size: {}\".format(max_candidate_p, param))\n",
    "                print(\"*\" * 80)\n",
    "            elif step_idx == 0:\n",
    "                embedding = self._factorizer.model.get_embedding()\n",
    "                emb_save_path = self._opt['emb_save_path'].format(num_parameter='initial_embedding')\n",
    "                emb_save_dir, _ = os.path.split(emb_save_path)\n",
    "                if not os.path.exists(emb_save_dir):\n",
    "                    os.makedirs(emb_save_dir)\n",
    "                np.save(emb_save_path, embedding)\n",
    "                print(\"*\" * 80)\n",
    "                print(\"Save the initial embedding table\")\n",
    "                print(\"*\" * 80)\n",
    "        return 1\n",
    "    \n",
    "    def train_an_episode(self, max_steps, max_epochs, episode_idx=''):\n",
    "        \"\"\"Train a feature_based recommendation model\"\"\"\n",
    "        assert self.mode in ['partial', 'complete']\n",
    "\n",
    "        print('-' * 80)\n",
    "        print('[{} episode {} starts!]'.format(self.mode, episode_idx))\n",
    "        print('Initializing ...')\n",
    "        self._factorizer.init_episode()\n",
    "\n",
    "        log_interval = self._opt.get('log_interval')\n",
    "        eval_interval = self._opt.get('eval_interval')\n",
    "        display_interval = self._opt.get('display_interval')\n",
    "\n",
    "        status = dict()\n",
    "        flag, test_flag, valid_flag = 0, 0, 0\n",
    "        valid_mf_loss, train_mf_loss = np.inf, np.inf\n",
    "        best_valid_result = {\"AUC\": [0, 0], \"LogLoss\": [np.inf, 0]}\n",
    "        best_test_result = {\"AUC\": [0, 0], \"LogLoss\": [np.inf, 0]}\n",
    "        epoch_start = datetime.now()\n",
    "        for step_idx in range(int(max_steps)):\n",
    "            # Prepare status for current step\n",
    "            status['done'] = False\n",
    "            status['sampler'] = self._sampler\n",
    "            train_mf_loss = self._factorizer.update(self._sampler)\n",
    "            status['train_mf_loss'] = train_mf_loss\n",
    "\n",
    "            # Logging & Evaluate on the Evaluate Set\n",
    "            if self.mode == 'complete' and step_idx % log_interval == 0:\n",
    "                epoch_idx = int(step_idx / self._sampler.num_batches_train)\n",
    "                if epoch_idx > max_epochs and self.retrain:\n",
    "                    return best_test_result\n",
    "\n",
    "                sparsity, params = self._factorizer.model.calc_sparsity()\n",
    "                if not self.retrain:\n",
    "                    returnflag = self.save_pruned_embedding(params, step_idx)\n",
    "                    if not returnflag:\n",
    "                        return best_test_result\n",
    "                    \n",
    "                self._writer.add_scalar('train/step_wise/mf_loss', train_mf_loss, step_idx)\n",
    "                self._writer.add_scalar('train/step_wise/sparsity', sparsity, step_idx)\n",
    "\n",
    "                if step_idx % display_interval == 0:\n",
    "                    print('[Epoch {}|Step {}|Flag {}|Sparsity {:.4f}|Params {}]'.format(epoch_idx,\n",
    "                                                                                        step_idx % self._sampler.num_batches_train,\n",
    "                                                                                        flag, sparsity, params))\n",
    "\n",
    "                if step_idx % self._sampler.num_batches_train == 0:\n",
    "                    threshold = self._factorizer.model.get_threshold()\n",
    "\n",
    "                    self._writer.add_histogram('threshold/epoch_wise/threshold', threshold, epoch_idx)\n",
    "                    self._writer.add_scalar('train/epoch_wise/sparsity', sparsity, epoch_idx)\n",
    "                    self._writer.add_scalar('train/epoch_wise/params', params, epoch_idx)\n",
    "\n",
    "                if (step_idx % self._sampler.num_batches_train == 0) and (epoch_idx % eval_interval == 0) and self.retrain:\n",
    "                    print('Evaluate on test ...')\n",
    "                    start = datetime.now()\n",
    "                    eval_res_path = self._opt['eval_res_path'].format(epoch_idx=epoch_idx)\n",
    "                    eval_res_dir, _ = os.path.split(eval_res_path)\n",
    "                    if not os.path.exists(eval_res_dir):\n",
    "                        os.makedirs(eval_res_dir)\n",
    "\n",
    "                    use_cuda = self._opt['use_cuda']\n",
    "                    logloss, auc = evaluate_fm(self._factorizer, self._sampler, use_cuda)\n",
    "                    self._writer.add_scalar('test/epoch_wise/metron_auc', auc, epoch_idx)\n",
    "                    self._writer.add_scalar('test/epoch_wise/metron_logloss', logloss, epoch_idx)\n",
    "                    if logloss < best_test_result['LogLoss'][0]:\n",
    "                        best_test_result['LogLoss'][0] = logloss\n",
    "                        best_test_result['LogLoss'][1] = epoch_idx\n",
    "                    if auc > best_test_result['AUC'][0]:\n",
    "                        best_test_result['AUC'][0] = auc\n",
    "                        best_test_result['AUC'][1] = epoch_idx\n",
    "                        test_flag = 0\n",
    "                    else:\n",
    "                        test_flag += 1\n",
    "                    pd.Series(best_test_result).to_csv(eval_res_path)\n",
    "                    print(\"*\" * 80)\n",
    "                    print(\"Test AUC: {:4f} | Logloss: {:4f}\".format(auc, logloss))\n",
    "                    end = datetime.now()\n",
    "                    print('Evaluate Time {} minutes'.format((end - start).total_seconds() / 60))\n",
    "                    epoch_end = datetime.now()\n",
    "                    dur = (epoch_end - epoch_start).total_seconds() / 60\n",
    "                    epoch_start = datetime.now()\n",
    "                    print('[Epoch {:4d}] train MF loss: {:04.8f}, '\n",
    "                          'valid loss: {:04.8f}, time {:04.8f} minutes'.format(epoch_idx,\n",
    "                                                                               train_mf_loss,\n",
    "                                                                               valid_mf_loss,\n",
    "                                                                               dur))\n",
    "                    print(\"*\"*80)\n",
    "                    \n",
    "            flag = test_flag\n",
    "            if self.early_stop is not None and flag >= self.early_stop :\n",
    "                print(\"Early stop training process\")\n",
    "                print(\"Best performance on test data: \", best_test_result)\n",
    "                print(\"Best performance on valid data: \", best_valid_result)\n",
    "                self._writer.add_text('best_valid_result', str(best_valid_result), 0)\n",
    "                self._writer.add_text('best_test_result', str(best_test_result), 0)\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\")\n",
    "                    return best_test_result\n",
    "                \n",
    "            \n",
    "            \n",
    "    def train_finish(self, best_test_result):\n",
    "        Engine._global_writer.add_scalar(\n",
    "            'parameter_comparison/best_auc_vs_params', \n",
    "            best_test_result['AUC'][0],\n",
    "            Engine._param_step\n",
    "        )\n",
    "        Engine._param_step += 1  # Increment the step counter\n",
    "        Engine._global_writer.flush()\n",
    "\n",
    "    def __del__(self):\n",
    "        if hasattr(self, '_writer'):\n",
    "            self._writer.close()\n",
    "        if Engine._global_writer is not None:  # Close global writer if exists\n",
    "            Engine._global_writer.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        self.mode = 'complete'\n",
    "        best_test_result = self.train_an_episode(self._opt['max_steps'],self._opt['max_epochs'])\n",
    "        return best_test_result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train FM, with MovieLense Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FM_train_BaseDim32_bsz1024_lr_0.001_optim_adam_thresholdTypeFEATURE_DIM_thres_init-15_sigmoid-1_l2_penalty0\n",
      "Reconstructing ml-1m data from ./data/ml-1m/\n",
      "\tNum of train records: 591209\n",
      "\tNum of valid records: 73901\n",
      "\tNum of test records: 73902\n",
      "\tNum of fields: 7\n",
      "\tNum of features: 4129\n",
      "BackBone Embedding Parameters:  132128\n",
      "--------------------------------------------------------------------------------\n",
      "[complete episode  starts!]\n",
      "Initializing ...\n",
      "BackBone Embedding Parameters:  132128\n",
      "********************************************************************************\n",
      "Save the initial embedding table\n",
      "********************************************************************************\n",
      "[Epoch 0|Step 0|Flag 1|Sparsity 0.0000|Params 132128]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pytorchEnv\\env\\Lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0|Step 500|Flag 1|Sparsity 0.0055|Params 131406]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 96\u001b[0m\n\u001b[0;32m     94\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmanual_seed_all(opt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     95\u001b[0m engine \u001b[38;5;241m=\u001b[39m Engine(opt)\n\u001b[1;32m---> 96\u001b[0m best_result \u001b[38;5;241m=\u001b[39m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m , best_result, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[1;32mIn[15], line 217\u001b[0m, in \u001b[0;36mEngine.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomplete\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 217\u001b[0m     best_test_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_an_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_opt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_steps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_opt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_epochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m best_test_result\n",
      "Cell \u001b[1;32mIn[15], line 117\u001b[0m, in \u001b[0;36mEngine.train_an_episode\u001b[1;34m(self, max_steps, max_epochs, episode_idx)\u001b[0m\n\u001b[0;32m    115\u001b[0m status[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdone\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    116\u001b[0m status[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msampler\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler\n\u001b[1;32m--> 117\u001b[0m train_mf_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_factorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sampler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m status[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_mf_loss\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m train_mf_loss\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# Logging & Evaluate on the Evaluate Set\u001b[39;00m\n",
      "File \u001b[1;32md:\\pytorchEnv\\Advertising-Using-a-Recommender-System-with-Learnable-Embedding-Size-CISC867\\learnable-embed-sizes-for-RecSys\\models\\factorizer.py:131\u001b[0m, in \u001b[0;36mFMFactorizer.update\u001b[1;34m(self, sampler)\u001b[0m\n\u001b[0;32m    129\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip)\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "parser = setup_args()\n",
    "parser.set_defaults(\n",
    "    alias='train',\n",
    "    tensorboard='./tmp/runs/{factorizer}/{data_type}',\n",
    "    ##########\n",
    "    ## data ##\n",
    "    ##########\n",
    "    data_type='ml-1m',\n",
    "    data_path='./data/{data_type}/',\n",
    "    load_in_queue=False,\n",
    "    category_only=False,\n",
    "    rebuild_cache=False,\n",
    "    eval_res_path='./tmp/res/{factorizer}/{data_type}/{alias}/{epoch_idx}.csv',\n",
    "    emb_save_path='./tmp/embedding/{factorizer}/{data_type}/{alias}/{num_parameter}',\n",
    "    ######################\n",
    "    ## train/test split ##\n",
    "    ######################\n",
    "    test_ratio=0.1,\n",
    "    valid_ratio=1/9,\n",
    "    ##########################\n",
    "    ## Devices & Efficiency ##\n",
    "    ##########################\n",
    "    use_cuda=True,\n",
    "    early_stop=10,\n",
    "    log_interval=1,\n",
    "    display_interval=500,\n",
    "    eval_interval=5,  # 10 epochs between 2 evaluations\n",
    "    device_ids_test=[0],\n",
    "    device_id=0,\n",
    "    batch_size_train=1024,\n",
    "    batch_size_valid=1024,\n",
    "    batch_size_test=1024,\n",
    "    ###########\n",
    "    ## Model ##\n",
    "    ###########\n",
    "    factorizer='fm',\n",
    "    model='fm',\n",
    "    fm_lr=1e-3,\n",
    "    # Deep\n",
    "    mlp_dims=[100, 100],\n",
    "    # AutoInt\n",
    "    has_residual=True,\n",
    "    full_part=True,\n",
    "    num_heads=2,\n",
    "    num_layers=3,\n",
    "    att_dropout=0.4,\n",
    "    atten_embed_dim=64,\n",
    "    # optimizer setting\n",
    "    fm_optimizer='adam',\n",
    "    fm_amsgrad=False,\n",
    "    fm_eps=1e-8,\n",
    "    fm_l2_regularization=1e-5,\n",
    "    fm_betas=(0.9, 0.999),\n",
    "    fm_grad_clip=100,  # 0.1\n",
    "    fm_lr_exp_decay=1,\n",
    "    l2_penalty=0,\n",
    "    #########\n",
    "    ## Embeddings//PEP ##\n",
    "    #########\n",
    "    latent_dim=32,\n",
    "    threshold_type='feature_dim',\n",
    "    g_type='sigmoid',\n",
    "    gk=1,\n",
    "    threshold_init=-15,\n",
    "    candidate_p=[50000, 30000, 20000],\n",
    ")\n",
    "\n",
    "opt = parser.parse_args(args=[])\n",
    "opt = vars(opt)\n",
    "\n",
    "# rename alias\n",
    "\n",
    "opt['alias'] = '{}_{}_BaseDim{}_bsz{}_lr_{}_optim_{}_thresholdType{}_thres_init{}_{}-{}_l2_penalty{}'.format(\n",
    "    opt['model'].upper(),\n",
    "    opt['alias'],\n",
    "    opt['latent_dim'],\n",
    "    opt['batch_size_train'],\n",
    "    opt['fm_lr'],\n",
    "    opt['fm_optimizer'],\n",
    "    opt['threshold_type'].upper(),\n",
    "    opt['threshold_init'],\n",
    "    opt['g_type'],\n",
    "    opt['gk'],\n",
    "    opt['l2_penalty']\n",
    ")\n",
    "print(opt['alias'])\n",
    "random.seed(opt['seed'])\n",
    "# np.random.seed(opt['seed'])\n",
    "torch.manual_seed(opt['seed'])\n",
    "torch.cuda.manual_seed_all(opt['seed'])\n",
    "engine = Engine(opt)\n",
    "best_result = engine.train()\n",
    "print(\"*\"*10 , best_result, \"*\"*10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrain FM, with MovieLense Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrain for target parameters 20000\n",
      "FM_test_BaseDim32_bsz1024_lr_0.001_optim_adam_thresholdTypeFEATURE_DIM_thres_init-15_sigmoid-1_l2_penalty0\n",
      "Reconstructing ml-1m data from ./data/ml-1m/\n",
      "\tNum of train records: 591209\n",
      "\tNum of valid records: 73901\n",
      "\tNum of test records: 73902\n",
      "\tNum of fields: 7\n",
      "\tNum of features: 4129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pytorchEnv\\env\\Lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrain epoch 20000\n",
      "BackBone Embedding Parameters:  132128\n",
      "--------------------------------------------------------------------------------\n",
      "[complete episode  starts!]\n",
      "Initializing ...\n",
      "Retrain epoch 20000\n",
      "BackBone Embedding Parameters:  132128\n",
      "[Epoch 0|Step 0|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.496777 | Logloss: 1.962264\n",
      "Evaluate Time 0.013823733333333334 minutes\n",
      "[Epoch    0] train MF loss: 1.87465298, valid loss: 0inf, time 0.01501175 minutes\n",
      "********************************************************************************\n",
      "[Epoch 0|Step 500|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 1|Step 422|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 2|Step 344|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 3|Step 266|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 4|Step 188|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.818861 | Logloss: 0.403015\n",
      "Evaluate Time 0.013415183333333334 minutes\n",
      "[Epoch    5] train MF loss: 0.41531336, valid loss: 0inf, time 0.94925683 minutes\n",
      "********************************************************************************\n",
      "[Epoch 5|Step 110|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 6|Step 32|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 6|Step 532|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 7|Step 454|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 8|Step 376|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 9|Step 298|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.841128 | Logloss: 0.380366\n",
      "Evaluate Time 0.015863966666666666 minutes\n",
      "[Epoch   10] train MF loss: 0.34765178, valid loss: 0inf, time 1.06403458 minutes\n",
      "********************************************************************************\n",
      "[Epoch 10|Step 220|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 11|Step 142|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 12|Step 64|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 12|Step 564|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 13|Step 486|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 14|Step 408|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.845959 | Logloss: 0.375316\n",
      "Evaluate Time 0.014792516666666667 minutes\n",
      "[Epoch   15] train MF loss: 0.34999701, valid loss: 0inf, time 1.07085190 minutes\n",
      "********************************************************************************\n",
      "[Epoch 15|Step 330|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 16|Step 252|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 17|Step 174|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 18|Step 96|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 19|Step 18|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 19|Step 518|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.847291 | Logloss: 0.374195\n",
      "Evaluate Time 0.014191 minutes\n",
      "[Epoch   20] train MF loss: 0.35430869, valid loss: 0inf, time 0.95051292 minutes\n",
      "********************************************************************************\n",
      "[Epoch 20|Step 440|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 21|Step 362|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 22|Step 284|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 23|Step 206|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 24|Step 128|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.847571 | Logloss: 0.374230\n",
      "Evaluate Time 0.01370475 minutes\n",
      "[Epoch   25] train MF loss: 0.33978865, valid loss: 0inf, time 0.97359867 minutes\n",
      "********************************************************************************\n",
      "[Epoch 25|Step 50|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 25|Step 550|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 26|Step 472|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 27|Step 394|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 28|Step 316|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 29|Step 238|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.847755 | Logloss: 0.374330\n",
      "Evaluate Time 0.010761650000000001 minutes\n",
      "[Epoch   30] train MF loss: 0.35165069, valid loss: 0inf, time 0.86641148 minutes\n",
      "********************************************************************************\n",
      "[Epoch 30|Step 160|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 31|Step 82|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 32|Step 4|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 32|Step 504|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 33|Step 426|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 34|Step 348|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.847852 | Logloss: 0.374272\n",
      "Evaluate Time 0.01061085 minutes\n",
      "[Epoch   35] train MF loss: 0.34577879, valid loss: 0inf, time 0.75269925 minutes\n",
      "********************************************************************************\n",
      "[Epoch 35|Step 270|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 36|Step 192|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 37|Step 114|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 38|Step 36|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 38|Step 536|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 39|Step 458|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.847653 | Logloss: 0.374568\n",
      "Evaluate Time 0.010679949999999999 minutes\n",
      "[Epoch   40] train MF loss: 0.32149148, valid loss: 0inf, time 0.74292642 minutes\n",
      "********************************************************************************\n",
      "[Epoch 40|Step 380|Flag 1|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 41|Step 302|Flag 1|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 42|Step 224|Flag 1|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 43|Step 146|Flag 1|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 44|Step 68|Flag 1|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 44|Step 568|Flag 1|Sparsity 0.8486|Params 20000]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.847802 | Logloss: 0.374624\n",
      "Evaluate Time 0.010900933333333333 minutes\n",
      "[Epoch   45] train MF loss: 0.34094238, valid loss: 0inf, time 0.72564892 minutes\n",
      "********************************************************************************\n",
      "[Epoch 45|Step 490|Flag 2|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 46|Step 412|Flag 2|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 47|Step 334|Flag 2|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 48|Step 256|Flag 2|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 49|Step 178|Flag 2|Sparsity 0.8486|Params 20000]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.847411 | Logloss: 0.374858\n",
      "Evaluate Time 0.010922049999999999 minutes\n",
      "[Epoch   50] train MF loss: 0.33687541, valid loss: 0inf, time 0.72610535 minutes\n",
      "********************************************************************************\n",
      "[Epoch 50|Step 100|Flag 3|Sparsity 0.8486|Params 20000]\n",
      "retrain for target parameters 29997\n",
      "FM_test_BaseDim32_bsz1024_lr_0.001_optim_adam_thresholdTypeFEATURE_DIM_thres_init-15_sigmoid-1_l2_penalty0\n",
      "Reconstructing ml-1m data from ./data/ml-1m/\n",
      "\tNum of train records: 591209\n",
      "\tNum of valid records: 73901\n",
      "\tNum of test records: 73902\n",
      "\tNum of fields: 7\n",
      "\tNum of features: 4129\n",
      "Retrain epoch 29997\n",
      "BackBone Embedding Parameters:  132128\n",
      "--------------------------------------------------------------------------------\n",
      "[complete episode  starts!]\n",
      "Initializing ...\n",
      "Retrain epoch 29997\n",
      "BackBone Embedding Parameters:  132128\n",
      "[Epoch 0|Step 0|Flag 0|Sparsity 0.7730|Params 29997]\n",
      "Evaluate on test ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pytorchEnv\\env\\Lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "Test AUC: 0.496770 | Logloss: 1.962529\n",
      "Evaluate Time 0.01324875 minutes\n",
      "[Epoch    0] train MF loss: 1.87501597, valid loss: 0inf, time 0.01437150 minutes\n",
      "********************************************************************************\n",
      "[Epoch 0|Step 500|Flag 0|Sparsity 0.7730|Params 29997]\n",
      "[Epoch 1|Step 422|Flag 0|Sparsity 0.7730|Params 29997]\n",
      "[Epoch 2|Step 344|Flag 0|Sparsity 0.7730|Params 29997]\n",
      "[Epoch 3|Step 266|Flag 0|Sparsity 0.7730|Params 29997]\n",
      "[Epoch 4|Step 188|Flag 0|Sparsity 0.7730|Params 29997]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.821321 | Logloss: 0.400702\n",
      "Evaluate Time 0.013444816666666666 minutes\n",
      "[Epoch    5] train MF loss: 0.41088229, valid loss: 0inf, time 0.80698367 minutes\n",
      "********************************************************************************\n",
      "[Epoch 5|Step 110|Flag 0|Sparsity 0.7730|Params 29997]\n",
      "[Epoch 6|Step 32|Flag 0|Sparsity 0.7730|Params 29997]\n",
      "[Epoch 6|Step 532|Flag 0|Sparsity 0.7730|Params 29997]\n",
      "[Epoch 7|Step 454|Flag 0|Sparsity 0.7730|Params 29997]\n",
      "[Epoch 8|Step 376|Flag 0|Sparsity 0.7730|Params 29997]\n",
      "[Epoch 9|Step 298|Flag 0|Sparsity 0.7730|Params 29997]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.842708 | Logloss: 0.378699\n",
      "Evaluate Time 0.01070585 minutes\n",
      "[Epoch   10] train MF loss: 0.34508538, valid loss: 0inf, time 0.75400338 minutes\n",
      "********************************************************************************\n",
      "[Epoch 10|Step 220|Flag 0|Sparsity 0.7730|Params 29997]\n",
      "[Epoch 11|Step 142|Flag 0|Sparsity 0.7730|Params 29997]\n",
      "[Epoch 12|Step 64|Flag 0|Sparsity 0.7730|Params 29997]\n",
      "[Epoch 12|Step 564|Flag 0|Sparsity 0.7730|Params 29997]\n",
      "[Epoch 13|Step 486|Flag 0|Sparsity 0.7730|Params 29997]\n",
      "[Epoch 14|Step 408|Flag 0|Sparsity 0.7730|Params 29997]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.846821 | Logloss: 0.374529\n",
      "Evaluate Time 0.011666783333333335 minutes\n",
      "[Epoch   15] train MF loss: 0.34090638, valid loss: 0inf, time 0.74445088 minutes\n",
      "********************************************************************************\n",
      "[Epoch 15|Step 330|Flag 0|Sparsity 0.7730|Params 29997]\n",
      "[Epoch 16|Step 252|Flag 0|Sparsity 0.7730|Params 29997]\n",
      "[Epoch 17|Step 174|Flag 0|Sparsity 0.7730|Params 29997]\n",
      "[Epoch 18|Step 96|Flag 0|Sparsity 0.7730|Params 29997]\n",
      "[Epoch 19|Step 18|Flag 0|Sparsity 0.7730|Params 29997]\n",
      "[Epoch 19|Step 518|Flag 0|Sparsity 0.7730|Params 29997]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.847982 | Logloss: 0.373781\n",
      "Evaluate Time 0.013800249999999998 minutes\n",
      "[Epoch   20] train MF loss: 0.34784359, valid loss: 0inf, time 0.76621548 minutes\n",
      "********************************************************************************\n",
      "[Epoch 20|Step 440|Flag 0|Sparsity 0.7730|Params 29997]\n",
      "[Epoch 21|Step 362|Flag 0|Sparsity 0.7730|Params 29997]\n",
      "[Epoch 22|Step 284|Flag 0|Sparsity 0.7730|Params 29997]\n",
      "[Epoch 23|Step 206|Flag 0|Sparsity 0.7730|Params 29997]\n",
      "[Epoch 24|Step 128|Flag 0|Sparsity 0.7730|Params 29997]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.848040 | Logloss: 0.374174\n",
      "Evaluate Time 0.011688316666666667 minutes\n",
      "[Epoch   25] train MF loss: 0.32992741, valid loss: 0inf, time 0.77780317 minutes\n",
      "********************************************************************************\n",
      "[Epoch 25|Step 50|Flag 0|Sparsity 0.7730|Params 29997]\n",
      "[Epoch 25|Step 550|Flag 0|Sparsity 0.7730|Params 29997]\n",
      "[Epoch 26|Step 472|Flag 0|Sparsity 0.7730|Params 29997]\n",
      "[Epoch 27|Step 394|Flag 0|Sparsity 0.7730|Params 29997]\n",
      "[Epoch 28|Step 316|Flag 0|Sparsity 0.7730|Params 29997]\n",
      "[Epoch 29|Step 238|Flag 0|Sparsity 0.7730|Params 29997]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.848145 | Logloss: 0.374431\n",
      "Evaluate Time 0.011016666666666668 minutes\n",
      "[Epoch   30] train MF loss: 0.34385622, valid loss: 0inf, time 0.75496343 minutes\n",
      "********************************************************************************\n",
      "[Epoch 30|Step 160|Flag 0|Sparsity 0.7730|Params 29997]\n",
      "[Epoch 31|Step 82|Flag 0|Sparsity 0.7730|Params 29997]\n",
      "[Epoch 32|Step 4|Flag 0|Sparsity 0.7730|Params 29997]\n",
      "[Epoch 32|Step 504|Flag 0|Sparsity 0.7730|Params 29997]\n",
      "[Epoch 33|Step 426|Flag 0|Sparsity 0.7730|Params 29997]\n",
      "[Epoch 34|Step 348|Flag 0|Sparsity 0.7730|Params 29997]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.848147 | Logloss: 0.374581\n",
      "Evaluate Time 0.011041416666666666 minutes\n",
      "[Epoch   35] train MF loss: 0.33985519, valid loss: 0inf, time 0.75166990 minutes\n",
      "********************************************************************************\n",
      "[Epoch 35|Step 270|Flag 0|Sparsity 0.7730|Params 29997]\n",
      "[Epoch 36|Step 192|Flag 0|Sparsity 0.7730|Params 29997]\n",
      "[Epoch 37|Step 114|Flag 0|Sparsity 0.7730|Params 29997]\n",
      "[Epoch 38|Step 36|Flag 0|Sparsity 0.7730|Params 29997]\n",
      "[Epoch 38|Step 536|Flag 0|Sparsity 0.7730|Params 29997]\n",
      "[Epoch 39|Step 458|Flag 0|Sparsity 0.7730|Params 29997]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.847965 | Logloss: 0.374908\n",
      "Evaluate Time 0.011070216666666667 minutes\n",
      "[Epoch   40] train MF loss: 0.31326151, valid loss: 0inf, time 0.74412092 minutes\n",
      "********************************************************************************\n",
      "[Epoch 40|Step 380|Flag 1|Sparsity 0.7730|Params 29997]\n",
      "[Epoch 41|Step 302|Flag 1|Sparsity 0.7730|Params 29997]\n",
      "[Epoch 42|Step 224|Flag 1|Sparsity 0.7730|Params 29997]\n",
      "[Epoch 43|Step 146|Flag 1|Sparsity 0.7730|Params 29997]\n",
      "[Epoch 44|Step 68|Flag 1|Sparsity 0.7730|Params 29997]\n",
      "[Epoch 44|Step 568|Flag 1|Sparsity 0.7730|Params 29997]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.848116 | Logloss: 0.375079\n",
      "Evaluate Time 0.012114116666666667 minutes\n",
      "[Epoch   45] train MF loss: 0.33536249, valid loss: 0inf, time 0.75712073 minutes\n",
      "********************************************************************************\n",
      "[Epoch 45|Step 490|Flag 2|Sparsity 0.7730|Params 29997]\n",
      "[Epoch 46|Step 412|Flag 2|Sparsity 0.7730|Params 29997]\n",
      "[Epoch 47|Step 334|Flag 2|Sparsity 0.7730|Params 29997]\n",
      "[Epoch 48|Step 256|Flag 2|Sparsity 0.7730|Params 29997]\n",
      "[Epoch 49|Step 178|Flag 2|Sparsity 0.7730|Params 29997]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.847816 | Logloss: 0.375267\n",
      "Evaluate Time 0.011300366666666667 minutes\n",
      "[Epoch   50] train MF loss: 0.32782078, valid loss: 0inf, time 0.76895027 minutes\n",
      "********************************************************************************\n",
      "[Epoch 50|Step 100|Flag 3|Sparsity 0.7730|Params 29997]\n",
      "retrain for target parameters 49997\n",
      "FM_test_BaseDim32_bsz1024_lr_0.001_optim_adam_thresholdTypeFEATURE_DIM_thres_init-15_sigmoid-1_l2_penalty0\n",
      "Reconstructing ml-1m data from ./data/ml-1m/\n",
      "\tNum of train records: 591209\n",
      "\tNum of valid records: 73901\n",
      "\tNum of test records: 73902\n",
      "\tNum of fields: 7\n",
      "\tNum of features: 4129\n",
      "Retrain epoch 49997\n",
      "BackBone Embedding Parameters:  132128\n",
      "--------------------------------------------------------------------------------\n",
      "[complete episode  starts!]\n",
      "Initializing ...\n",
      "Retrain epoch 49997\n",
      "BackBone Embedding Parameters:  132128\n",
      "[Epoch 0|Step 0|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "Evaluate on test ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pytorchEnv\\env\\Lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "Test AUC: 0.496769 | Logloss: 1.962745\n",
      "Evaluate Time 0.0124793 minutes\n",
      "[Epoch    0] train MF loss: 1.87530637, valid loss: 0inf, time 0.01381427 minutes\n",
      "********************************************************************************\n",
      "[Epoch 0|Step 500|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 1|Step 422|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 2|Step 344|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 3|Step 266|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 4|Step 188|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.821560 | Logloss: 0.400776\n",
      "Evaluate Time 0.015773033333333332 minutes\n",
      "[Epoch    5] train MF loss: 0.40700075, valid loss: 0inf, time 0.76218147 minutes\n",
      "********************************************************************************\n",
      "[Epoch 5|Step 110|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 6|Step 32|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 6|Step 532|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 7|Step 454|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 8|Step 376|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 9|Step 298|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.842969 | Logloss: 0.378509\n",
      "Evaluate Time 0.011742216666666666 minutes\n",
      "[Epoch   10] train MF loss: 0.34511614, valid loss: 0inf, time 0.95141945 minutes\n",
      "********************************************************************************\n",
      "[Epoch 10|Step 220|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 11|Step 142|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 12|Step 64|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 12|Step 564|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 13|Step 486|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 14|Step 408|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.847084 | Logloss: 0.374276\n",
      "Evaluate Time 0.01086575 minutes\n",
      "[Epoch   15] train MF loss: 0.33738428, valid loss: 0inf, time 0.71885338 minutes\n",
      "********************************************************************************\n",
      "[Epoch 15|Step 330|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 16|Step 252|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 17|Step 174|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 18|Step 96|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 19|Step 18|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 19|Step 518|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.848049 | Logloss: 0.373869\n",
      "Evaluate Time 0.019230733333333336 minutes\n",
      "[Epoch   20] train MF loss: 0.34841043, valid loss: 0inf, time 0.74707725 minutes\n",
      "********************************************************************************\n",
      "[Epoch 20|Step 440|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 21|Step 362|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 22|Step 284|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 23|Step 206|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 24|Step 128|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.848034 | Logloss: 0.374581\n",
      "Evaluate Time 0.011073 minutes\n",
      "[Epoch   25] train MF loss: 0.32500216, valid loss: 0inf, time 0.86114663 minutes\n",
      "********************************************************************************\n",
      "[Epoch 25|Step 50|Flag 1|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 25|Step 550|Flag 1|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 26|Step 472|Flag 1|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 27|Step 394|Flag 1|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 28|Step 316|Flag 1|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 29|Step 238|Flag 1|Sparsity 0.6216|Params 49997]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.847975 | Logloss: 0.375126\n",
      "Evaluate Time 0.01094105 minutes\n",
      "[Epoch   30] train MF loss: 0.33588046, valid loss: 0inf, time 0.73630425 minutes\n",
      "********************************************************************************\n",
      "[Epoch 30|Step 160|Flag 2|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 31|Step 82|Flag 2|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 32|Step 4|Flag 2|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 32|Step 504|Flag 2|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 33|Step 426|Flag 2|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 34|Step 348|Flag 2|Sparsity 0.6216|Params 49997]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.847859 | Logloss: 0.375549\n",
      "Evaluate Time 0.012517083333333335 minutes\n",
      "[Epoch   35] train MF loss: 0.32952154, valid loss: 0inf, time 0.72527178 minutes\n",
      "********************************************************************************\n",
      "[Epoch 35|Step 270|Flag 3|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 36|Step 192|Flag 3|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 37|Step 114|Flag 3|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 38|Step 36|Flag 3|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 38|Step 536|Flag 3|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 39|Step 458|Flag 3|Sparsity 0.6216|Params 49997]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.847558 | Logloss: 0.376027\n",
      "Evaluate Time 0.011042116666666666 minutes\n",
      "[Epoch   40] train MF loss: 0.30864489, valid loss: 0inf, time 0.81784873 minutes\n",
      "********************************************************************************\n",
      "[Epoch 40|Step 380|Flag 4|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 41|Step 302|Flag 4|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 42|Step 224|Flag 4|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 43|Step 146|Flag 4|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 44|Step 68|Flag 4|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 44|Step 568|Flag 4|Sparsity 0.6216|Params 49997]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.847557 | Logloss: 0.376551\n",
      "Evaluate Time 0.011209333333333333 minutes\n",
      "[Epoch   45] train MF loss: 0.32671010, valid loss: 0inf, time 0.78884385 minutes\n",
      "********************************************************************************\n",
      "[Epoch 45|Step 490|Flag 5|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 46|Step 412|Flag 5|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 47|Step 334|Flag 5|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 48|Step 256|Flag 5|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 49|Step 178|Flag 5|Sparsity 0.6216|Params 49997]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.847260 | Logloss: 0.376863\n",
      "Evaluate Time 0.01128435 minutes\n",
      "[Epoch   50] train MF loss: 0.32145530, valid loss: 0inf, time 0.79070435 minutes\n",
      "********************************************************************************\n",
      "[Epoch 50|Step 100|Flag 6|Sparsity 0.6216|Params 49997]\n"
     ]
    }
   ],
   "source": [
    "target_parameters = [20000, 29997, 49997]\n",
    "best_test_results_list = []\n",
    "for target_param in target_parameters:\n",
    "\n",
    "    print(f\"retrain for target parameters {target_param}\")\n",
    "    parser = setup_args()\n",
    "    parser.set_defaults(\n",
    "        alias='test',\n",
    "        tensorboard='./tmp/runs/{factorizer}/{data_type}',\n",
    "        ##########\n",
    "        ## data ##\n",
    "        ##########\n",
    "        data_type='ml-1m',\n",
    "        data_path='./data/{data_type}/',\n",
    "        load_in_queue=False,\n",
    "        category_only=False,\n",
    "        rebuild_cache=False,\n",
    "        eval_res_path='./tmp/res/{factorizer}/{data_type}/{alias}/{epoch_idx}.csv',\n",
    "        emb_save_path='./tmp/embedding/{factorizer}/{data_type}/{alias}/{num_parameter}',\n",
    "        ######################\n",
    "        ## train/test split ##\n",
    "        ######################\n",
    "        test_ratio=0.1,\n",
    "        valid_ratio=1/9,\n",
    "        ##########################\n",
    "        ## Devices & Efficiency ##\n",
    "        ##########################\n",
    "        use_cuda=True,\n",
    "        early_stop=40,\n",
    "        log_interval=1,\n",
    "        display_interval=500,\n",
    "        eval_interval=5,  # 10 epochs between 2 evaluations\n",
    "        device_ids_test=[0],\n",
    "        device_id=0,\n",
    "        batch_size_train=1024,\n",
    "        batch_size_valid=1024,\n",
    "        batch_size_test=1024,\n",
    "        ###########\n",
    "        ## Model ##\n",
    "        ###########\n",
    "        factorizer='fm',\n",
    "        model='fm',\n",
    "        fm_lr=1e-3,\n",
    "        # Deep\n",
    "        mlp_dims=[100, 100],\n",
    "        # AutoInt\n",
    "        has_residual=True,\n",
    "        full_part=True,\n",
    "        num_heads=2,\n",
    "        num_layers=3,\n",
    "        att_dropout=0.4,\n",
    "        atten_embed_dim=64,\n",
    "        # optimizer setting\n",
    "        fm_optimizer='adam',\n",
    "        fm_amsgrad=False,\n",
    "        fm_eps=1e-8,\n",
    "        fm_l2_regularization=1e-5,\n",
    "        fm_betas=(0.9, 0.999),\n",
    "        fm_grad_clip=100,  # 0.1\n",
    "        fm_lr_exp_decay=1,\n",
    "        l2_penalty=0,\n",
    "        #########\n",
    "        ## PEP ##\n",
    "        #########\n",
    "        latent_dim=32,\n",
    "        threshold_type='feature_dim',\n",
    "        g_type='sigmoid',\n",
    "        gk=1,\n",
    "        threshold_init=-15,\n",
    "        retrain_emb_param=target_param,\n",
    "        re_init=False,\n",
    "    )\n",
    "\n",
    "    opt = parser.parse_args(args=[])\n",
    "    opt = vars(opt)\n",
    "    opt['alias'] = '{}_{}_BaseDim{}_bsz{}_lr_{}_optim_{}_thresholdType{}_thres_init{}_{}-{}_l2_penalty{}'.format(\n",
    "        opt['model'].upper(),\n",
    "        opt['alias'],\n",
    "        opt['latent_dim'],\n",
    "        opt['batch_size_train'],\n",
    "        opt['fm_lr'],\n",
    "        opt['fm_optimizer'],\n",
    "        opt['threshold_type'].upper(),\n",
    "        opt['threshold_init'],\n",
    "        opt['g_type'],\n",
    "        opt['gk'],\n",
    "        opt['l2_penalty']\n",
    "    )\n",
    "    print(opt['alias'])\n",
    "    random.seed(opt['seed'])\n",
    "    # np.random.seed(opt['seed'])\n",
    "    torch.manual_seed(opt['seed'])\n",
    "    torch.cuda.manual_seed_all(opt['seed'])\n",
    "    engine = Engine(opt)\n",
    "    best_result = engine.train()\n",
    "\n",
    "    best_test_results_list.append(best_result)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FM_PEP_retrain_20000_29997_49997 = {}\n",
    "for i, param in enumerate([20000, 29997, 49997]):\n",
    "    FM_PEP_retrain_20000_29997_49997[param] =  best_test_results_list[i]\n",
    "\n",
    "pd.DataFrame(FM_PEP_retrain_20000_29997_49997).to_csv(\"FM_PEP_retrain_[20000, 29997, 49997].csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train DEEPFM, with Criteo Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEEPFM_train_BaseDim32_bsz1024_lr_0.001_optim_adam_thresholdTypeFEATURE_DIM_thres_init-15_sigmoid-1_l2_penalty0\n",
      "Reconstructing ml-1m data from ./data/ml-1m/\n",
      "\tNum of train records: 591209\n",
      "\tNum of valid records: 73901\n",
      "\tNum of test records: 73902\n",
      "\tNum of fields: 7\n",
      "\tNum of features: 4129\n",
      "BackBone Embedding Parameters:  132128\n",
      "--------------------------------------------------------------------------------\n",
      "[complete episode  starts!]\n",
      "Initializing ...\n",
      "BackBone Embedding Parameters:  132128\n",
      "********************************************************************************\n",
      "Save the initial embedding table\n",
      "********************************************************************************\n",
      "[Epoch 0|Step 0|Flag 0|Sparsity 0.0000|Params 132128]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pytorchEnv\\env\\Lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0|Step 500|Flag 0|Sparsity 0.0041|Params 131589]\n",
      "[Epoch 1|Step 422|Flag 0|Sparsity 0.0041|Params 131583]\n",
      "[Epoch 2|Step 344|Flag 0|Sparsity 0.0042|Params 131573]\n",
      "[Epoch 3|Step 266|Flag 0|Sparsity 0.0042|Params 131570]\n",
      "[Epoch 4|Step 188|Flag 0|Sparsity 0.0043|Params 131566]\n",
      "[Epoch 5|Step 110|Flag 0|Sparsity 0.0044|Params 131542]\n",
      "[Epoch 6|Step 32|Flag 0|Sparsity 0.0046|Params 131521]\n",
      "[Epoch 6|Step 532|Flag 0|Sparsity 0.0049|Params 131485]\n",
      "[Epoch 7|Step 454|Flag 0|Sparsity 0.0054|Params 131413]\n",
      "[Epoch 8|Step 376|Flag 0|Sparsity 0.0065|Params 131274]\n",
      "[Epoch 9|Step 298|Flag 0|Sparsity 0.0084|Params 131022]\n",
      "[Epoch 10|Step 220|Flag 0|Sparsity 0.0125|Params 130477]\n",
      "[Epoch 11|Step 142|Flag 0|Sparsity 0.0194|Params 129562]\n",
      "[Epoch 12|Step 64|Flag 0|Sparsity 0.0327|Params 127805]\n",
      "[Epoch 12|Step 564|Flag 0|Sparsity 0.0552|Params 124840]\n",
      "[Epoch 13|Step 486|Flag 0|Sparsity 0.0878|Params 120528]\n",
      "[Epoch 14|Step 408|Flag 0|Sparsity 0.1284|Params 115163]\n",
      "[Epoch 15|Step 330|Flag 0|Sparsity 0.1685|Params 109863]\n",
      "[Epoch 16|Step 252|Flag 0|Sparsity 0.2032|Params 105274]\n",
      "[Epoch 17|Step 174|Flag 0|Sparsity 0.2330|Params 101342]\n",
      "[Epoch 18|Step 96|Flag 0|Sparsity 0.2586|Params 97960]\n",
      "[Epoch 19|Step 18|Flag 0|Sparsity 0.2831|Params 94719]\n",
      "[Epoch 19|Step 518|Flag 0|Sparsity 0.3074|Params 91514]\n",
      "[Epoch 20|Step 440|Flag 0|Sparsity 0.3333|Params 88093]\n",
      "[Epoch 21|Step 362|Flag 0|Sparsity 0.3635|Params 84098]\n",
      "[Epoch 22|Step 284|Flag 0|Sparsity 0.3997|Params 79316]\n",
      "[Epoch 23|Step 206|Flag 0|Sparsity 0.4474|Params 73009]\n",
      "[Epoch 24|Step 128|Flag 0|Sparsity 0.5080|Params 65013]\n",
      "[Epoch 25|Step 50|Flag 0|Sparsity 0.5725|Params 56490]\n",
      "********************************************************************************\n",
      "Reach the target parameter: 50000, save embedding with size: 49988\n",
      "********************************************************************************\n",
      "[Epoch 25|Step 550|Flag 0|Sparsity 0.6321|Params 48607]\n",
      "[Epoch 26|Step 472|Flag 0|Sparsity 0.6795|Params 42346]\n",
      "[Epoch 27|Step 394|Flag 0|Sparsity 0.7123|Params 38016]\n",
      "[Epoch 28|Step 316|Flag 0|Sparsity 0.7357|Params 34925]\n",
      "[Epoch 29|Step 238|Flag 0|Sparsity 0.7524|Params 32714]\n",
      "[Epoch 30|Step 160|Flag 0|Sparsity 0.7642|Params 31154]\n",
      "********************************************************************************\n",
      "Reach the target parameter: 30000, save embedding with size: 30000\n",
      "********************************************************************************\n",
      "[Epoch 31|Step 82|Flag 0|Sparsity 0.7738|Params 29885]\n",
      "[Epoch 32|Step 4|Flag 0|Sparsity 0.7818|Params 28836]\n",
      "[Epoch 32|Step 504|Flag 0|Sparsity 0.7887|Params 27922]\n",
      "[Epoch 33|Step 426|Flag 0|Sparsity 0.7950|Params 27084]\n",
      "[Epoch 34|Step 348|Flag 0|Sparsity 0.7999|Params 26436]\n",
      "[Epoch 35|Step 270|Flag 0|Sparsity 0.8043|Params 25857]\n",
      "[Epoch 36|Step 192|Flag 0|Sparsity 0.8088|Params 25269]\n",
      "[Epoch 37|Step 114|Flag 0|Sparsity 0.8125|Params 24768]\n",
      "[Epoch 38|Step 36|Flag 0|Sparsity 0.8157|Params 24349]\n",
      "[Epoch 38|Step 536|Flag 0|Sparsity 0.8188|Params 23947]\n",
      "[Epoch 39|Step 458|Flag 0|Sparsity 0.8217|Params 23560]\n",
      "[Epoch 40|Step 380|Flag 0|Sparsity 0.8244|Params 23204]\n",
      "[Epoch 41|Step 302|Flag 0|Sparsity 0.8267|Params 22903]\n",
      "[Epoch 42|Step 224|Flag 0|Sparsity 0.8287|Params 22627]\n",
      "[Epoch 43|Step 146|Flag 0|Sparsity 0.8306|Params 22385]\n",
      "[Epoch 44|Step 68|Flag 0|Sparsity 0.8324|Params 22142]\n",
      "[Epoch 44|Step 568|Flag 0|Sparsity 0.8343|Params 21897]\n",
      "[Epoch 45|Step 490|Flag 0|Sparsity 0.8360|Params 21673]\n",
      "[Epoch 46|Step 412|Flag 0|Sparsity 0.8376|Params 21455]\n",
      "[Epoch 47|Step 334|Flag 0|Sparsity 0.8389|Params 21282]\n",
      "[Epoch 48|Step 256|Flag 0|Sparsity 0.8401|Params 21121]\n",
      "[Epoch 49|Step 178|Flag 0|Sparsity 0.8415|Params 20943]\n",
      "[Epoch 50|Step 100|Flag 0|Sparsity 0.8427|Params 20781]\n",
      "[Epoch 51|Step 22|Flag 0|Sparsity 0.8438|Params 20633]\n",
      "[Epoch 51|Step 522|Flag 0|Sparsity 0.8449|Params 20499]\n",
      "[Epoch 52|Step 444|Flag 0|Sparsity 0.8461|Params 20335]\n",
      "[Epoch 53|Step 366|Flag 0|Sparsity 0.8471|Params 20200]\n",
      "[Epoch 54|Step 288|Flag 0|Sparsity 0.8481|Params 20069]\n",
      "********************************************************************************\n",
      "Reach the target parameter: 20000, save embedding with size: 20000\n",
      "********************************************************************************\n",
      "Minimal target parameters achieved, stop pruning.\n",
      "********** {'AUC': [0, 0], 'LogLoss': [inf, 0]} **********\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "parser = setup_args()\n",
    "parser.set_defaults(\n",
    "    alias='train',\n",
    "    tensorboard='./tmp/runs/{factorizer}/{data_type}',\n",
    "    ##########\n",
    "    ## data ##\n",
    "    ##########\n",
    "    data_type='ml-1m',\n",
    "    data_path='./data/{data_type}/',\n",
    "    load_in_queue=False,\n",
    "    category_only=False,\n",
    "    rebuild_cache=False,\n",
    "    eval_res_path='./tmp/res/{factorizer}/{data_type}/{alias}/{epoch_idx}.csv',\n",
    "    emb_save_path='./tmp/embedding/{factorizer}/{data_type}/{alias}/{num_parameter}',\n",
    "    ######################\n",
    "    ## train/test split ##\n",
    "    ######################\n",
    "    test_ratio=0.1,\n",
    "    valid_ratio=1/9,\n",
    "    ##########################\n",
    "    ## Devices & Efficiency ##\n",
    "    ##########################\n",
    "    use_cuda=True,\n",
    "    early_stop=10,\n",
    "    log_interval=1,\n",
    "    display_interval=500,\n",
    "    eval_interval=5,  # 10 epochs between 2 evaluations\n",
    "    device_ids_test=[0],\n",
    "    device_id=0,\n",
    "    batch_size_train=1024,\n",
    "    batch_size_valid=1024,\n",
    "    batch_size_test=1024,\n",
    "    ###########\n",
    "    ## Model ##\n",
    "    ###########\n",
    "    factorizer='fm',\n",
    "    model='deepfm',\n",
    "    fm_lr=1e-3,\n",
    "    # Deep\n",
    "    mlp_dims=[100, 100],\n",
    "    # AutoInt\n",
    "    has_residual=True,\n",
    "    full_part=True,\n",
    "    num_heads=2,\n",
    "    num_layers=3,\n",
    "    att_dropout=0.4,\n",
    "    atten_embed_dim=64,\n",
    "    # optimizer setting\n",
    "    fm_optimizer='adam',\n",
    "    fm_amsgrad=False,\n",
    "    fm_eps=1e-8,\n",
    "    fm_l2_regularization=1e-5,\n",
    "    fm_betas=(0.9, 0.999),\n",
    "    fm_grad_clip=100,  # 0.1\n",
    "    fm_lr_exp_decay=1,\n",
    "    l2_penalty=0,\n",
    "    #########\n",
    "    ## Embeddings//PEP ##\n",
    "    #########\n",
    "    latent_dim=32,\n",
    "    threshold_type='feature_dim',\n",
    "    g_type='sigmoid',\n",
    "    gk=1,\n",
    "    threshold_init=-15,\n",
    "    candidate_p=[50000, 30000, 20000],\n",
    ")\n",
    "\n",
    "opt = parser.parse_args(args=[])\n",
    "opt = vars(opt)\n",
    "\n",
    "# rename alias\n",
    "\n",
    "opt['alias'] = '{}_{}_BaseDim{}_bsz{}_lr_{}_optim_{}_thresholdType{}_thres_init{}_{}-{}_l2_penalty{}'.format(\n",
    "    opt['model'].upper(),\n",
    "    opt['alias'],\n",
    "    opt['latent_dim'],\n",
    "    opt['batch_size_train'],\n",
    "    opt['fm_lr'],\n",
    "    opt['fm_optimizer'],\n",
    "    opt['threshold_type'].upper(),\n",
    "    opt['threshold_init'],\n",
    "    opt['g_type'],\n",
    "    opt['gk'],\n",
    "    opt['l2_penalty']\n",
    ")\n",
    "print(opt['alias'])\n",
    "random.seed(opt['seed'])\n",
    "# np.random.seed(opt['seed'])\n",
    "torch.manual_seed(opt['seed'])\n",
    "torch.cuda.manual_seed_all(opt['seed'])\n",
    "engine = Engine(opt)\n",
    "best_result = engine.train()\n",
    "print(\"*\"*10 , best_result, \"*\"*10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrain for DeepFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrain for target parameters 20000\n",
      "DEEPFM_train_BaseDim32_bsz1024_lr_0.001_optim_adam_thresholdTypeFEATURE_DIM_thres_init-15_sigmoid-1_l2_penalty0\n",
      "Reconstructing ml-1m data from ./data/ml-1m/\n",
      "\tNum of train records: 591209\n",
      "\tNum of valid records: 73901\n",
      "\tNum of test records: 73902\n",
      "\tNum of fields: 7\n",
      "\tNum of features: 4129\n",
      "Retrain epoch 20000\n",
      "BackBone Embedding Parameters:  132128\n",
      "--------------------------------------------------------------------------------\n",
      "[complete episode  starts!]\n",
      "Initializing ...\n",
      "Retrain epoch 20000\n",
      "BackBone Embedding Parameters:  132128\n",
      "[Epoch 0|Step 0|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "Evaluate on test ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pytorchEnv\\env\\Lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "Test AUC: 0.526084 | Logloss: 1.598122\n",
      "Evaluate Time 0.013960499999999999 minutes\n",
      "[Epoch    0] train MF loss: 1.52217436, valid loss: 0inf, time 0.01515077 minutes\n",
      "********************************************************************************\n",
      "[Epoch 0|Step 500|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 1|Step 422|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 2|Step 344|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 3|Step 266|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 4|Step 188|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.838329 | Logloss: 0.383244\n",
      "Evaluate Time 0.011680066666666666 minutes\n",
      "[Epoch    5] train MF loss: 0.34952039, valid loss: 0inf, time 0.81011163 minutes\n",
      "********************************************************************************\n",
      "[Epoch 5|Step 110|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 6|Step 32|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 6|Step 532|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 7|Step 454|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 8|Step 376|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 9|Step 298|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.843414 | Logloss: 0.379373\n",
      "Evaluate Time 0.013535733333333333 minutes\n",
      "[Epoch   10] train MF loss: 0.32715750, valid loss: 0inf, time 0.81726147 minutes\n",
      "********************************************************************************\n",
      "[Epoch 10|Step 220|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 11|Step 142|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 12|Step 64|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 12|Step 564|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 13|Step 486|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 14|Step 408|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.844204 | Logloss: 0.378666\n",
      "Evaluate Time 0.011125083333333334 minutes\n",
      "[Epoch   15] train MF loss: 0.32701695, valid loss: 0inf, time 0.79649038 minutes\n",
      "********************************************************************************\n",
      "[Epoch 15|Step 330|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 16|Step 252|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 17|Step 174|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 18|Step 96|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 19|Step 18|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 19|Step 518|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.845731 | Logloss: 0.378197\n",
      "Evaluate Time 0.0109467 minutes\n",
      "[Epoch   20] train MF loss: 0.34231737, valid loss: 0inf, time 0.79681423 minutes\n",
      "********************************************************************************\n",
      "[Epoch 20|Step 440|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 21|Step 362|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 22|Step 284|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 23|Step 206|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 24|Step 128|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.845296 | Logloss: 0.377750\n",
      "Evaluate Time 0.0114897 minutes\n",
      "[Epoch   25] train MF loss: 0.32887110, valid loss: 0inf, time 0.82882120 minutes\n",
      "********************************************************************************\n",
      "[Epoch 25|Step 50|Flag 1|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 25|Step 550|Flag 1|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 26|Step 472|Flag 1|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 27|Step 394|Flag 1|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 28|Step 316|Flag 1|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 29|Step 238|Flag 1|Sparsity 0.8486|Params 20000]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.846443 | Logloss: 0.377239\n",
      "Evaluate Time 0.012196 minutes\n",
      "[Epoch   30] train MF loss: 0.31526160, valid loss: 0inf, time 0.79583572 minutes\n",
      "********************************************************************************\n",
      "[Epoch 30|Step 160|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 31|Step 82|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 32|Step 4|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 32|Step 504|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 33|Step 426|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 34|Step 348|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.845926 | Logloss: 0.378841\n",
      "Evaluate Time 0.011905933333333334 minutes\n",
      "[Epoch   35] train MF loss: 0.30603698, valid loss: 0inf, time 0.83232752 minutes\n",
      "********************************************************************************\n",
      "[Epoch 35|Step 270|Flag 1|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 36|Step 192|Flag 1|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 37|Step 114|Flag 1|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 38|Step 36|Flag 1|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 38|Step 536|Flag 1|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 39|Step 458|Flag 1|Sparsity 0.8486|Params 20000]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.846075 | Logloss: 0.378878\n",
      "Evaluate Time 0.011221933333333333 minutes\n",
      "[Epoch   40] train MF loss: 0.32512927, valid loss: 0inf, time 0.80642078 minutes\n",
      "********************************************************************************\n",
      "[Epoch 40|Step 380|Flag 2|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 41|Step 302|Flag 2|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 42|Step 224|Flag 2|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 43|Step 146|Flag 2|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 44|Step 68|Flag 2|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 44|Step 568|Flag 2|Sparsity 0.8486|Params 20000]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.845607 | Logloss: 0.380628\n",
      "Evaluate Time 0.012235766666666667 minutes\n",
      "[Epoch   45] train MF loss: 0.31271780, valid loss: 0inf, time 0.81506433 minutes\n",
      "********************************************************************************\n",
      "[Epoch 45|Step 490|Flag 3|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 46|Step 412|Flag 3|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 47|Step 334|Flag 3|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 48|Step 256|Flag 3|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 49|Step 178|Flag 3|Sparsity 0.8486|Params 20000]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.845828 | Logloss: 0.380181\n",
      "Evaluate Time 0.0108922 minutes\n",
      "[Epoch   50] train MF loss: 0.32581753, valid loss: 0inf, time 0.80121498 minutes\n",
      "********************************************************************************\n",
      "[Epoch 50|Step 100|Flag 4|Sparsity 0.8486|Params 20000]\n",
      "retrain for target parameters 30000\n",
      "DEEPFM_train_BaseDim32_bsz1024_lr_0.001_optim_adam_thresholdTypeFEATURE_DIM_thres_init-15_sigmoid-1_l2_penalty0\n",
      "Reconstructing ml-1m data from ./data/ml-1m/\n",
      "\tNum of train records: 591209\n",
      "\tNum of valid records: 73901\n",
      "\tNum of test records: 73902\n",
      "\tNum of fields: 7\n",
      "\tNum of features: 4129\n",
      "Retrain epoch 30000\n",
      "BackBone Embedding Parameters:  132128\n",
      "--------------------------------------------------------------------------------\n",
      "[complete episode  starts!]\n",
      "Initializing ...\n",
      "Retrain epoch 30000\n",
      "BackBone Embedding Parameters:  132128\n",
      "[Epoch 0|Step 0|Flag 0|Sparsity 0.7729|Params 30000]\n",
      "Evaluate on test ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pytorchEnv\\env\\Lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "Test AUC: 0.526057 | Logloss: 1.597610\n",
      "Evaluate Time 0.014207250000000001 minutes\n",
      "[Epoch    0] train MF loss: 1.51286018, valid loss: 0inf, time 0.01549913 minutes\n",
      "********************************************************************************\n",
      "[Epoch 0|Step 500|Flag 0|Sparsity 0.7729|Params 30000]\n",
      "[Epoch 1|Step 422|Flag 0|Sparsity 0.7729|Params 30000]\n",
      "[Epoch 2|Step 344|Flag 0|Sparsity 0.7729|Params 30000]\n",
      "[Epoch 3|Step 266|Flag 0|Sparsity 0.7729|Params 30000]\n",
      "[Epoch 4|Step 188|Flag 0|Sparsity 0.7729|Params 30000]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.841060 | Logloss: 0.380721\n",
      "Evaluate Time 0.011400033333333334 minutes\n",
      "[Epoch    5] train MF loss: 0.34550154, valid loss: 0inf, time 0.83183807 minutes\n",
      "********************************************************************************\n",
      "[Epoch 5|Step 110|Flag 0|Sparsity 0.7729|Params 30000]\n",
      "[Epoch 6|Step 32|Flag 0|Sparsity 0.7729|Params 30000]\n",
      "[Epoch 6|Step 532|Flag 0|Sparsity 0.7729|Params 30000]\n",
      "[Epoch 7|Step 454|Flag 0|Sparsity 0.7729|Params 30000]\n",
      "[Epoch 8|Step 376|Flag 0|Sparsity 0.7729|Params 30000]\n",
      "[Epoch 9|Step 298|Flag 0|Sparsity 0.7729|Params 30000]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.844633 | Logloss: 0.378539\n",
      "Evaluate Time 0.011306583333333333 minutes\n",
      "[Epoch   10] train MF loss: 0.31705943, valid loss: 0inf, time 0.81463670 minutes\n",
      "********************************************************************************\n",
      "[Epoch 10|Step 220|Flag 0|Sparsity 0.7729|Params 30000]\n",
      "[Epoch 11|Step 142|Flag 0|Sparsity 0.7729|Params 30000]\n",
      "[Epoch 12|Step 64|Flag 0|Sparsity 0.7729|Params 30000]\n",
      "[Epoch 12|Step 564|Flag 0|Sparsity 0.7729|Params 30000]\n",
      "[Epoch 13|Step 486|Flag 0|Sparsity 0.7729|Params 30000]\n",
      "[Epoch 14|Step 408|Flag 0|Sparsity 0.7729|Params 30000]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.845337 | Logloss: 0.378273\n",
      "Evaluate Time 0.0118325 minutes\n",
      "[Epoch   15] train MF loss: 0.31141096, valid loss: 0inf, time 0.81225480 minutes\n",
      "********************************************************************************\n",
      "[Epoch 15|Step 330|Flag 0|Sparsity 0.7729|Params 30000]\n",
      "[Epoch 16|Step 252|Flag 0|Sparsity 0.7729|Params 30000]\n",
      "[Epoch 17|Step 174|Flag 0|Sparsity 0.7729|Params 30000]\n",
      "[Epoch 18|Step 96|Flag 0|Sparsity 0.7729|Params 30000]\n",
      "[Epoch 19|Step 18|Flag 0|Sparsity 0.7729|Params 30000]\n",
      "[Epoch 19|Step 518|Flag 0|Sparsity 0.7729|Params 30000]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.845283 | Logloss: 0.379692\n",
      "Evaluate Time 0.013807733333333332 minutes\n",
      "[Epoch   20] train MF loss: 0.32703179, valid loss: 0inf, time 0.81164072 minutes\n",
      "********************************************************************************\n",
      "[Epoch 20|Step 440|Flag 1|Sparsity 0.7729|Params 30000]\n",
      "[Epoch 21|Step 362|Flag 1|Sparsity 0.7729|Params 30000]\n",
      "[Epoch 22|Step 284|Flag 1|Sparsity 0.7729|Params 30000]\n",
      "[Epoch 23|Step 206|Flag 1|Sparsity 0.7729|Params 30000]\n",
      "[Epoch 24|Step 128|Flag 1|Sparsity 0.7729|Params 30000]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.844724 | Logloss: 0.379657\n",
      "Evaluate Time 0.01201435 minutes\n",
      "[Epoch   25] train MF loss: 0.30907136, valid loss: 0inf, time 0.83375252 minutes\n",
      "********************************************************************************\n",
      "[Epoch 25|Step 50|Flag 2|Sparsity 0.7729|Params 30000]\n",
      "[Epoch 25|Step 550|Flag 2|Sparsity 0.7729|Params 30000]\n",
      "[Epoch 26|Step 472|Flag 2|Sparsity 0.7729|Params 30000]\n",
      "[Epoch 27|Step 394|Flag 2|Sparsity 0.7729|Params 30000]\n",
      "[Epoch 28|Step 316|Flag 2|Sparsity 0.7729|Params 30000]\n",
      "[Epoch 29|Step 238|Flag 2|Sparsity 0.7729|Params 30000]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.845968 | Logloss: 0.379207\n",
      "Evaluate Time 0.011986566666666667 minutes\n",
      "[Epoch   30] train MF loss: 0.29472002, valid loss: 0inf, time 0.83686637 minutes\n",
      "********************************************************************************\n",
      "[Epoch 30|Step 160|Flag 0|Sparsity 0.7729|Params 30000]\n",
      "[Epoch 31|Step 82|Flag 0|Sparsity 0.7729|Params 30000]\n",
      "[Epoch 32|Step 4|Flag 0|Sparsity 0.7729|Params 30000]\n",
      "[Epoch 32|Step 504|Flag 0|Sparsity 0.7729|Params 30000]\n",
      "[Epoch 33|Step 426|Flag 0|Sparsity 0.7729|Params 30000]\n",
      "[Epoch 34|Step 348|Flag 0|Sparsity 0.7729|Params 30000]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.845338 | Logloss: 0.381762\n",
      "Evaluate Time 0.012079233333333333 minutes\n",
      "[Epoch   35] train MF loss: 0.29260734, valid loss: 0inf, time 0.82618603 minutes\n",
      "********************************************************************************\n",
      "[Epoch 35|Step 270|Flag 1|Sparsity 0.7729|Params 30000]\n",
      "[Epoch 36|Step 192|Flag 1|Sparsity 0.7729|Params 30000]\n",
      "[Epoch 37|Step 114|Flag 1|Sparsity 0.7729|Params 30000]\n",
      "[Epoch 38|Step 36|Flag 1|Sparsity 0.7729|Params 30000]\n",
      "[Epoch 38|Step 536|Flag 1|Sparsity 0.7729|Params 30000]\n",
      "[Epoch 39|Step 458|Flag 1|Sparsity 0.7729|Params 30000]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.845546 | Logloss: 0.381717\n",
      "Evaluate Time 0.01169635 minutes\n",
      "[Epoch   40] train MF loss: 0.31533301, valid loss: 0inf, time 0.83950163 minutes\n",
      "********************************************************************************\n",
      "[Epoch 40|Step 380|Flag 2|Sparsity 0.7729|Params 30000]\n",
      "[Epoch 41|Step 302|Flag 2|Sparsity 0.7729|Params 30000]\n",
      "[Epoch 42|Step 224|Flag 2|Sparsity 0.7729|Params 30000]\n",
      "[Epoch 43|Step 146|Flag 2|Sparsity 0.7729|Params 30000]\n",
      "[Epoch 44|Step 68|Flag 2|Sparsity 0.7729|Params 30000]\n",
      "[Epoch 44|Step 568|Flag 2|Sparsity 0.7729|Params 30000]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.844469 | Logloss: 0.384565\n",
      "Evaluate Time 0.011223633333333333 minutes\n",
      "[Epoch   45] train MF loss: 0.31603062, valid loss: 0inf, time 0.86737563 minutes\n",
      "********************************************************************************\n",
      "[Epoch 45|Step 490|Flag 3|Sparsity 0.7729|Params 30000]\n",
      "[Epoch 46|Step 412|Flag 3|Sparsity 0.7729|Params 30000]\n",
      "[Epoch 47|Step 334|Flag 3|Sparsity 0.7729|Params 30000]\n",
      "[Epoch 48|Step 256|Flag 3|Sparsity 0.7729|Params 30000]\n",
      "[Epoch 49|Step 178|Flag 3|Sparsity 0.7729|Params 30000]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.844407 | Logloss: 0.384361\n",
      "Evaluate Time 0.011407316666666667 minutes\n",
      "[Epoch   50] train MF loss: 0.31185728, valid loss: 0inf, time 0.81182310 minutes\n",
      "********************************************************************************\n",
      "[Epoch 50|Step 100|Flag 4|Sparsity 0.7729|Params 30000]\n",
      "retrain for target parameters 49988\n",
      "DEEPFM_train_BaseDim32_bsz1024_lr_0.001_optim_adam_thresholdTypeFEATURE_DIM_thres_init-15_sigmoid-1_l2_penalty0\n",
      "Reconstructing ml-1m data from ./data/ml-1m/\n",
      "\tNum of train records: 591209\n",
      "\tNum of valid records: 73901\n",
      "\tNum of test records: 73902\n",
      "\tNum of fields: 7\n",
      "\tNum of features: 4129\n",
      "Retrain epoch 49988\n",
      "BackBone Embedding Parameters:  132128\n",
      "--------------------------------------------------------------------------------\n",
      "[complete episode  starts!]\n",
      "Initializing ...\n",
      "Retrain epoch 49988\n",
      "BackBone Embedding Parameters:  132128\n",
      "[Epoch 0|Step 0|Flag 0|Sparsity 0.6217|Params 49988]\n",
      "Evaluate on test ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pytorchEnv\\env\\Lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "Test AUC: 0.526055 | Logloss: 1.597284\n",
      "Evaluate Time 0.01292345 minutes\n",
      "[Epoch    0] train MF loss: 1.51031423, valid loss: 0inf, time 0.01418282 minutes\n",
      "********************************************************************************\n",
      "[Epoch 0|Step 500|Flag 0|Sparsity 0.6217|Params 49988]\n",
      "[Epoch 1|Step 422|Flag 0|Sparsity 0.6217|Params 49988]\n",
      "[Epoch 2|Step 344|Flag 0|Sparsity 0.6217|Params 49988]\n",
      "[Epoch 3|Step 266|Flag 0|Sparsity 0.6217|Params 49988]\n",
      "[Epoch 4|Step 188|Flag 0|Sparsity 0.6217|Params 49988]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.842238 | Logloss: 0.379733\n",
      "Evaluate Time 0.011332583333333333 minutes\n",
      "[Epoch    5] train MF loss: 0.33781564, valid loss: 0inf, time 0.81064522 minutes\n",
      "********************************************************************************\n",
      "[Epoch 5|Step 110|Flag 0|Sparsity 0.6217|Params 49988]\n",
      "[Epoch 6|Step 32|Flag 0|Sparsity 0.6217|Params 49988]\n",
      "[Epoch 6|Step 532|Flag 0|Sparsity 0.6217|Params 49988]\n",
      "[Epoch 7|Step 454|Flag 0|Sparsity 0.6217|Params 49988]\n",
      "[Epoch 8|Step 376|Flag 0|Sparsity 0.6217|Params 49988]\n",
      "[Epoch 9|Step 298|Flag 0|Sparsity 0.6217|Params 49988]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.844888 | Logloss: 0.379858\n",
      "Evaluate Time 0.015840366666666668 minutes\n",
      "[Epoch   10] train MF loss: 0.30366910, valid loss: 0inf, time 0.82768202 minutes\n",
      "********************************************************************************\n",
      "[Epoch 10|Step 220|Flag 0|Sparsity 0.6217|Params 49988]\n",
      "[Epoch 11|Step 142|Flag 0|Sparsity 0.6217|Params 49988]\n",
      "[Epoch 12|Step 64|Flag 0|Sparsity 0.6217|Params 49988]\n",
      "[Epoch 12|Step 564|Flag 0|Sparsity 0.6217|Params 49988]\n",
      "[Epoch 13|Step 486|Flag 0|Sparsity 0.6217|Params 49988]\n",
      "[Epoch 14|Step 408|Flag 0|Sparsity 0.6217|Params 49988]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.844282 | Logloss: 0.381512\n",
      "Evaluate Time 0.0109198 minutes\n",
      "[Epoch   15] train MF loss: 0.30584413, valid loss: 0inf, time 0.81296070 minutes\n",
      "********************************************************************************\n",
      "[Epoch 15|Step 330|Flag 1|Sparsity 0.6217|Params 49988]\n",
      "[Epoch 16|Step 252|Flag 1|Sparsity 0.6217|Params 49988]\n",
      "[Epoch 17|Step 174|Flag 1|Sparsity 0.6217|Params 49988]\n",
      "[Epoch 18|Step 96|Flag 1|Sparsity 0.6217|Params 49988]\n",
      "[Epoch 19|Step 18|Flag 1|Sparsity 0.6217|Params 49988]\n",
      "[Epoch 19|Step 518|Flag 1|Sparsity 0.6217|Params 49988]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.844501 | Logloss: 0.382802\n",
      "Evaluate Time 0.0110247 minutes\n",
      "[Epoch   20] train MF loss: 0.32874134, valid loss: 0inf, time 0.80027382 minutes\n",
      "********************************************************************************\n",
      "[Epoch 20|Step 440|Flag 2|Sparsity 0.6217|Params 49988]\n",
      "[Epoch 21|Step 362|Flag 2|Sparsity 0.6217|Params 49988]\n",
      "[Epoch 22|Step 284|Flag 2|Sparsity 0.6217|Params 49988]\n",
      "[Epoch 23|Step 206|Flag 2|Sparsity 0.6217|Params 49988]\n",
      "[Epoch 24|Step 128|Flag 2|Sparsity 0.6217|Params 49988]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.843962 | Logloss: 0.383285\n",
      "Evaluate Time 0.0118458 minutes\n",
      "[Epoch   25] train MF loss: 0.31057417, valid loss: 0inf, time 0.80670758 minutes\n",
      "********************************************************************************\n",
      "[Epoch 25|Step 50|Flag 3|Sparsity 0.6217|Params 49988]\n",
      "[Epoch 25|Step 550|Flag 3|Sparsity 0.6217|Params 49988]\n",
      "[Epoch 26|Step 472|Flag 3|Sparsity 0.6217|Params 49988]\n",
      "[Epoch 27|Step 394|Flag 3|Sparsity 0.6217|Params 49988]\n",
      "[Epoch 28|Step 316|Flag 3|Sparsity 0.6217|Params 49988]\n",
      "[Epoch 29|Step 238|Flag 3|Sparsity 0.6217|Params 49988]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.843629 | Logloss: 0.385022\n",
      "Evaluate Time 0.011556233333333334 minutes\n",
      "[Epoch   30] train MF loss: 0.28635228, valid loss: 0inf, time 0.81233488 minutes\n",
      "********************************************************************************\n",
      "[Epoch 30|Step 160|Flag 4|Sparsity 0.6217|Params 49988]\n",
      "[Epoch 31|Step 82|Flag 4|Sparsity 0.6217|Params 49988]\n",
      "[Epoch 32|Step 4|Flag 4|Sparsity 0.6217|Params 49988]\n",
      "[Epoch 32|Step 504|Flag 4|Sparsity 0.6217|Params 49988]\n",
      "[Epoch 33|Step 426|Flag 4|Sparsity 0.6217|Params 49988]\n",
      "[Epoch 34|Step 348|Flag 4|Sparsity 0.6217|Params 49988]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.843467 | Logloss: 0.387486\n",
      "Evaluate Time 0.011512633333333333 minutes\n",
      "[Epoch   35] train MF loss: 0.28006268, valid loss: 0inf, time 0.81245233 minutes\n",
      "********************************************************************************\n",
      "[Epoch 35|Step 270|Flag 5|Sparsity 0.6217|Params 49988]\n",
      "[Epoch 36|Step 192|Flag 5|Sparsity 0.6217|Params 49988]\n",
      "[Epoch 37|Step 114|Flag 5|Sparsity 0.6217|Params 49988]\n",
      "[Epoch 38|Step 36|Flag 5|Sparsity 0.6217|Params 49988]\n",
      "[Epoch 38|Step 536|Flag 5|Sparsity 0.6217|Params 49988]\n",
      "[Epoch 39|Step 458|Flag 5|Sparsity 0.6217|Params 49988]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.842722 | Logloss: 0.388029\n",
      "Evaluate Time 0.012469000000000001 minutes\n",
      "[Epoch   40] train MF loss: 0.30822119, valid loss: 0inf, time 0.88138305 minutes\n",
      "********************************************************************************\n",
      "[Epoch 40|Step 380|Flag 6|Sparsity 0.6217|Params 49988]\n",
      "[Epoch 41|Step 302|Flag 6|Sparsity 0.6217|Params 49988]\n",
      "[Epoch 42|Step 224|Flag 6|Sparsity 0.6217|Params 49988]\n",
      "[Epoch 43|Step 146|Flag 6|Sparsity 0.6217|Params 49988]\n",
      "[Epoch 44|Step 68|Flag 6|Sparsity 0.6217|Params 49988]\n",
      "[Epoch 44|Step 568|Flag 6|Sparsity 0.6217|Params 49988]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.842223 | Logloss: 0.390905\n",
      "Evaluate Time 0.012206466666666667 minutes\n",
      "[Epoch   45] train MF loss: 0.31183246, valid loss: 0inf, time 0.87658122 minutes\n",
      "********************************************************************************\n",
      "[Epoch 45|Step 490|Flag 7|Sparsity 0.6217|Params 49988]\n",
      "[Epoch 46|Step 412|Flag 7|Sparsity 0.6217|Params 49988]\n",
      "[Epoch 47|Step 334|Flag 7|Sparsity 0.6217|Params 49988]\n",
      "[Epoch 48|Step 256|Flag 7|Sparsity 0.6217|Params 49988]\n",
      "[Epoch 49|Step 178|Flag 7|Sparsity 0.6217|Params 49988]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.841941 | Logloss: 0.391904\n",
      "Evaluate Time 0.011641166666666668 minutes\n",
      "[Epoch   50] train MF loss: 0.30479479, valid loss: 0inf, time 1.03688090 minutes\n",
      "********************************************************************************\n",
      "[Epoch 50|Step 100|Flag 8|Sparsity 0.6217|Params 49988]\n"
     ]
    }
   ],
   "source": [
    "target_parameters = [20000, 30000, 49988]\n",
    "best_test_results_list = []\n",
    "for target_param in target_parameters:\n",
    "\n",
    "    print(f\"retrain for target parameters {target_param}\")\n",
    "    parser = setup_args()\n",
    "    parser.set_defaults(\n",
    "        alias='train',\n",
    "        tensorboard='./tmp/runs/{factorizer}/{data_type}',\n",
    "        ##########\n",
    "        ## data ##\n",
    "        ##########\n",
    "        data_type='ml-1m',\n",
    "        data_path='./data/{data_type}/',\n",
    "        load_in_queue=False,\n",
    "        category_only=False,\n",
    "        rebuild_cache=False,\n",
    "        eval_res_path='./tmp/res/{factorizer}/{data_type}/{alias}/{epoch_idx}.csv',\n",
    "        emb_save_path='./tmp/embedding/{factorizer}/{data_type}/{alias}/{num_parameter}',\n",
    "        ######################\n",
    "        ## train/test split ##\n",
    "        ######################\n",
    "        test_ratio=0.1,\n",
    "        valid_ratio=1/9,\n",
    "        ##########################\n",
    "        ## Devices & Efficiency ##\n",
    "        ##########################\n",
    "        use_cuda=True,\n",
    "        early_stop=40,\n",
    "        log_interval=1,\n",
    "        display_interval=500,\n",
    "        eval_interval=5,  # 10 epochs between 2 evaluations\n",
    "        device_ids_test=[0],\n",
    "        device_id=0,\n",
    "        batch_size_train=1024,\n",
    "        batch_size_valid=1024,\n",
    "        batch_size_test=1024,\n",
    "        ###########\n",
    "        ## Model ##\n",
    "        ###########\n",
    "        factorizer='fm',\n",
    "        model='deepfm',\n",
    "        fm_lr=1e-3,\n",
    "        # Deep\n",
    "        mlp_dims=[100, 100],\n",
    "        # AutoInt\n",
    "        has_residual=True,\n",
    "        full_part=True,\n",
    "        num_heads=2,\n",
    "        num_layers=3,\n",
    "        att_dropout=0.4,\n",
    "        atten_embed_dim=64,\n",
    "        # optimizer setting\n",
    "        fm_optimizer='adam',\n",
    "        fm_amsgrad=False,\n",
    "        fm_eps=1e-8,\n",
    "        fm_l2_regularization=1e-5,\n",
    "        fm_betas=(0.9, 0.999),\n",
    "        fm_grad_clip=100,  # 0.1\n",
    "        fm_lr_exp_decay=1,\n",
    "        l2_penalty=0,\n",
    "        #########\n",
    "        ## PEP ##\n",
    "        #########\n",
    "        latent_dim=32,\n",
    "        threshold_type='feature_dim',\n",
    "        g_type='sigmoid',\n",
    "        gk=1,\n",
    "        threshold_init=-15,\n",
    "        retrain_emb_param=target_param,\n",
    "        re_init=False,\n",
    "    )\n",
    "\n",
    "    opt = parser.parse_args(args=[])\n",
    "    opt = vars(opt)\n",
    "    opt['alias'] = '{}_{}_BaseDim{}_bsz{}_lr_{}_optim_{}_thresholdType{}_thres_init{}_{}-{}_l2_penalty{}'.format(\n",
    "        opt['model'].upper(),\n",
    "        opt['alias'],\n",
    "        opt['latent_dim'],\n",
    "        opt['batch_size_train'],\n",
    "        opt['fm_lr'],\n",
    "        opt['fm_optimizer'],\n",
    "        opt['threshold_type'].upper(),\n",
    "        opt['threshold_init'],\n",
    "        opt['g_type'],\n",
    "        opt['gk'],\n",
    "        opt['l2_penalty']\n",
    "    )\n",
    "    print(opt['alias'])\n",
    "    random.seed(opt['seed'])\n",
    "    # np.random.seed(opt['seed'])\n",
    "    torch.manual_seed(opt['seed'])\n",
    "    torch.cuda.manual_seed_all(opt['seed'])\n",
    "    engine = Engine(opt)\n",
    "    best_result = engine.train()\n",
    "\n",
    "    best_test_results_list.append(best_result)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Deepfm_PEP_retrain_20000_30000_49988 = {}\n",
    "for i, param in enumerate([20000, 30000, 49988]):\n",
    "    Deepfm_PEP_retrain_20000_30000_49988[param] =  best_test_results_list[i]\n",
    "\n",
    "pd.DataFrame(Deepfm_PEP_retrain_20000_30000_49988).to_csv(\"Deepfm_PEP_retrain[20000,30000,49988].csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train For AutoInt, MovieLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUTOINT_train_BaseDim32_bsz1024_lr_0.001_optim_adam_thresholdTypeFEATURE_DIM_thres_init-15_sigmoid-1_l2_penalty0\n",
      "Reconstructing ml-1m data from ./data/ml-1m/\n",
      "\tNum of train records: 591209\n",
      "\tNum of valid records: 73901\n",
      "\tNum of test records: 73902\n",
      "\tNum of fields: 7\n",
      "\tNum of features: 4129\n",
      "BackBone Embedding Parameters:  132128\n",
      "--------------------------------------------------------------------------------\n",
      "[complete episode  starts!]\n",
      "Initializing ...\n",
      "BackBone Embedding Parameters:  132128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pytorchEnv\\env\\Lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "Save the initial embedding table\n",
      "********************************************************************************\n",
      "[Epoch 0|Step 0|Flag 0|Sparsity 0.0000|Params 132127]\n",
      "[Epoch 0|Step 500|Flag 0|Sparsity 0.0056|Params 131385]\n",
      "[Epoch 1|Step 422|Flag 0|Sparsity 0.0057|Params 131378]\n",
      "[Epoch 2|Step 344|Flag 0|Sparsity 0.0057|Params 131372]\n",
      "[Epoch 3|Step 266|Flag 0|Sparsity 0.0058|Params 131368]\n",
      "[Epoch 4|Step 188|Flag 0|Sparsity 0.0058|Params 131360]\n",
      "[Epoch 5|Step 110|Flag 0|Sparsity 0.0059|Params 131353]\n",
      "[Epoch 6|Step 32|Flag 0|Sparsity 0.0060|Params 131330]\n",
      "[Epoch 6|Step 532|Flag 0|Sparsity 0.0062|Params 131303]\n",
      "[Epoch 7|Step 454|Flag 0|Sparsity 0.0067|Params 131249]\n",
      "[Epoch 8|Step 376|Flag 0|Sparsity 0.0077|Params 131112]\n",
      "[Epoch 9|Step 298|Flag 0|Sparsity 0.0098|Params 130828]\n",
      "[Epoch 10|Step 220|Flag 0|Sparsity 0.0139|Params 130285]\n",
      "[Epoch 11|Step 142|Flag 0|Sparsity 0.0207|Params 129389]\n",
      "[Epoch 12|Step 64|Flag 0|Sparsity 0.0343|Params 127602]\n",
      "[Epoch 12|Step 564|Flag 0|Sparsity 0.0558|Params 124760]\n",
      "[Epoch 13|Step 486|Flag 0|Sparsity 0.0888|Params 120399]\n",
      "[Epoch 14|Step 408|Flag 0|Sparsity 0.1283|Params 115176]\n",
      "[Epoch 15|Step 330|Flag 0|Sparsity 0.1670|Params 110058]\n",
      "[Epoch 16|Step 252|Flag 0|Sparsity 0.2015|Params 105501]\n",
      "[Epoch 17|Step 174|Flag 0|Sparsity 0.2307|Params 101644]\n",
      "[Epoch 18|Step 96|Flag 0|Sparsity 0.2561|Params 98293]\n",
      "[Epoch 19|Step 18|Flag 0|Sparsity 0.2801|Params 95114]\n",
      "[Epoch 19|Step 518|Flag 0|Sparsity 0.3029|Params 92104]\n",
      "[Epoch 20|Step 440|Flag 0|Sparsity 0.3288|Params 88689]\n",
      "[Epoch 21|Step 362|Flag 0|Sparsity 0.3583|Params 84788]\n",
      "[Epoch 22|Step 284|Flag 0|Sparsity 0.3947|Params 79974]\n",
      "[Epoch 23|Step 206|Flag 0|Sparsity 0.4429|Params 73610]\n",
      "[Epoch 24|Step 128|Flag 0|Sparsity 0.5013|Params 65891]\n",
      "[Epoch 25|Step 50|Flag 0|Sparsity 0.5651|Params 57460]\n",
      "********************************************************************************\n",
      "Reach the target parameter: 50000, save embedding with size: 49997\n",
      "********************************************************************************\n",
      "[Epoch 25|Step 550|Flag 0|Sparsity 0.6242|Params 49651]\n",
      "[Epoch 26|Step 472|Flag 0|Sparsity 0.6706|Params 43519]\n",
      "[Epoch 27|Step 394|Flag 0|Sparsity 0.7047|Params 39016]\n",
      "[Epoch 28|Step 316|Flag 0|Sparsity 0.7279|Params 35947]\n",
      "[Epoch 29|Step 238|Flag 0|Sparsity 0.7450|Params 33698]\n",
      "[Epoch 30|Step 160|Flag 0|Sparsity 0.7578|Params 31999]\n",
      "[Epoch 31|Step 82|Flag 0|Sparsity 0.7679|Params 30665]\n",
      "********************************************************************************\n",
      "Reach the target parameter: 30000, save embedding with size: 29999\n",
      "********************************************************************************\n",
      "[Epoch 32|Step 4|Flag 0|Sparsity 0.7767|Params 29507]\n",
      "[Epoch 32|Step 504|Flag 0|Sparsity 0.7839|Params 28548]\n",
      "[Epoch 33|Step 426|Flag 0|Sparsity 0.7899|Params 27754]\n",
      "[Epoch 34|Step 348|Flag 0|Sparsity 0.7952|Params 27065]\n",
      "[Epoch 35|Step 270|Flag 0|Sparsity 0.7999|Params 26437]\n",
      "[Epoch 36|Step 192|Flag 0|Sparsity 0.8041|Params 25888]\n",
      "[Epoch 37|Step 114|Flag 0|Sparsity 0.8076|Params 25415]\n",
      "[Epoch 38|Step 36|Flag 0|Sparsity 0.8107|Params 25010]\n",
      "[Epoch 38|Step 536|Flag 0|Sparsity 0.8140|Params 24573]\n",
      "[Epoch 39|Step 458|Flag 0|Sparsity 0.8170|Params 24180]\n",
      "[Epoch 40|Step 380|Flag 0|Sparsity 0.8197|Params 23820]\n",
      "[Epoch 41|Step 302|Flag 0|Sparsity 0.8221|Params 23507]\n",
      "[Epoch 42|Step 224|Flag 0|Sparsity 0.8239|Params 23274]\n",
      "[Epoch 43|Step 146|Flag 0|Sparsity 0.8258|Params 23015]\n",
      "[Epoch 44|Step 68|Flag 0|Sparsity 0.8277|Params 22763]\n",
      "[Epoch 44|Step 568|Flag 0|Sparsity 0.8295|Params 22526]\n",
      "[Epoch 45|Step 490|Flag 0|Sparsity 0.8314|Params 22271]\n",
      "[Epoch 46|Step 412|Flag 0|Sparsity 0.8328|Params 22088]\n",
      "[Epoch 47|Step 334|Flag 0|Sparsity 0.8343|Params 21897]\n",
      "[Epoch 48|Step 256|Flag 0|Sparsity 0.8354|Params 21747]\n",
      "[Epoch 49|Step 178|Flag 0|Sparsity 0.8366|Params 21589]\n",
      "[Epoch 50|Step 100|Flag 0|Sparsity 0.8376|Params 21457]\n",
      "[Epoch 51|Step 22|Flag 0|Sparsity 0.8387|Params 21307]\n",
      "[Epoch 51|Step 522|Flag 0|Sparsity 0.8400|Params 21147]\n",
      "[Epoch 52|Step 444|Flag 0|Sparsity 0.8410|Params 21003]\n",
      "[Epoch 53|Step 366|Flag 0|Sparsity 0.8420|Params 20881]\n",
      "[Epoch 54|Step 288|Flag 0|Sparsity 0.8429|Params 20763]\n",
      "[Epoch 55|Step 210|Flag 0|Sparsity 0.8438|Params 20637]\n",
      "[Epoch 56|Step 132|Flag 0|Sparsity 0.8445|Params 20543]\n",
      "[Epoch 57|Step 54|Flag 0|Sparsity 0.8454|Params 20427]\n",
      "[Epoch 57|Step 554|Flag 0|Sparsity 0.8462|Params 20318]\n",
      "[Epoch 58|Step 476|Flag 0|Sparsity 0.8469|Params 20225]\n",
      "[Epoch 59|Step 398|Flag 0|Sparsity 0.8476|Params 20139]\n",
      "[Epoch 60|Step 320|Flag 0|Sparsity 0.8484|Params 20037]\n",
      "********************************************************************************\n",
      "Reach the target parameter: 20000, save embedding with size: 20000\n",
      "********************************************************************************\n",
      "Minimal target parameters achieved, stop pruning.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AUC': [0, 0], 'LogLoss': [inf, 0]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "parser = setup_args()\n",
    "parser.set_defaults(\n",
    "    alias='train',\n",
    "    tensorboard='./tmp/runs/{factorizer}/{data_type}',\n",
    "    ##########\n",
    "    ## data ##\n",
    "    ##########\n",
    "    data_type='ml-1m',\n",
    "    data_path='./data/{data_type}/',\n",
    "    load_in_queue=False,\n",
    "    category_only=False,\n",
    "    rebuild_cache=False,\n",
    "    eval_res_path='./tmp/res/{factorizer}/{data_type}/{alias}/{epoch_idx}.csv',\n",
    "    emb_save_path='./tmp/embedding/{factorizer}/{data_type}/{alias}/{num_parameter}',\n",
    "    ######################\n",
    "    ## train/test split ##\n",
    "    ######################\n",
    "    test_ratio=0.1,\n",
    "    valid_ratio=1/9,\n",
    "    ##########################\n",
    "    ## Devices & Efficiency ##\n",
    "    ##########################\n",
    "    use_cuda=True,\n",
    "    early_stop=10,\n",
    "    log_interval=1,\n",
    "    display_interval=500,\n",
    "    eval_interval=5,  # 10 epochs between 2 evaluations\n",
    "    device_ids_test=[0],\n",
    "    device_id=0,\n",
    "    batch_size_train=1024,\n",
    "    batch_size_valid=1024,\n",
    "    batch_size_test=1024,\n",
    "    ###########\n",
    "    ## Model ##\n",
    "    ###########\n",
    "    factorizer='fm',\n",
    "    model='autoint',\n",
    "    fm_lr=1e-3,\n",
    "    # Deep\n",
    "    mlp_dims=[100, 100],\n",
    "    # AutoInt\n",
    "    has_residual=True,\n",
    "    full_part=True,\n",
    "    num_heads=2,\n",
    "    num_layers=3,\n",
    "    att_dropout=0.4,\n",
    "    atten_embed_dim=64,\n",
    "    # optimizer setting\n",
    "    fm_optimizer='adam',\n",
    "    fm_amsgrad=False,\n",
    "    fm_eps=1e-8,\n",
    "    fm_l2_regularization=1e-5,\n",
    "    fm_betas=(0.9, 0.999),\n",
    "    fm_grad_clip=100,  # 0.1\n",
    "    fm_lr_exp_decay=1,\n",
    "    l2_penalty=0,\n",
    "    #########\n",
    "    ## Embeddings//PEP ##\n",
    "    #########\n",
    "    latent_dim=32,\n",
    "    threshold_type='feature_dim',\n",
    "    g_type='sigmoid',\n",
    "    gk=1,\n",
    "    threshold_init=-15,\n",
    "    candidate_p=[50000, 30000, 20000],\n",
    ")\n",
    "\n",
    "opt = parser.parse_args(args=[])\n",
    "opt = vars(opt)\n",
    "\n",
    "# rename alias\n",
    "\n",
    "opt['alias'] = '{}_{}_BaseDim{}_bsz{}_lr_{}_optim_{}_thresholdType{}_thres_init{}_{}-{}_l2_penalty{}'.format(\n",
    "    opt['model'].upper(),\n",
    "    opt['alias'],\n",
    "    opt['latent_dim'],\n",
    "    opt['batch_size_train'],\n",
    "    opt['fm_lr'],\n",
    "    opt['fm_optimizer'],\n",
    "    opt['threshold_type'].upper(),\n",
    "    opt['threshold_init'],\n",
    "    opt['g_type'],\n",
    "    opt['gk'],\n",
    "    opt['l2_penalty']\n",
    ")\n",
    "print(opt['alias'])\n",
    "random.seed(opt['seed'])\n",
    "# np.random.seed(opt['seed'])\n",
    "torch.manual_seed(opt['seed'])\n",
    "torch.cuda.manual_seed_all(opt['seed'])\n",
    "engine = Engine(opt)\n",
    "engine.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrain AutoInt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrain for target parameters 20000\n",
      "AUTOINT_train_BaseDim32_bsz1024_lr_0.001_optim_adam_thresholdTypeFEATURE_DIM_thres_init-15_sigmoid-1_l2_penalty0\n",
      "Reconstructing ml-1m data from ./data/ml-1m/\n",
      "\tNum of train records: 591209\n",
      "\tNum of valid records: 73901\n",
      "\tNum of test records: 73902\n",
      "\tNum of fields: 7\n",
      "\tNum of features: 4129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pytorchEnv\\env\\Lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrain epoch 20000\n",
      "BackBone Embedding Parameters:  132128\n",
      "--------------------------------------------------------------------------------\n",
      "[complete episode  starts!]\n",
      "Initializing ...\n",
      "Retrain epoch 20000\n",
      "BackBone Embedding Parameters:  132128\n",
      "[Epoch 0|Step 0|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.479325 | Logloss: 0.692462\n",
      "Evaluate Time 0.018474833333333333 minutes\n",
      "[Epoch    0] train MF loss: 0.71756637, valid loss: 0inf, time 0.02101397 minutes\n",
      "********************************************************************************\n",
      "[Epoch 0|Step 500|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 1|Step 422|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 2|Step 344|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 3|Step 266|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 4|Step 188|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.844847 | Logloss: 0.376766\n",
      "Evaluate Time 0.022761416666666666 minutes\n",
      "[Epoch    5] train MF loss: 0.37368011, valid loss: 0inf, time 1.74884573 minutes\n",
      "********************************************************************************\n",
      "[Epoch 5|Step 110|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 6|Step 32|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 6|Step 532|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 7|Step 454|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 8|Step 376|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 9|Step 298|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.847143 | Logloss: 0.375313\n",
      "Evaluate Time 0.021830433333333333 minutes\n",
      "[Epoch   10] train MF loss: 0.34421328, valid loss: 0inf, time 1.95791480 minutes\n",
      "********************************************************************************\n",
      "[Epoch 10|Step 220|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 11|Step 142|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 12|Step 64|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 12|Step 564|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 13|Step 486|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 14|Step 408|Flag 0|Sparsity 0.8486|Params 20000]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.846367 | Logloss: 0.377715\n",
      "Evaluate Time 0.0250521 minutes\n",
      "[Epoch   15] train MF loss: 0.33308959, valid loss: 0inf, time 1.72455695 minutes\n",
      "********************************************************************************\n",
      "[Epoch 15|Step 330|Flag 1|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 16|Step 252|Flag 1|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 17|Step 174|Flag 1|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 18|Step 96|Flag 1|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 19|Step 18|Flag 1|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 19|Step 518|Flag 1|Sparsity 0.8486|Params 20000]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.845859 | Logloss: 0.377685\n",
      "Evaluate Time 0.014295716666666668 minutes\n",
      "[Epoch   20] train MF loss: 0.32046446, valid loss: 0inf, time 1.15017362 minutes\n",
      "********************************************************************************\n",
      "[Epoch 20|Step 440|Flag 2|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 21|Step 362|Flag 2|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 22|Step 284|Flag 2|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 23|Step 206|Flag 2|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 24|Step 128|Flag 2|Sparsity 0.8486|Params 20000]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.846227 | Logloss: 0.378300\n",
      "Evaluate Time 0.012643816666666667 minutes\n",
      "[Epoch   25] train MF loss: 0.32578236, valid loss: 0inf, time 1.12272583 minutes\n",
      "********************************************************************************\n",
      "[Epoch 25|Step 50|Flag 3|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 25|Step 550|Flag 3|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 26|Step 472|Flag 3|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 27|Step 394|Flag 3|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 28|Step 316|Flag 3|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 29|Step 238|Flag 3|Sparsity 0.8486|Params 20000]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.845731 | Logloss: 0.380429\n",
      "Evaluate Time 0.013719733333333334 minutes\n",
      "[Epoch   30] train MF loss: 0.33136559, valid loss: 0inf, time 1.13054657 minutes\n",
      "********************************************************************************\n",
      "[Epoch 30|Step 160|Flag 4|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 31|Step 82|Flag 4|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 32|Step 4|Flag 4|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 32|Step 504|Flag 4|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 33|Step 426|Flag 4|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 34|Step 348|Flag 4|Sparsity 0.8486|Params 20000]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.845970 | Logloss: 0.379208\n",
      "Evaluate Time 0.0157894 minutes\n",
      "[Epoch   35] train MF loss: 0.32248801, valid loss: 0inf, time 1.25632048 minutes\n",
      "********************************************************************************\n",
      "[Epoch 35|Step 270|Flag 5|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 36|Step 192|Flag 5|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 37|Step 114|Flag 5|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 38|Step 36|Flag 5|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 38|Step 536|Flag 5|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 39|Step 458|Flag 5|Sparsity 0.8486|Params 20000]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.845733 | Logloss: 0.381183\n",
      "Evaluate Time 0.02106795 minutes\n",
      "[Epoch   40] train MF loss: 0.33083618, valid loss: 0inf, time 1.59281575 minutes\n",
      "********************************************************************************\n",
      "[Epoch 40|Step 380|Flag 6|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 41|Step 302|Flag 6|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 42|Step 224|Flag 6|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 43|Step 146|Flag 6|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 44|Step 68|Flag 6|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 44|Step 568|Flag 6|Sparsity 0.8486|Params 20000]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.844156 | Logloss: 0.382059\n",
      "Evaluate Time 0.020105133333333334 minutes\n",
      "[Epoch   45] train MF loss: 0.32751358, valid loss: 0inf, time 1.79521360 minutes\n",
      "********************************************************************************\n",
      "[Epoch 45|Step 490|Flag 7|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 46|Step 412|Flag 7|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 47|Step 334|Flag 7|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 48|Step 256|Flag 7|Sparsity 0.8486|Params 20000]\n",
      "[Epoch 49|Step 178|Flag 7|Sparsity 0.8486|Params 20000]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.844844 | Logloss: 0.382068\n",
      "Evaluate Time 0.02166405 minutes\n",
      "[Epoch   50] train MF loss: 0.30645850, valid loss: 0inf, time 1.86711903 minutes\n",
      "********************************************************************************\n",
      "[Epoch 50|Step 100|Flag 8|Sparsity 0.8486|Params 20000]\n",
      "retrain for target parameters 29999\n",
      "AUTOINT_train_BaseDim32_bsz1024_lr_0.001_optim_adam_thresholdTypeFEATURE_DIM_thres_init-15_sigmoid-1_l2_penalty0\n",
      "Reconstructing ml-1m data from ./data/ml-1m/\n",
      "\tNum of train records: 591209\n",
      "\tNum of valid records: 73901\n",
      "\tNum of test records: 73902\n",
      "\tNum of fields: 7\n",
      "\tNum of features: 4129\n",
      "Retrain epoch 29999\n",
      "BackBone Embedding Parameters:  132128\n",
      "--------------------------------------------------------------------------------\n",
      "[complete episode  starts!]\n",
      "Initializing ...\n",
      "Retrain epoch 29999\n",
      "BackBone Embedding Parameters:  132128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pytorchEnv\\env\\Lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0|Step 0|Flag 0|Sparsity 0.7730|Params 29999]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.479678 | Logloss: 0.692123\n",
      "Evaluate Time 0.028046750000000002 minutes\n",
      "[Epoch    0] train MF loss: 0.71821302, valid loss: 0inf, time 0.03051143 minutes\n",
      "********************************************************************************\n",
      "[Epoch 0|Step 500|Flag 0|Sparsity 0.7730|Params 29999]\n",
      "[Epoch 1|Step 422|Flag 0|Sparsity 0.7730|Params 29999]\n",
      "[Epoch 2|Step 344|Flag 0|Sparsity 0.7730|Params 29999]\n",
      "[Epoch 3|Step 266|Flag 0|Sparsity 0.7730|Params 29999]\n",
      "[Epoch 4|Step 188|Flag 0|Sparsity 0.7730|Params 29999]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.845806 | Logloss: 0.376244\n",
      "Evaluate Time 0.0306454 minutes\n",
      "[Epoch    5] train MF loss: 0.37380260, valid loss: 0inf, time 2.79405922 minutes\n",
      "********************************************************************************\n",
      "[Epoch 5|Step 110|Flag 0|Sparsity 0.7730|Params 29999]\n",
      "[Epoch 6|Step 32|Flag 0|Sparsity 0.7730|Params 29999]\n",
      "[Epoch 6|Step 532|Flag 0|Sparsity 0.7730|Params 29999]\n",
      "[Epoch 7|Step 454|Flag 0|Sparsity 0.7730|Params 29999]\n",
      "[Epoch 8|Step 376|Flag 0|Sparsity 0.7730|Params 29999]\n",
      "[Epoch 9|Step 298|Flag 0|Sparsity 0.7730|Params 29999]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.845807 | Logloss: 0.377184\n",
      "Evaluate Time 0.03154643333333333 minutes\n",
      "[Epoch   10] train MF loss: 0.32658395, valid loss: 0inf, time 3.34377555 minutes\n",
      "********************************************************************************\n",
      "[Epoch 10|Step 220|Flag 0|Sparsity 0.7730|Params 29999]\n",
      "[Epoch 11|Step 142|Flag 0|Sparsity 0.7730|Params 29999]\n",
      "[Epoch 12|Step 64|Flag 0|Sparsity 0.7730|Params 29999]\n",
      "[Epoch 12|Step 564|Flag 0|Sparsity 0.7730|Params 29999]\n",
      "[Epoch 13|Step 486|Flag 0|Sparsity 0.7730|Params 29999]\n",
      "[Epoch 14|Step 408|Flag 0|Sparsity 0.7730|Params 29999]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.846342 | Logloss: 0.379540\n",
      "Evaluate Time 0.015958366666666664 minutes\n",
      "[Epoch   15] train MF loss: 0.33199799, valid loss: 0inf, time 2.93350168 minutes\n",
      "********************************************************************************\n",
      "[Epoch 15|Step 330|Flag 0|Sparsity 0.7730|Params 29999]\n",
      "[Epoch 16|Step 252|Flag 0|Sparsity 0.7730|Params 29999]\n",
      "[Epoch 17|Step 174|Flag 0|Sparsity 0.7730|Params 29999]\n",
      "[Epoch 18|Step 96|Flag 0|Sparsity 0.7730|Params 29999]\n",
      "[Epoch 19|Step 18|Flag 0|Sparsity 0.7730|Params 29999]\n",
      "[Epoch 19|Step 518|Flag 0|Sparsity 0.7730|Params 29999]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.846123 | Logloss: 0.379915\n",
      "Evaluate Time 0.015164216666666668 minutes\n",
      "[Epoch   20] train MF loss: 0.30869806, valid loss: 0inf, time 1.31692707 minutes\n",
      "********************************************************************************\n",
      "[Epoch 20|Step 440|Flag 1|Sparsity 0.7730|Params 29999]\n",
      "[Epoch 21|Step 362|Flag 1|Sparsity 0.7730|Params 29999]\n",
      "[Epoch 22|Step 284|Flag 1|Sparsity 0.7730|Params 29999]\n",
      "[Epoch 23|Step 206|Flag 1|Sparsity 0.7730|Params 29999]\n",
      "[Epoch 24|Step 128|Flag 1|Sparsity 0.7730|Params 29999]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.845635 | Logloss: 0.381452\n",
      "Evaluate Time 0.014089516666666666 minutes\n",
      "[Epoch   25] train MF loss: 0.31473112, valid loss: 0inf, time 1.20468437 minutes\n",
      "********************************************************************************\n",
      "[Epoch 25|Step 50|Flag 2|Sparsity 0.7730|Params 29999]\n",
      "[Epoch 25|Step 550|Flag 2|Sparsity 0.7730|Params 29999]\n",
      "[Epoch 26|Step 472|Flag 2|Sparsity 0.7730|Params 29999]\n",
      "[Epoch 27|Step 394|Flag 2|Sparsity 0.7730|Params 29999]\n",
      "[Epoch 28|Step 316|Flag 2|Sparsity 0.7730|Params 29999]\n",
      "[Epoch 29|Step 238|Flag 2|Sparsity 0.7730|Params 29999]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.845029 | Logloss: 0.384134\n",
      "Evaluate Time 0.013656466666666667 minutes\n",
      "[Epoch   30] train MF loss: 0.31697688, valid loss: 0inf, time 1.17665415 minutes\n",
      "********************************************************************************\n",
      "[Epoch 30|Step 160|Flag 3|Sparsity 0.7730|Params 29999]\n",
      "[Epoch 31|Step 82|Flag 3|Sparsity 0.7730|Params 29999]\n",
      "[Epoch 32|Step 4|Flag 3|Sparsity 0.7730|Params 29999]\n",
      "[Epoch 32|Step 504|Flag 3|Sparsity 0.7730|Params 29999]\n",
      "[Epoch 33|Step 426|Flag 3|Sparsity 0.7730|Params 29999]\n",
      "[Epoch 34|Step 348|Flag 3|Sparsity 0.7730|Params 29999]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.845110 | Logloss: 0.383590\n",
      "Evaluate Time 0.013554983333333333 minutes\n",
      "[Epoch   35] train MF loss: 0.29405218, valid loss: 0inf, time 1.16924700 minutes\n",
      "********************************************************************************\n",
      "[Epoch 35|Step 270|Flag 4|Sparsity 0.7730|Params 29999]\n",
      "[Epoch 36|Step 192|Flag 4|Sparsity 0.7730|Params 29999]\n",
      "[Epoch 37|Step 114|Flag 4|Sparsity 0.7730|Params 29999]\n",
      "[Epoch 38|Step 36|Flag 4|Sparsity 0.7730|Params 29999]\n",
      "[Epoch 38|Step 536|Flag 4|Sparsity 0.7730|Params 29999]\n",
      "[Epoch 39|Step 458|Flag 4|Sparsity 0.7730|Params 29999]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.844144 | Logloss: 0.386514\n",
      "Evaluate Time 0.014118849999999999 minutes\n",
      "[Epoch   40] train MF loss: 0.32116362, valid loss: 0inf, time 1.17580832 minutes\n",
      "********************************************************************************\n",
      "[Epoch 40|Step 380|Flag 5|Sparsity 0.7730|Params 29999]\n",
      "[Epoch 41|Step 302|Flag 5|Sparsity 0.7730|Params 29999]\n",
      "[Epoch 42|Step 224|Flag 5|Sparsity 0.7730|Params 29999]\n",
      "[Epoch 43|Step 146|Flag 5|Sparsity 0.7730|Params 29999]\n",
      "[Epoch 44|Step 68|Flag 5|Sparsity 0.7730|Params 29999]\n",
      "[Epoch 44|Step 568|Flag 5|Sparsity 0.7730|Params 29999]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.843691 | Logloss: 0.385549\n",
      "Evaluate Time 0.013526483333333334 minutes\n",
      "[Epoch   45] train MF loss: 0.33316758, valid loss: 0inf, time 1.17864482 minutes\n",
      "********************************************************************************\n",
      "[Epoch 45|Step 490|Flag 6|Sparsity 0.7730|Params 29999]\n",
      "[Epoch 46|Step 412|Flag 6|Sparsity 0.7730|Params 29999]\n",
      "[Epoch 47|Step 334|Flag 6|Sparsity 0.7730|Params 29999]\n",
      "[Epoch 48|Step 256|Flag 6|Sparsity 0.7730|Params 29999]\n",
      "[Epoch 49|Step 178|Flag 6|Sparsity 0.7730|Params 29999]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.843544 | Logloss: 0.386871\n",
      "Evaluate Time 0.013579616666666666 minutes\n",
      "[Epoch   50] train MF loss: 0.29372442, valid loss: 0inf, time 1.18936815 minutes\n",
      "********************************************************************************\n",
      "[Epoch 50|Step 100|Flag 7|Sparsity 0.7730|Params 29999]\n",
      "retrain for target parameters 49997\n",
      "AUTOINT_train_BaseDim32_bsz1024_lr_0.001_optim_adam_thresholdTypeFEATURE_DIM_thres_init-15_sigmoid-1_l2_penalty0\n",
      "Reconstructing ml-1m data from ./data/ml-1m/\n",
      "\tNum of train records: 591209\n",
      "\tNum of valid records: 73901\n",
      "\tNum of test records: 73902\n",
      "\tNum of fields: 7\n",
      "\tNum of features: 4129\n",
      "Retrain epoch 49997\n",
      "BackBone Embedding Parameters:  132128\n",
      "--------------------------------------------------------------------------------\n",
      "[complete episode  starts!]\n",
      "Initializing ...\n",
      "Retrain epoch 49997\n",
      "BackBone Embedding Parameters:  132128\n",
      "[Epoch 0|Step 0|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "Evaluate on test ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pytorchEnv\\env\\Lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "Test AUC: 0.482694 | Logloss: 0.692206\n",
      "Evaluate Time 0.017897383333333336 minutes\n",
      "[Epoch    0] train MF loss: 0.71574235, valid loss: 0inf, time 0.01971242 minutes\n",
      "********************************************************************************\n",
      "[Epoch 0|Step 500|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 1|Step 422|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 2|Step 344|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 3|Step 266|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 4|Step 188|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.845665 | Logloss: 0.376828\n",
      "Evaluate Time 0.014397650000000001 minutes\n",
      "[Epoch    5] train MF loss: 0.36456934, valid loss: 0inf, time 1.21206862 minutes\n",
      "********************************************************************************\n",
      "[Epoch 5|Step 110|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 6|Step 32|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 6|Step 532|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 7|Step 454|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 8|Step 376|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 9|Step 298|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.846170 | Logloss: 0.378070\n",
      "Evaluate Time 0.013468766666666666 minutes\n",
      "[Epoch   10] train MF loss: 0.31867206, valid loss: 0inf, time 1.16778247 minutes\n",
      "********************************************************************************\n",
      "[Epoch 10|Step 220|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 11|Step 142|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 12|Step 64|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 12|Step 564|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 13|Step 486|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 14|Step 408|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.846554 | Logloss: 0.381396\n",
      "Evaluate Time 0.0133799 minutes\n",
      "[Epoch   15] train MF loss: 0.32145062, valid loss: 0inf, time 1.16051842 minutes\n",
      "********************************************************************************\n",
      "[Epoch 15|Step 330|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 16|Step 252|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 17|Step 174|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 18|Step 96|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 19|Step 18|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 19|Step 518|Flag 0|Sparsity 0.6216|Params 49997]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.845746 | Logloss: 0.382985\n",
      "Evaluate Time 0.013459466666666666 minutes\n",
      "[Epoch   20] train MF loss: 0.30641031, valid loss: 0inf, time 1.15172472 minutes\n",
      "********************************************************************************\n",
      "[Epoch 20|Step 440|Flag 1|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 21|Step 362|Flag 1|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 22|Step 284|Flag 1|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 23|Step 206|Flag 1|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 24|Step 128|Flag 1|Sparsity 0.6216|Params 49997]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.844615 | Logloss: 0.386367\n",
      "Evaluate Time 0.013496516666666668 minutes\n",
      "[Epoch   25] train MF loss: 0.30309883, valid loss: 0inf, time 1.12450115 minutes\n",
      "********************************************************************************\n",
      "[Epoch 25|Step 50|Flag 2|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 25|Step 550|Flag 2|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 26|Step 472|Flag 2|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 27|Step 394|Flag 2|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 28|Step 316|Flag 2|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 29|Step 238|Flag 2|Sparsity 0.6216|Params 49997]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.843776 | Logloss: 0.390234\n",
      "Evaluate Time 0.012994183333333333 minutes\n",
      "[Epoch   30] train MF loss: 0.30377346, valid loss: 0inf, time 1.11183455 minutes\n",
      "********************************************************************************\n",
      "[Epoch 30|Step 160|Flag 3|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 31|Step 82|Flag 3|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 32|Step 4|Flag 3|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 32|Step 504|Flag 3|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 33|Step 426|Flag 3|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 34|Step 348|Flag 3|Sparsity 0.6216|Params 49997]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.843073 | Logloss: 0.390912\n",
      "Evaluate Time 0.012646233333333333 minutes\n",
      "[Epoch   35] train MF loss: 0.29117286, valid loss: 0inf, time 1.12549853 minutes\n",
      "********************************************************************************\n",
      "[Epoch 35|Step 270|Flag 4|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 36|Step 192|Flag 4|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 37|Step 114|Flag 4|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 38|Step 36|Flag 4|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 38|Step 536|Flag 4|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 39|Step 458|Flag 4|Sparsity 0.6216|Params 49997]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.842557 | Logloss: 0.394163\n",
      "Evaluate Time 0.0134816 minutes\n",
      "[Epoch   40] train MF loss: 0.30421820, valid loss: 0inf, time 1.11298758 minutes\n",
      "********************************************************************************\n",
      "[Epoch 40|Step 380|Flag 5|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 41|Step 302|Flag 5|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 42|Step 224|Flag 5|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 43|Step 146|Flag 5|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 44|Step 68|Flag 5|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 44|Step 568|Flag 5|Sparsity 0.6216|Params 49997]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.842506 | Logloss: 0.393060\n",
      "Evaluate Time 0.014567666666666666 minutes\n",
      "[Epoch   45] train MF loss: 0.33345014, valid loss: 0inf, time 1.10236518 minutes\n",
      "********************************************************************************\n",
      "[Epoch 45|Step 490|Flag 6|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 46|Step 412|Flag 6|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 47|Step 334|Flag 6|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 48|Step 256|Flag 6|Sparsity 0.6216|Params 49997]\n",
      "[Epoch 49|Step 178|Flag 6|Sparsity 0.6216|Params 49997]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.842638 | Logloss: 0.394062\n",
      "Evaluate Time 0.013072566666666667 minutes\n",
      "[Epoch   50] train MF loss: 0.27280155, valid loss: 0inf, time 1.10366567 minutes\n",
      "********************************************************************************\n",
      "[Epoch 50|Step 100|Flag 7|Sparsity 0.6216|Params 49997]\n"
     ]
    }
   ],
   "source": [
    "target_parameters = [20000, 29999, 49997]\n",
    "best_test_results_list = []\n",
    "for target_param in target_parameters:\n",
    "\n",
    "    print(f\"retrain for target parameters {target_param}\")\n",
    "    parser = setup_args()\n",
    "    parser.set_defaults(\n",
    "        alias='train',\n",
    "        tensorboard='./tmp/runs/{factorizer}/{data_type}',\n",
    "        ##########\n",
    "        ## data ##\n",
    "        ##########\n",
    "        data_type='ml-1m',\n",
    "        data_path='./data/{data_type}/',\n",
    "        load_in_queue=False,\n",
    "        category_only=False,\n",
    "        rebuild_cache=False,\n",
    "        eval_res_path='./tmp/res/{factorizer}/{data_type}/{alias}/{epoch_idx}.csv',\n",
    "        emb_save_path='./tmp/embedding/{factorizer}/{data_type}/{alias}/{num_parameter}',\n",
    "        ######################\n",
    "        ## train/test split ##\n",
    "        ######################\n",
    "        test_ratio=0.1,\n",
    "        valid_ratio=1/9,\n",
    "        ##########################\n",
    "        ## Devices & Efficiency ##\n",
    "        ##########################\n",
    "        use_cuda=True,\n",
    "        early_stop=40,\n",
    "        log_interval=1,\n",
    "        display_interval=500,\n",
    "        eval_interval=5,  # 10 epochs between 2 evaluations\n",
    "        device_ids_test=[0],\n",
    "        device_id=0,\n",
    "        batch_size_train=1024,\n",
    "        batch_size_valid=1024,\n",
    "        batch_size_test=1024,\n",
    "        ###########\n",
    "        ## Model ##\n",
    "        ###########\n",
    "        factorizer='fm',\n",
    "        model='autoint',\n",
    "        fm_lr=1e-3,\n",
    "        # Deep\n",
    "        mlp_dims=[100, 100],\n",
    "        # AutoInt\n",
    "        has_residual=True,\n",
    "        full_part=True,\n",
    "        num_heads=2,\n",
    "        num_layers=3,\n",
    "        att_dropout=0.4,\n",
    "        atten_embed_dim=64,\n",
    "        # optimizer setting\n",
    "        fm_optimizer='adam',\n",
    "        fm_amsgrad=False,\n",
    "        fm_eps=1e-8,\n",
    "        fm_l2_regularization=1e-5,\n",
    "        fm_betas=(0.9, 0.999),\n",
    "        fm_grad_clip=100,  # 0.1\n",
    "        fm_lr_exp_decay=1,\n",
    "        l2_penalty=0,\n",
    "        #########\n",
    "        ## PEP ##\n",
    "        #########\n",
    "        latent_dim=32,\n",
    "        threshold_type='feature_dim',\n",
    "        g_type='sigmoid',\n",
    "        gk=1,\n",
    "        threshold_init=-15,\n",
    "        retrain_emb_param=target_param,\n",
    "        re_init=False,\n",
    "    )\n",
    "\n",
    "    opt = parser.parse_args(args=[])\n",
    "    opt = vars(opt)\n",
    "    opt['alias'] = '{}_{}_BaseDim{}_bsz{}_lr_{}_optim_{}_thresholdType{}_thres_init{}_{}-{}_l2_penalty{}'.format(\n",
    "        opt['model'].upper(),\n",
    "        opt['alias'],\n",
    "        opt['latent_dim'],\n",
    "        opt['batch_size_train'],\n",
    "        opt['fm_lr'],\n",
    "        opt['fm_optimizer'],\n",
    "        opt['threshold_type'].upper(),\n",
    "        opt['threshold_init'],\n",
    "        opt['g_type'],\n",
    "        opt['gk'],\n",
    "        opt['l2_penalty']\n",
    "    )\n",
    "    print(opt['alias'])\n",
    "    random.seed(opt['seed'])\n",
    "    # np.random.seed(opt['seed'])\n",
    "    torch.manual_seed(opt['seed'])\n",
    "    torch.cuda.manual_seed_all(opt['seed'])\n",
    "    engine = Engine(opt)\n",
    "    best_result = engine.train()\n",
    "\n",
    "    best_test_results_list.append(best_result)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "AutoInt_PEP_retrain_20000_29999_49997 = {}\n",
    "for i, param in enumerate([20000, 29999, 49997]):\n",
    "    AutoInt_PEP_retrain_20000_29999_49997[param] =  best_test_results_list[i]\n",
    "\n",
    "pd.DataFrame(AutoInt_PEP_retrain_20000_29999_49997).to_csv(\"autoInt_PEP_retrain[20000,29999,49997].csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0                         1                        2  \\\n",
      "0      NaN                     20000                    29997   \n",
      "1      AUC  [0.8478515367360074, 35]  [0.848147181307165, 35]   \n",
      "2  LogLoss          [0.37419495, 20]          [0.3737808, 20]   \n",
      "\n",
      "                          3  \n",
      "0                     49997  \n",
      "1  [0.8480486863604217, 20]  \n",
      "2          [0.37386855, 20]  \n",
      "number of parameters: ['20000', '29997', '49997']\n",
      "AUC: [0.8478515367360074, 0.848147181307165, 0.8480486863604217]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_7456\\3515794313.py:17: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df.iloc[i][j] = ast.literal_eval(df.iloc[i][j])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results = [\"./FM_PEP_retrain_[20000, 29997, 49997].csv\",\"./Deepfm_PEP_retrain[20000,30000,49988].csv\",\"./autoInt_PEP_retrain[20000,29999,49997].csv\"]\n",
    "dataframes = [pd.read_csv(res, header = None) for res in results]\n",
    "\n",
    "# just some preprocessing and parsing, do not pay alot of attention here\n",
    "for df in dataframes:\n",
    "    for i in (1,2):\n",
    "        for j in (1,2,3):\n",
    "            df.iloc[i][j] = ast.literal_eval(df.iloc[i][j])\n",
    "\n",
    "\n",
    "print(dataframes[0])\n",
    "print(f\"number of parameters: {[par for par in dataframes[0][[1,2,3]].iloc[0]]}\")\n",
    "print(f\"AUC: {[auc[0] for auc in dataframes[0][[1,2,3]].iloc[1]]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   parameters        AUC                      Model\n",
      "0       33032  82.574414       Uniform embedding FM\n",
      "1       66064  83.241420       Uniform embedding FM\n",
      "2      132128  83.695519       Uniform embedding FM\n",
      "3      264256  83.872533       Uniform embedding FM\n",
      "0       33032  84.461651   Uniform embedding DeepFM\n",
      "1       66064  84.167283   Uniform embedding DeepFM\n",
      "2      132128  84.309726   Uniform embedding DeepFM\n",
      "3      264256  84.421620   Uniform embedding DeepFM\n",
      "0       33032  84.389306  Uniform embedding AutoInt\n",
      "1       66064  84.693403  Uniform embedding AutoInt\n",
      "2      132128  84.745978  Uniform embedding AutoInt\n",
      "3      264256  84.844958  Uniform embedding AutoInt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load JSON data\n",
    "with open('../UniformEmbedding/tmp/json/best_results_uniformEmbedding_FM.json', 'r') as f:\n",
    "    data_fm = json.load(f)\n",
    "with open('../UniformEmbedding/tmp/json/best_results_uniformEmbedding_deepfm.json', 'r') as f:\n",
    "    data_deepfm = json.load(f)\n",
    "with open('../UniformEmbedding/tmp/json/best_results_uniformEmbedding_autoint.json', 'r') as f:\n",
    "    data_autoint = json.load(f)\n",
    "\n",
    "# Extract data_uniform\n",
    "best_test_results_fm = data_fm['best_test_results']\n",
    "best_test_results_deepfm = data_deepfm['best_test_results']\n",
    "best_test_results_autoint = data_autoint['best_test_results']\n",
    "best_test_results_fm = [float(auc)*100 for auc in best_test_results_fm]\n",
    "best_test_results_deepfm = [float(auc)*100 for auc in best_test_results_deepfm]\n",
    "best_test_results_autoint = [float(auc)*100 for auc in best_test_results_autoint]\n",
    "\n",
    "n_embedding_parameters_fm = data_fm['n_embedding_parameters']\n",
    "n_embedding_parameters_deepfm = data_deepfm['n_embedding_parameters']\n",
    "\n",
    "# Create DataFrames for uniform embedding models\n",
    "df_FM_uniform = pd.DataFrame({\n",
    "    'parameters': n_embedding_parameters_fm,\n",
    "    'AUC': best_test_results_fm,\n",
    "    'Model': 'Uniform embedding FM'\n",
    "})\n",
    "\n",
    "df_DeepFM_uniform = pd.DataFrame({\n",
    "    'parameters': n_embedding_parameters_deepfm,\n",
    "    'AUC': best_test_results_deepfm,\n",
    "    'Model': 'Uniform embedding DeepFM'\n",
    "})\n",
    "\n",
    "df_autoint_uniform = pd.DataFrame({\n",
    "    'parameters': n_embedding_parameters_deepfm,\n",
    "    'AUC': best_test_results_autoint,\n",
    "    'Model': 'Uniform embedding AutoInt'\n",
    "})\n",
    "\n",
    "# Combine uniform embedding DataFrames\n",
    "df_uniform = pd.concat([df_FM_uniform, df_DeepFM_uniform, df_autoint_uniform])\n",
    "\n",
    "print(df_uniform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "df_FM = pd.DataFrame({'parameters': [int(param) for param in dataframes[0][[1,2,3]].iloc[0]],\n",
    "                           'AUC': [auc[0]*100 for auc in dataframes[0][[1,2,3]].iloc[1]],\n",
    "                           'Model': 'PEP FM'})\n",
    "\n",
    "df_deepFM = pd.DataFrame({'parameters': [int(param) for param in dataframes[1][[1,2,3]].iloc[0]],\n",
    "                          'AUC': [auc[0]*100 for auc in dataframes[1][[1,2,3]].iloc[1]],\n",
    "                          'Model': 'PEP DeepFM'})\n",
    "\n",
    "df_autoint = pd.DataFrame({'parameters': [int(param) for param in dataframes[2][[1,2,3]].iloc[0]],\n",
    "                          'AUC': [auc[0]*100 for auc in dataframes[2][[1,2,3]].iloc[1]],\n",
    "                          'Model': 'PEP AutoInt'})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Combine DataFrames\n",
    "df = pd.concat([df_FM,df_deepFM,df_autoint])\n",
    "\n",
    "# Plot the data with separate calls for each model (specifying color and marker)\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Plot `df` with one color palette\n",
    "sns.lineplot(\n",
    "    x='parameters', y='AUC', hue='Model', data=df, \n",
    "    style='Model', palette='tab10', marker='o', markersize=10\n",
    ")\n",
    "\n",
    "# Plot `df_uniform` with another color palette\n",
    "sns.lineplot(\n",
    "    x='parameters', y='AUC', hue='Model', data=df_uniform, \n",
    "    style='Model', palette='Set2', marker='s', markersize=10\n",
    ")\n",
    "\n",
    "# Customize the plot\n",
    "# Set x-axis to log scale\n",
    "\n",
    "\n",
    "# Add custom ticks for the x-axis\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Parameters - log Scale')\n",
    "plt.ylabel('AUC')\n",
    "plt.title('AUC vs. Number of Parameters')\n",
    "\n",
    "# Re-order legend entries to match line order (optional)\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "plt.legend(by_label.values(), by_label.keys())\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.git',\n",
       " 'autoInt_PEP_retrain[20000,29999,49997].csv',\n",
       " 'data',\n",
       " 'data_loader',\n",
       " 'Deepfm_PEP_retrain[20000,30000,49988].csv',\n",
       " 'engine.py',\n",
       " 'FM_PEP_retrain_[20000, 29997, 49997].csv',\n",
       " 'models',\n",
       " 'README.md',\n",
       " 'tmp',\n",
       " 'train_avazu.py',\n",
       " 'train_avazu_retrain.py',\n",
       " 'train_criteo.ipynb',\n",
       " 'train_criteo.py',\n",
       " 'train_criteo_retrain.py',\n",
       " 'train_ml-1m.py',\n",
       " 'train_ml-1m_retrain.py',\n",
       " 'train_ml.ipynb',\n",
       " 'utils',\n",
       " '__pycache__']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
