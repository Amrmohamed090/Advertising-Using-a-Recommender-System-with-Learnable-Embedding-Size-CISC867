{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from argparse import ArgumentParser\n",
    "from tensorboardX import SummaryWriter\n",
    "import sys\n",
    "from models.factorizer import setup_factorizer\n",
    "from data_loader.data_loader import setup_generator\n",
    "from utils.evaluate import evaluate_fm\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "torch.backends.cudnn.enabled = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def setup_args(parser=None):\n",
    "    \"\"\" Set up arguments for the Engine\n",
    "\n",
    "    return:\n",
    "        python dictionary\n",
    "    \"\"\"\n",
    "    if parser is None:\n",
    "        parser = ArgumentParser()\n",
    "    data = parser.add_argument_group('Data')\n",
    "    engine = parser.add_argument_group('Engine Arguments')\n",
    "    factorize = parser.add_argument_group('Factorizer Arguments')\n",
    "    matrix_factorize = parser.add_argument_group('MF Arguments')\n",
    "    regularize = parser.add_argument_group('Regularizer Arguments')\n",
    "    log = parser.add_argument_group('Tensorboard Arguments')\n",
    "\n",
    "    engine.add_argument('--alias', default='experiment',\n",
    "                        help='Name for the experiment')\n",
    "    engine.add_argument('--seed', default='42')\n",
    "\n",
    "    data.add_argument('--data-type', default='ml1m', help='type of the dataset')\n",
    "    data.add_argument('--data-path', default='./data/{data_type}/')\n",
    "    data.add_argument('--train_test-freq-bd', help='split the data freq-wise, bound of the user freq')\n",
    "    data.add_argument('--train-valid-freq-bd', help='split the data freq-wise, bound of the user freq')\n",
    "    data.add_argument('--batch-size-train', default=1)\n",
    "    data.add_argument('--batch-size-valid', default=1)\n",
    "    data.add_argument('--batch-size-test', default=1)\n",
    "    data.add_argument('--device-ids-test', default=[0], help='devices used for multi-processing evaluate')\n",
    "\n",
    "    regularize.add_argument('--max-steps', default=1e5)\n",
    "    regularize.add_argument('--max-epochs', default=5)\n",
    "    regularize.add_argument('--use-cuda', default=True)\n",
    "    regularize.add_argument('--device-id', default=0, help='Training Devices')\n",
    "\n",
    "    factorize.add_argument('--factorizer', default='fm', help='Type of the Factorization Model')\n",
    "    factorize.add_argument('--latent-dim', default=8)\n",
    "\n",
    "    type_opt = 'fm'\n",
    "    matrix_factorize.add_argument('--{}-optimizer'.format(type_opt), default='sgd')\n",
    "    matrix_factorize.add_argument('--{}-lr'.format(type_opt), default=1e-3)\n",
    "    matrix_factorize.add_argument('--{}-grad-clip'.format(type_opt), default=1)\n",
    "\n",
    "    log.add_argument('--log-interval', default=1)\n",
    "    log.add_argument('--tensorboard', default='./tmp/runs')\n",
    "    log.add_argument('--early_stop', default=None)\n",
    "    log.add_argument('--display_interval', default=100)\n",
    "    return parser\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Engine(object):\n",
    "    \"\"\"Engine wrapping the training & evaluation\n",
    "       of adpative regularized maxtirx factorization\n",
    "    \"\"\"\n",
    "    _global_writer = None  # Class variable for the global writer\n",
    "    _param_step = 0  # Class variable to track parameter steps\n",
    "\n",
    "\n",
    "    def __init__(self, opt):\n",
    "        self._opt = opt\n",
    "        self._opt['data_path'] = self._opt['data_path'].format(data_type=self._opt['data_type'])\n",
    "        self._sampler = setup_generator(opt)\n",
    "\n",
    "        self._opt['field_dims'] = self._sampler.field_dims\n",
    "\n",
    "        if Engine._global_writer is None:\n",
    "            Engine._global_writer = SummaryWriter(log_dir='{}/parameter_comparison'.format(self._opt['tensorboard']))\n",
    "        \n",
    "\n",
    "        self._opt['emb_save_path'] = self._opt['emb_save_path'].format(\n",
    "            factorizer=self._opt['factorizer'],\n",
    "            data_type=self._opt['data_type'],\n",
    "            alias=self._opt['alias'],\n",
    "            num_parameter='{num_parameter}'\n",
    "        )\n",
    "        if 'retrain_emb_param' in opt:\n",
    "            self.retrain = True\n",
    "            if opt['re_init']:\n",
    "                self._opt['alias'] += '_reinitTrue'\n",
    "            else:\n",
    "                self._opt['alias'] += '_reinitFalse'\n",
    "            self._opt['alias'] += '_retrain_emb_param{}'.format(opt['retrain_emb_param'])\n",
    "        else:\n",
    "            self.retrain = False\n",
    "            self.candidate_p = self._opt.get('candidate_p')\n",
    "        self._opt['eval_res_path'] = self._opt['eval_res_path'].format(\n",
    "            factorizer=self._opt['factorizer'],\n",
    "            data_type=self._opt['data_type'],\n",
    "            alias=self._opt['alias'],\n",
    "            epoch_idx='{epoch_idx}'\n",
    "        )\n",
    "        self._factorizer = setup_factorizer(opt)\n",
    "        self._opt['tensorboard'] = self._opt['tensorboard'].format(\n",
    "            factorizer=self._opt['factorizer'],\n",
    "            data_type=self._opt['data_type'],\n",
    "        )\n",
    "        self._writer = SummaryWriter(log_dir='{}/{}'.format(self._opt['tensorboard'], opt['alias']))\n",
    "        self._writer.add_text('option', str(opt), 0)\n",
    "        self._mode = None\n",
    "        self.early_stop = self._opt.get('early_stop')\n",
    "\n",
    "\n",
    "    @property\n",
    "    def mode(self):\n",
    "        return self._mode\n",
    "\n",
    "    @mode.setter\n",
    "    def mode(self, new_mode):\n",
    "        assert new_mode in ['complete', 'partial', None]  # training a complete trajectory or a partial trajctory\n",
    "        self._mode = new_mode\n",
    "\n",
    "    def save_pruned_embedding(self, param, step_idx):\n",
    "        max_candidate_p = max(self.candidate_p)\n",
    "        if max_candidate_p == 0:\n",
    "\n",
    "            print(\"Minimal target parameters achieved, stop pruning.\")\n",
    "            return 0\n",
    "\n",
    "        else:\n",
    "            if param <= max_candidate_p:\n",
    "                embedding = self._factorizer.model.get_embedding()\n",
    "                emb_save_path = self._opt['emb_save_path'].format(num_parameter=param)\n",
    "                emb_save_dir, _ = os.path.split(emb_save_path)\n",
    "                if not os.path.exists(emb_save_dir):\n",
    "                    os.makedirs(emb_save_dir)\n",
    "                np.save(emb_save_path, embedding)\n",
    "                max_idx = self.candidate_p.index(max(self.candidate_p))\n",
    "                self.candidate_p[max_idx] = 0\n",
    "                print(\"*\" * 80)\n",
    "                print(\"Reach the target parameter: {}, save embedding with size: {}\".format(max_candidate_p, param))\n",
    "                print(\"*\" * 80)\n",
    "            elif step_idx == 0:\n",
    "                embedding = self._factorizer.model.get_embedding()\n",
    "                emb_save_path = self._opt['emb_save_path'].format(num_parameter='initial_embedding')\n",
    "                emb_save_dir, _ = os.path.split(emb_save_path)\n",
    "                if not os.path.exists(emb_save_dir):\n",
    "                    os.makedirs(emb_save_dir)\n",
    "                np.save(emb_save_path, embedding)\n",
    "                print(\"*\" * 80)\n",
    "                print(\"Save the initial embedding table\")\n",
    "                print(\"*\" * 80)\n",
    "        return 1\n",
    "    \n",
    "    def train_an_episode(self, max_steps, max_epochs, episode_idx=''):\n",
    "        \"\"\"Train a feature_based recommendation model\"\"\"\n",
    "        assert self.mode in ['partial', 'complete']\n",
    "\n",
    "        print('-' * 80)\n",
    "        print('[{} episode {} starts!]'.format(self.mode, episode_idx))\n",
    "        print('Initializing ...')\n",
    "        self._factorizer.init_episode()\n",
    "\n",
    "        log_interval = self._opt.get('log_interval')\n",
    "        eval_interval = self._opt.get('eval_interval')\n",
    "        display_interval = self._opt.get('display_interval')\n",
    "\n",
    "        status = dict()\n",
    "        flag, test_flag, valid_flag = 0, 0, 0\n",
    "        valid_mf_loss, train_mf_loss = np.inf, np.inf\n",
    "        best_valid_result = {\"AUC\": [0, 0], \"LogLoss\": [np.inf, 0]}\n",
    "        best_test_result = {\"AUC\": [0, 0], \"LogLoss\": [np.inf, 0]}\n",
    "        epoch_start = datetime.now()\n",
    "        for step_idx in range(int(max_steps)):\n",
    "            # Prepare status for current step\n",
    "            status['done'] = False\n",
    "            status['sampler'] = self._sampler\n",
    "            train_mf_loss = self._factorizer.update(self._sampler)\n",
    "            status['train_mf_loss'] = train_mf_loss\n",
    "\n",
    "            # Logging & Evaluate on the Evaluate Set\n",
    "            if self.mode == 'complete' and step_idx % log_interval == 0:\n",
    "                epoch_idx = int(step_idx / self._sampler.num_batches_train)\n",
    "                if epoch_idx > max_epochs and self.retrain:\n",
    "                    return best_test_result\n",
    "\n",
    "                sparsity, params = self._factorizer.model.calc_sparsity()\n",
    "                if not self.retrain:\n",
    "                    returnflag = self.save_pruned_embedding(params, step_idx)\n",
    "                    if not returnflag:\n",
    "                        return best_test_result\n",
    "                    \n",
    "                self._writer.add_scalar('train/step_wise/mf_loss', train_mf_loss, step_idx)\n",
    "                self._writer.add_scalar('train/step_wise/sparsity', sparsity, step_idx)\n",
    "\n",
    "                if step_idx % display_interval == 0:\n",
    "                    print('[Epoch {}|Step {}|Flag {}|Sparsity {:.4f}|Params {}| Best AUC {}]'.format(epoch_idx,\n",
    "                                                                                        step_idx % self._sampler.num_batches_train,\n",
    "                                                                                        flag, sparsity, params,\n",
    "                                                                                        round(best_test_result['AUC'][0],4)))\n",
    "\n",
    "                if step_idx % self._sampler.num_batches_train == 0:\n",
    "                    threshold = self._factorizer.model.get_threshold()\n",
    "\n",
    "                    self._writer.add_histogram('threshold/epoch_wise/threshold', threshold, epoch_idx)\n",
    "                    self._writer.add_scalar('train/epoch_wise/sparsity', sparsity, epoch_idx)\n",
    "                    self._writer.add_scalar('train/epoch_wise/params', params, epoch_idx)\n",
    "\n",
    "                if (step_idx % self._sampler.num_batches_train == 0) and (epoch_idx % eval_interval == 0) and self.retrain:\n",
    "                    print('Evaluate on test ...')\n",
    "                    start = datetime.now()\n",
    "                    eval_res_path = self._opt['eval_res_path'].format(epoch_idx=epoch_idx)\n",
    "                    eval_res_dir, _ = os.path.split(eval_res_path)\n",
    "                    if not os.path.exists(eval_res_dir):\n",
    "                        os.makedirs(eval_res_dir)\n",
    "\n",
    "                    use_cuda = self._opt['use_cuda']\n",
    "                    logloss, auc = evaluate_fm(self._factorizer, self._sampler, use_cuda)\n",
    "                    self._writer.add_scalar('test/epoch_wise/metron_auc', auc, epoch_idx)\n",
    "                    self._writer.add_scalar('test/epoch_wise/metron_logloss', logloss, epoch_idx)\n",
    "                    if logloss < best_test_result['LogLoss'][0]:\n",
    "                        best_test_result['LogLoss'][0] = logloss\n",
    "                        best_test_result['LogLoss'][1] = epoch_idx\n",
    "                    if auc > best_test_result['AUC'][0]:\n",
    "                        best_test_result['AUC'][0] = auc\n",
    "                        best_test_result['AUC'][1] = epoch_idx\n",
    "                        test_flag = 0\n",
    "                    else:\n",
    "                        test_flag += 1\n",
    "                    pd.Series(best_test_result).to_csv(eval_res_path)\n",
    "                    print(\"*\" * 80)\n",
    "                    print(\"Test AUC: {:4f} | Logloss: {:4f}\".format(auc, logloss))\n",
    "                    end = datetime.now()\n",
    "                    print('Evaluate Time {} minutes'.format((end - start).total_seconds() / 60))\n",
    "                    epoch_end = datetime.now()\n",
    "                    dur = (epoch_end - epoch_start).total_seconds() / 60\n",
    "                    epoch_start = datetime.now()\n",
    "                    print('[Epoch {:4d}] train MF loss: {:04.8f}, '\n",
    "                          'valid loss: {:04.8f}, time {:04.8f} minutes'.format(epoch_idx,\n",
    "                                                                               train_mf_loss,\n",
    "                                                                               valid_mf_loss,\n",
    "                                                                               dur))\n",
    "                    print(\"*\"*80)\n",
    "                    \n",
    "            flag = test_flag\n",
    "            if self.early_stop is not None and flag >= self.early_stop :\n",
    "                print(\"Early stop training process\")\n",
    "                print(\"Best performance on test data: \", best_test_result)\n",
    "                print(\"Best performance on valid data: \", best_valid_result)\n",
    "                self._writer.add_text('best_valid_result', str(best_valid_result), 0)\n",
    "                self._writer.add_text('best_test_result', str(best_test_result), 0)\n",
    "                return best_test_result\n",
    "        return best_test_result\n",
    "                \n",
    "            \n",
    "            \n",
    "    def train_finish(self, best_test_result):\n",
    "        Engine._global_writer.add_scalar(\n",
    "            'parameter_comparison/best_auc_vs_params', \n",
    "            best_test_result['AUC'][0],\n",
    "            Engine._param_step\n",
    "        )\n",
    "        Engine._param_step += 1  # Increment the step counter\n",
    "        Engine._global_writer.flush()\n",
    "\n",
    "    def __del__(self):\n",
    "        if hasattr(self, '_writer'):\n",
    "            self._writer.close()\n",
    "        if Engine._global_writer is not None:  # Close global writer if exists\n",
    "            Engine._global_writer.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        self.mode = 'complete'\n",
    "        best_test_result = self.train_an_episode(self._opt['max_steps'],self._opt['max_epochs'])\n",
    "        return best_test_result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train FM, with avazu Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FM_train_BaseDim32_bsz1024_lr_0.001_optim_adam_thresholdTypeFEATURE_DIM_thres_init-15_sigmoid-1_l2_penalty0\n",
      "\tNum of train records: 32343173\n",
      "\tNum of valid records: 4042896\n",
      "\tNum of test records: 4042898\n",
      "\tNum of fields: 22\n",
      "\tNum of features: 645394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pytorchEnv\\env\\Lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BackBone Embedding Parameters:  20652608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pytorchEnv\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "[complete episode  starts!]\n",
      "Initializing ...\n",
      "BackBone Embedding Parameters:  20652608\n",
      "********************************************************************************\n",
      "Save the initial embedding table\n",
      "********************************************************************************\n",
      "[Epoch 0|Step 0|Flag 0|Sparsity 0.0001|Params 20650466| Best AUC 0]\n",
      "[Epoch 0|Step 1000|Flag 0|Sparsity 0.9389|Params 1262518| Best AUC 0]\n",
      "[Epoch 0|Step 2000|Flag 0|Sparsity 0.9472|Params 1090701| Best AUC 0]\n",
      "[Epoch 0|Step 3000|Flag 0|Sparsity 0.9508|Params 1016313| Best AUC 0]\n",
      "[Epoch 0|Step 4000|Flag 0|Sparsity 0.9532|Params 966268| Best AUC 0]\n",
      "[Epoch 0|Step 5000|Flag 0|Sparsity 0.9553|Params 922795| Best AUC 0]\n",
      "[Epoch 0|Step 6000|Flag 0|Sparsity 0.9575|Params 876892| Best AUC 0]\n",
      "[Epoch 0|Step 7000|Flag 0|Sparsity 0.9606|Params 813026| Best AUC 0]\n",
      "[Epoch 0|Step 8000|Flag 0|Sparsity 0.9658|Params 706435| Best AUC 0]\n",
      "[Epoch 0|Step 9000|Flag 0|Sparsity 0.9736|Params 545259| Best AUC 0]\n",
      "[Epoch 0|Step 10000|Flag 0|Sparsity 0.9823|Params 364630| Best AUC 0]\n",
      "[Epoch 0|Step 11000|Flag 0|Sparsity 0.9896|Params 214544| Best AUC 0]\n",
      "[Epoch 0|Step 12000|Flag 0|Sparsity 0.9939|Params 125677| Best AUC 0]\n",
      "[Epoch 0|Step 13000|Flag 0|Sparsity 0.9963|Params 75815| Best AUC 0]\n",
      "********************************************************************************\n",
      "Reach the target parameter: 50000, save embedding with size: 50000\n",
      "********************************************************************************\n",
      "[Epoch 0|Step 14000|Flag 0|Sparsity 0.9980|Params 42166| Best AUC 0]\n",
      "********************************************************************************\n",
      "Reach the target parameter: 30000, save embedding with size: 29985\n",
      "********************************************************************************\n",
      "Minimal target parameters achieved, stop pruning.\n",
      "********** {'AUC': [0, 0], 'LogLoss': [inf, 0]} **********\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "parser = setup_args()\n",
    "parser.set_defaults(\n",
    "    alias='train',\n",
    "    tensorboard='./tmp/runs/{factorizer}/{data_type}',\n",
    "    ##########\n",
    "    ## data ##\n",
    "    ##########\n",
    "    data_type='avazu',\n",
    "    data_path='./data/{data_type}/',\n",
    "    load_in_queue=False,\n",
    "    category_only=False,\n",
    "    rebuild_cache=False,\n",
    "    eval_res_path='./tmp/res/{factorizer}/{data_type}/{alias}/{epoch_idx}.csv',\n",
    "    emb_save_path='./tmp/embedding/{factorizer}/{data_type}/{alias}/{num_parameter}',\n",
    "    ######################\n",
    "    ## train/test split ##\n",
    "    ######################\n",
    "    test_ratio=0.1,\n",
    "    valid_ratio=1/9,\n",
    "    ##########################\n",
    "    ## Devices & Efficiency ##\n",
    "    ##########################\n",
    "    use_cuda=True,\n",
    "    early_stop=10,\n",
    "    log_interval=1,\n",
    "    display_interval=1000,\n",
    "    eval_interval=5,  # 10 epochs between 2 evaluations\n",
    "    device_ids_test=[0],\n",
    "    device_id=0,\n",
    "    batch_size_train=1024,\n",
    "    batch_size_valid=1024,\n",
    "    batch_size_test=1024,\n",
    "    ###########\n",
    "    ## Model ##\n",
    "    ###########\n",
    "    factorizer='fm',\n",
    "    model='fm',\n",
    "    fm_lr=1e-3,\n",
    "    # Deep\n",
    "    mlp_dims=[100, 100],\n",
    "    # AutoInt\n",
    "    has_residual=True,\n",
    "    full_part=True,\n",
    "    num_heads=2,\n",
    "    num_layers=3,\n",
    "    att_dropout=0.4,\n",
    "    atten_embed_dim=64,\n",
    "    # optimizer setting\n",
    "    fm_optimizer='adam',\n",
    "    fm_amsgrad=False,\n",
    "    fm_eps=1e-8,\n",
    "    fm_l2_regularization=1e-5,\n",
    "    fm_betas=(0.9, 0.999),\n",
    "    fm_grad_clip=100,  # 0.1\n",
    "    fm_lr_exp_decay=1,\n",
    "    l2_penalty=0,\n",
    "    #########\n",
    "    ## Embeddings//PEP ##\n",
    "    #########\n",
    "    latent_dim=32,\n",
    "    threshold_type='feature_dim',\n",
    "    g_type='sigmoid',\n",
    "    gk=1,\n",
    "    threshold_init=-15,\n",
    "    candidate_p=[50000, 30000],\n",
    ")\n",
    "\n",
    "opt = parser.parse_args(args=[])\n",
    "opt = vars(opt)\n",
    "\n",
    "# rename alias\n",
    "\n",
    "opt['alias'] = '{}_{}_BaseDim{}_bsz{}_lr_{}_optim_{}_thresholdType{}_thres_init{}_{}-{}_l2_penalty{}'.format(\n",
    "    opt['model'].upper(),\n",
    "    opt['alias'],\n",
    "    opt['latent_dim'],\n",
    "    opt['batch_size_train'],\n",
    "    opt['fm_lr'],\n",
    "    opt['fm_optimizer'],\n",
    "    opt['threshold_type'].upper(),\n",
    "    opt['threshold_init'],\n",
    "    opt['g_type'],\n",
    "    opt['gk'],\n",
    "    opt['l2_penalty']\n",
    ")\n",
    "print(opt['alias'])\n",
    "random.seed(opt['seed'])\n",
    "# np.random.seed(opt['seed'])\n",
    "torch.manual_seed(opt['seed'])\n",
    "torch.cuda.manual_seed_all(opt['seed'])\n",
    "engine = Engine(opt)\n",
    "best_result = engine.train()\n",
    "print(\"*\"*10 , best_result, \"*\"*10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrain for target parameters 29985\n",
      "FM_train_BaseDim32_bsz1024_lr_0.001_optim_adam_thresholdTypeFEATURE_DIM_thres_init-15_sigmoid-1_l2_penalty0\n",
      "\tNum of train records: 32343173\n",
      "\tNum of valid records: 4042896\n",
      "\tNum of test records: 4042898\n",
      "\tNum of fields: 22\n",
      "\tNum of features: 645394\n",
      "Retrain epoch 29985\n",
      "BackBone Embedding Parameters:  20652608\n",
      "--------------------------------------------------------------------------------\n",
      "[complete episode  starts!]\n",
      "Initializing ...\n",
      "Retrain epoch 29985\n",
      "BackBone Embedding Parameters:  20652608\n",
      "[Epoch 0|Step 0|Flag 0|Sparsity 0.9985|Params 29985| Best AUC 0]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.501445 | Logloss: 2.388995\n",
      "Evaluate Time 10.274916916666665 minutes\n",
      "[Epoch    0] train MF loss: 2.33896732, valid loss: 0inf, time 10.37719768 minutes\n",
      "********************************************************************************\n",
      "[Epoch 0|Step 1000|Flag 0|Sparsity 0.9985|Params 29985| Best AUC 0.5014]\n",
      "[Epoch 0|Step 2000|Flag 0|Sparsity 0.9985|Params 29985| Best AUC 0.5014]\n",
      "[Epoch 0|Step 3000|Flag 0|Sparsity 0.9985|Params 29985| Best AUC 0.5014]\n",
      "[Epoch 0|Step 4000|Flag 0|Sparsity 0.9985|Params 29985| Best AUC 0.5014]\n",
      "[Epoch 0|Step 5000|Flag 0|Sparsity 0.9985|Params 29985| Best AUC 0.5014]\n",
      "[Epoch 0|Step 6000|Flag 0|Sparsity 0.9985|Params 29985| Best AUC 0.5014]\n",
      "[Epoch 0|Step 7000|Flag 0|Sparsity 0.9985|Params 29985| Best AUC 0.5014]\n",
      "[Epoch 0|Step 8000|Flag 0|Sparsity 0.9985|Params 29985| Best AUC 0.5014]\n",
      "[Epoch 0|Step 9000|Flag 0|Sparsity 0.9985|Params 29985| Best AUC 0.5014]\n",
      "[Epoch 0|Step 10000|Flag 0|Sparsity 0.9985|Params 29985| Best AUC 0.5014]\n",
      "[Epoch 0|Step 11000|Flag 0|Sparsity 0.9985|Params 29985| Best AUC 0.5014]\n",
      "[Epoch 0|Step 12000|Flag 0|Sparsity 0.9985|Params 29985| Best AUC 0.5014]\n",
      "[Epoch 0|Step 13000|Flag 0|Sparsity 0.9985|Params 29985| Best AUC 0.5014]\n",
      "[Epoch 0|Step 14000|Flag 0|Sparsity 0.9985|Params 29985| Best AUC 0.5014]\n",
      "[Epoch 0|Step 15000|Flag 0|Sparsity 0.9985|Params 29985| Best AUC 0.5014]\n",
      "[Epoch 0|Step 16000|Flag 0|Sparsity 0.9985|Params 29985| Best AUC 0.5014]\n",
      "[Epoch 0|Step 17000|Flag 0|Sparsity 0.9985|Params 29985| Best AUC 0.5014]\n",
      "[Epoch 0|Step 18000|Flag 0|Sparsity 0.9985|Params 29985| Best AUC 0.5014]\n",
      "[Epoch 0|Step 19000|Flag 0|Sparsity 0.9985|Params 29985| Best AUC 0.5014]\n",
      "[Epoch 0|Step 20000|Flag 0|Sparsity 0.9985|Params 29985| Best AUC 0.5014]\n",
      "[Epoch 0|Step 21000|Flag 0|Sparsity 0.9985|Params 29985| Best AUC 0.5014]\n",
      "[Epoch 0|Step 22000|Flag 0|Sparsity 0.9985|Params 29985| Best AUC 0.5014]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 95\u001b[0m\n\u001b[0;32m     93\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmanual_seed_all(opt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     94\u001b[0m engine \u001b[38;5;241m=\u001b[39m Engine(opt)\n\u001b[1;32m---> 95\u001b[0m best_result \u001b[38;5;241m=\u001b[39m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbbbbbbbbbbbb\u001b[39m\u001b[38;5;124m\"\u001b[39m,best_result)\n\u001b[0;32m     97\u001b[0m best_test_results_list\u001b[38;5;241m.\u001b[39mappend(best_result)\n",
      "Cell \u001b[1;32mIn[7], line 217\u001b[0m, in \u001b[0;36mEngine.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomplete\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 217\u001b[0m     best_test_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_an_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_opt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_steps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_opt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_epochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m best_test_result\n",
      "Cell \u001b[1;32mIn[7], line 117\u001b[0m, in \u001b[0;36mEngine.train_an_episode\u001b[1;34m(self, max_steps, max_epochs, episode_idx)\u001b[0m\n\u001b[0;32m    115\u001b[0m status[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdone\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    116\u001b[0m status[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msampler\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler\n\u001b[1;32m--> 117\u001b[0m train_mf_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_factorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sampler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m status[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_mf_loss\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m train_mf_loss\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# Logging & Evaluate on the Evaluate Set\u001b[39;00m\n",
      "File \u001b[1;32md:\\pytorchEnv\\Advertising-Using-a-Recommender-System-with-Learnable-Embedding-Size-CISC867\\learnable-embed-sizes-for-RecSys\\models\\factorizer.py:128\u001b[0m, in \u001b[0;36mFMFactorizer.update\u001b[1;34m(self, sampler)\u001b[0m\n\u001b[0;32m    126\u001b[0m l2_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39ml2_penalty(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml2_penalty) \u001b[38;5;241m/\u001b[39m (data\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    127\u001b[0m loss \u001b[38;5;241m=\u001b[39m non_reg_loss \u001b[38;5;241m+\u001b[39m l2_loss\n\u001b[1;32m--> 128\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip)\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32md:\\pytorchEnv\\env\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\pytorchEnv\\env\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "target_parameters = [ 29985, 50000]\n",
    "best_test_results_list = []\n",
    "for target_param in target_parameters:\n",
    "\n",
    "    print(f\"retrain for target parameters {target_param}\")\n",
    "    parser = setup_args()\n",
    "    parser.set_defaults(\n",
    "        alias='train',\n",
    "        tensorboard='./tmp/runs/{factorizer}/{data_type}',\n",
    "        ##########\n",
    "        ## data ##\n",
    "        ##########\n",
    "        data_type='avazu',\n",
    "        data_path='./data/{data_type}/',\n",
    "        load_in_queue=False,\n",
    "        category_only=False,\n",
    "        rebuild_cache=False,\n",
    "        eval_res_path='./tmp/res/{factorizer}/{data_type}/{alias}/{epoch_idx}.csv',\n",
    "        emb_save_path='./tmp/embedding/{factorizer}/{data_type}/{alias}/{num_parameter}',\n",
    "        ######################\n",
    "        ## train/test split ##\n",
    "        ######################\n",
    "        test_ratio=0.1,\n",
    "        valid_ratio=1/9,\n",
    "        ##########################\n",
    "        ## Devices & Efficiency ##\n",
    "        ##########################\n",
    "        use_cuda=True,\n",
    "        early_stop=40,\n",
    "        log_interval=1,\n",
    "        display_interval=1000,\n",
    "        eval_interval=5,  # 10 epochs between 2 evaluations\n",
    "        device_ids_test=[0],\n",
    "        device_id=0,\n",
    "        batch_size_train=1024,\n",
    "        batch_size_valid=1024,\n",
    "        batch_size_test=1024,\n",
    "        ###########\n",
    "        ## Model ##\n",
    "        ###########\n",
    "        factorizer='fm',\n",
    "        model='fm',\n",
    "        fm_lr=1e-3,\n",
    "        # Deep\n",
    "        mlp_dims=[100, 100],\n",
    "        # AutoInt\n",
    "        has_residual=True,\n",
    "        full_part=True,\n",
    "        num_heads=2,\n",
    "        num_layers=3,\n",
    "        att_dropout=0.4,\n",
    "        atten_embed_dim=64,\n",
    "        # optimizer setting\n",
    "        fm_optimizer='adam',\n",
    "        fm_amsgrad=False,\n",
    "        fm_eps=1e-8,\n",
    "        fm_l2_regularization=1e-5,\n",
    "        fm_betas=(0.9, 0.999),\n",
    "        fm_grad_clip=100,  # 0.1\n",
    "        fm_lr_exp_decay=1,\n",
    "        l2_penalty=0,\n",
    "        #########\n",
    "        ## PEP ##\n",
    "        #########\n",
    "        latent_dim=32,\n",
    "        threshold_type='feature_dim',\n",
    "        g_type='sigmoid',\n",
    "        gk=1,\n",
    "        threshold_init=-15,\n",
    "        retrain_emb_param=target_param,\n",
    "        re_init=False,\n",
    "    )\n",
    "\n",
    "    opt = parser.parse_args(args=[])\n",
    "    opt = vars(opt)\n",
    "    opt['alias'] = '{}_{}_BaseDim{}_bsz{}_lr_{}_optim_{}_thresholdType{}_thres_init{}_{}-{}_l2_penalty{}'.format(\n",
    "        opt['model'].upper(),\n",
    "        opt['alias'],\n",
    "        opt['latent_dim'],\n",
    "        opt['batch_size_train'],\n",
    "        opt['fm_lr'],\n",
    "        opt['fm_optimizer'],\n",
    "        opt['threshold_type'].upper(),\n",
    "        opt['threshold_init'],\n",
    "        opt['g_type'],\n",
    "        opt['gk'],\n",
    "        opt['l2_penalty']\n",
    "    )\n",
    "    print(opt['alias'])\n",
    "    random.seed(opt['seed'])\n",
    "    # np.random.seed(opt['seed'])\n",
    "    torch.manual_seed(opt['seed'])\n",
    "    torch.cuda.manual_seed_all(opt['seed'])\n",
    "    engine = Engine(opt)\n",
    "    best_result = engine.train()\n",
    "    print(\"bbbbbbbbbbbbb\",best_result)\n",
    "    best_test_results_list.append(best_result)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'AUC': [0.7926537810981514, 15], 'LogLoss': [0.45323467, 15]}, {'AUC': [0.7919997853039459, 15], 'LogLoss': [0.4540726, 15]}, {'AUC': [0.7907308301215545, 15], 'LogLoss': [0.4555841, 15]}]\n"
     ]
    }
   ],
   "source": [
    "print(best_test_results_list)\n",
    "avazu_FM_PEP = {}\n",
    "for i, param in enumerate([29985, 50000]):\n",
    "    avazu_FM_PEP[param] =  best_test_results_list[i]\n",
    "\n",
    "pd.DataFrame(avazu_FM_PEP).to_csv(\"avazu_FM_PEP.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train DEEPFM, with avazu Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEEPFM_train_BaseDim32_bsz1024_lr_0.001_optim_adam_thresholdTypeFEATURE_DIM_thres_init-15_sigmoid-1_l2_penalty0\n",
      "\tNum of train records: 32343173\n",
      "\tNum of valid records: 4042896\n",
      "\tNum of test records: 4042898\n",
      "\tNum of fields: 22\n",
      "\tNum of features: 645394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pytorchEnv\\env\\Lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BackBone Embedding Parameters:  20652608\n",
      "--------------------------------------------------------------------------------\n",
      "[complete episode  starts!]\n",
      "Initializing ...\n",
      "BackBone Embedding Parameters:  20652608\n",
      "********************************************************************************\n",
      "Save the initial embedding table\n",
      "********************************************************************************\n",
      "[Epoch 0|Step 0|Flag 0|Sparsity 0.0001|Params 20650473| Best AUC 0]\n",
      "[Epoch 0|Step 1000|Flag 0|Sparsity 0.8904|Params 2263960| Best AUC 0]\n",
      "[Epoch 0|Step 2000|Flag 0|Sparsity 0.8969|Params 2129418| Best AUC 0]\n",
      "[Epoch 0|Step 3000|Flag 0|Sparsity 0.9037|Params 1989730| Best AUC 0]\n",
      "[Epoch 0|Step 4000|Flag 0|Sparsity 0.9112|Params 1833394| Best AUC 0]\n",
      "[Epoch 0|Step 5000|Flag 0|Sparsity 0.9201|Params 1649551| Best AUC 0]\n",
      "[Epoch 0|Step 6000|Flag 0|Sparsity 0.9305|Params 1434510| Best AUC 0]\n",
      "[Epoch 0|Step 7000|Flag 0|Sparsity 0.9422|Params 1193746| Best AUC 0]\n",
      "[Epoch 0|Step 8000|Flag 0|Sparsity 0.9548|Params 933026| Best AUC 0]\n",
      "[Epoch 0|Step 9000|Flag 0|Sparsity 0.9683|Params 654701| Best AUC 0]\n",
      "[Epoch 0|Step 10000|Flag 0|Sparsity 0.9807|Params 397880| Best AUC 0]\n",
      "[Epoch 0|Step 11000|Flag 0|Sparsity 0.9895|Params 216690| Best AUC 0]\n",
      "[Epoch 0|Step 12000|Flag 0|Sparsity 0.9941|Params 121632| Best AUC 0]\n",
      "[Epoch 0|Step 13000|Flag 0|Sparsity 0.9965|Params 72304| Best AUC 0]\n",
      "********************************************************************************\n",
      "Reach the target parameter: 50000, save embedding with size: 49968\n",
      "********************************************************************************\n",
      "[Epoch 0|Step 14000|Flag 0|Sparsity 0.9981|Params 39064| Best AUC 0]\n",
      "********************************************************************************\n",
      "Reach the target parameter: 30000, save embedding with size: 29989\n",
      "********************************************************************************\n",
      "Minimal target parameters achieved, stop pruning.\n",
      "********** {'AUC': [0, 0], 'LogLoss': [inf, 0]} **********\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "parser = setup_args()\n",
    "parser.set_defaults(\n",
    "    alias='train',\n",
    "    tensorboard='./tmp/runs/{factorizer}/{data_type}',\n",
    "    ##########\n",
    "    ## data ##\n",
    "    ##########\n",
    "    data_type='avazu',\n",
    "    data_path='./data/{data_type}/',\n",
    "    load_in_queue=False,\n",
    "    category_only=False,\n",
    "    rebuild_cache=False,\n",
    "    eval_res_path='./tmp/res/{factorizer}/{data_type}/{alias}/{epoch_idx}.csv',\n",
    "    emb_save_path='./tmp/embedding/{factorizer}/{data_type}/{alias}/{num_parameter}',\n",
    "    ######################\n",
    "    ## train/test split ##\n",
    "    ######################\n",
    "    test_ratio=0.1,\n",
    "    valid_ratio=1/9,\n",
    "    ##########################\n",
    "    ## Devices & Efficiency ##\n",
    "    ##########################\n",
    "    use_cuda=True,\n",
    "    early_stop=10,\n",
    "    log_interval=1,\n",
    "    display_interval=1000,\n",
    "    eval_interval=5,  # 10 epochs between 2 evaluations\n",
    "    device_ids_test=[0],\n",
    "    device_id=0,\n",
    "    batch_size_train=1024,\n",
    "    batch_size_valid=1024,\n",
    "    batch_size_test=1024,\n",
    "    ###########\n",
    "    ## Model ##\n",
    "    ###########\n",
    "    factorizer='fm',\n",
    "    model='deepfm',\n",
    "    fm_lr=1e-3,\n",
    "    # Deep\n",
    "    mlp_dims=[100, 100],\n",
    "    # AutoInt\n",
    "    has_residual=True,\n",
    "    full_part=True,\n",
    "    num_heads=2,\n",
    "    num_layers=3,\n",
    "    att_dropout=0.4,\n",
    "    atten_embed_dim=64,\n",
    "    # optimizer setting\n",
    "    fm_optimizer='adam',\n",
    "    fm_amsgrad=False,\n",
    "    fm_eps=1e-8,\n",
    "    fm_l2_regularization=1e-5,\n",
    "    fm_betas=(0.9, 0.999),\n",
    "    fm_grad_clip=100,  # 0.1\n",
    "    fm_lr_exp_decay=1,\n",
    "    l2_penalty=0,\n",
    "    #########\n",
    "    ## Embeddings//PEP ##\n",
    "    #########\n",
    "    latent_dim=32,\n",
    "    threshold_type='feature_dim',\n",
    "    g_type='sigmoid',\n",
    "    gk=1,\n",
    "    threshold_init=-15,\n",
    "    candidate_p=[50000, 30000],\n",
    ")\n",
    "\n",
    "opt = parser.parse_args(args=[])\n",
    "opt = vars(opt)\n",
    "\n",
    "# rename alias\n",
    "\n",
    "opt['alias'] = '{}_{}_BaseDim{}_bsz{}_lr_{}_optim_{}_thresholdType{}_thres_init{}_{}-{}_l2_penalty{}'.format(\n",
    "    opt['model'].upper(),\n",
    "    opt['alias'],\n",
    "    opt['latent_dim'],\n",
    "    opt['batch_size_train'],\n",
    "    opt['fm_lr'],\n",
    "    opt['fm_optimizer'],\n",
    "    opt['threshold_type'].upper(),\n",
    "    opt['threshold_init'],\n",
    "    opt['g_type'],\n",
    "    opt['gk'],\n",
    "    opt['l2_penalty']\n",
    ")\n",
    "print(opt['alias'])\n",
    "random.seed(opt['seed'])\n",
    "# np.random.seed(opt['seed'])\n",
    "torch.manual_seed(opt['seed'])\n",
    "torch.cuda.manual_seed_all(opt['seed'])\n",
    "engine = Engine(opt)\n",
    "best_result = engine.train()\n",
    "print(\"*\"*10 , best_result, \"*\"*10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrain for DeepFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrain for target parameters 19998\n",
      "DEEPFM_train_BaseDim32_bsz1024_lr_0.001_optim_adam_thresholdTypeFEATURE_DIM_thres_init-15_sigmoid-1_l2_penalty0\n",
      "0\n",
      "\tNum of train records: 2400000\n",
      "\tNum of valid records: 300000\n",
      "\tNum of test records: 300000\n",
      "\tNum of fields: 39\n",
      "\tNum of features: 150032\n",
      "Retrain epoch 19998\n",
      "BackBone Embedding Parameters:  4801024\n",
      "--------------------------------------------------------------------------------\n",
      "[complete episode  starts!]\n",
      "Initializing ...\n",
      "Retrain epoch 19998\n",
      "BackBone Embedding Parameters:  4801024\n",
      "[Epoch 0|Step 0|Flag 0|Sparsity 0.9958|Params 19998| AUC [0, 0]]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.520811 | Logloss: 2.337113\n",
      "Evaluate Time 0.08480715 minutes\n",
      "[Epoch    0] train MF loss: 2.24896526, valid loss: 0inf, time 0.09538002 minutes\n",
      "********************************************************************************\n",
      "[Epoch 0|Step 1000|Flag 0|Sparsity 0.9958|Params 19998| AUC [0.520811361570302, 0]]\n",
      "[Epoch 0|Step 2000|Flag 0|Sparsity 0.9958|Params 19998| AUC [0.520811361570302, 0]]\n",
      "[Epoch 1|Step 656|Flag 0|Sparsity 0.9958|Params 19998| AUC [0.520811361570302, 0]]\n",
      "[Epoch 1|Step 1656|Flag 0|Sparsity 0.9958|Params 19998| AUC [0.520811361570302, 0]]\n",
      "[Epoch 2|Step 312|Flag 0|Sparsity 0.9958|Params 19998| AUC [0.520811361570302, 0]]\n",
      "[Epoch 2|Step 1312|Flag 0|Sparsity 0.9958|Params 19998| AUC [0.520811361570302, 0]]\n",
      "[Epoch 2|Step 2312|Flag 0|Sparsity 0.9958|Params 19998| AUC [0.520811361570302, 0]]\n",
      "[Epoch 3|Step 968|Flag 0|Sparsity 0.9958|Params 19998| AUC [0.520811361570302, 0]]\n",
      "[Epoch 3|Step 1968|Flag 0|Sparsity 0.9958|Params 19998| AUC [0.520811361570302, 0]]\n",
      "[Epoch 4|Step 624|Flag 0|Sparsity 0.9958|Params 19998| AUC [0.520811361570302, 0]]\n",
      "[Epoch 4|Step 1624|Flag 0|Sparsity 0.9958|Params 19998| AUC [0.520811361570302, 0]]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.788699 | Logloss: 0.457439\n",
      "Evaluate Time 0.07215061666666665 minutes\n",
      "[Epoch    5] train MF loss: 0.46400607, valid loss: 0inf, time 4.85076915 minutes\n",
      "********************************************************************************\n",
      "[Epoch 5|Step 280|Flag 0|Sparsity 0.9958|Params 19998| AUC [0.7886992581706898, 5]]\n",
      "[Epoch 5|Step 1280|Flag 0|Sparsity 0.9958|Params 19998| AUC [0.7886992581706898, 5]]\n",
      "[Epoch 5|Step 2280|Flag 0|Sparsity 0.9958|Params 19998| AUC [0.7886992581706898, 5]]\n",
      "[Epoch 6|Step 936|Flag 0|Sparsity 0.9958|Params 19998| AUC [0.7886992581706898, 5]]\n",
      "[Epoch 6|Step 1936|Flag 0|Sparsity 0.9958|Params 19998| AUC [0.7886992581706898, 5]]\n",
      "[Epoch 7|Step 592|Flag 0|Sparsity 0.9958|Params 19998| AUC [0.7886992581706898, 5]]\n",
      "[Epoch 7|Step 1592|Flag 0|Sparsity 0.9958|Params 19998| AUC [0.7886992581706898, 5]]\n",
      "[Epoch 8|Step 248|Flag 0|Sparsity 0.9958|Params 19998| AUC [0.7886992581706898, 5]]\n",
      "[Epoch 8|Step 1248|Flag 0|Sparsity 0.9958|Params 19998| AUC [0.7886992581706898, 5]]\n",
      "[Epoch 8|Step 2248|Flag 0|Sparsity 0.9958|Params 19998| AUC [0.7886992581706898, 5]]\n",
      "[Epoch 9|Step 904|Flag 0|Sparsity 0.9958|Params 19998| AUC [0.7886992581706898, 5]]\n",
      "[Epoch 9|Step 1904|Flag 0|Sparsity 0.9958|Params 19998| AUC [0.7886992581706898, 5]]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.793214 | Logloss: 0.453485\n",
      "Evaluate Time 0.09507651666666667 minutes\n",
      "[Epoch   10] train MF loss: 0.44024649, valid loss: 0inf, time 6.22966760 minutes\n",
      "********************************************************************************\n",
      "[Epoch 10|Step 560|Flag 0|Sparsity 0.9958|Params 19998| AUC [0.7932140198301028, 10]]\n",
      "[Epoch 10|Step 1560|Flag 0|Sparsity 0.9958|Params 19998| AUC [0.7932140198301028, 10]]\n",
      "[Epoch 11|Step 216|Flag 0|Sparsity 0.9958|Params 19998| AUC [0.7932140198301028, 10]]\n",
      "[Epoch 11|Step 1216|Flag 0|Sparsity 0.9958|Params 19998| AUC [0.7932140198301028, 10]]\n",
      "[Epoch 11|Step 2216|Flag 0|Sparsity 0.9958|Params 19998| AUC [0.7932140198301028, 10]]\n",
      "[Epoch 12|Step 872|Flag 0|Sparsity 0.9958|Params 19998| AUC [0.7932140198301028, 10]]\n",
      "[Epoch 12|Step 1872|Flag 0|Sparsity 0.9958|Params 19998| AUC [0.7932140198301028, 10]]\n",
      "[Epoch 13|Step 528|Flag 0|Sparsity 0.9958|Params 19998| AUC [0.7932140198301028, 10]]\n",
      "[Epoch 13|Step 1528|Flag 0|Sparsity 0.9958|Params 19998| AUC [0.7932140198301028, 10]]\n",
      "[Epoch 14|Step 184|Flag 0|Sparsity 0.9958|Params 19998| AUC [0.7932140198301028, 10]]\n",
      "[Epoch 14|Step 1184|Flag 0|Sparsity 0.9958|Params 19998| AUC [0.7932140198301028, 10]]\n",
      "[Epoch 14|Step 2184|Flag 0|Sparsity 0.9958|Params 19998| AUC [0.7932140198301028, 10]]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.793067 | Logloss: 0.452259\n",
      "Evaluate Time 0.08677073333333334 minutes\n",
      "[Epoch   15] train MF loss: 0.42215526, valid loss: 0inf, time 6.67146050 minutes\n",
      "********************************************************************************\n",
      "[Epoch 15|Step 840|Flag 1|Sparsity 0.9958|Params 19998| AUC [0.7932140198301028, 10]]\n",
      "[Epoch 15|Step 1840|Flag 1|Sparsity 0.9958|Params 19998| AUC [0.7932140198301028, 10]]\n",
      "retrain for target parameters 29984\n",
      "DEEPFM_train_BaseDim32_bsz1024_lr_0.001_optim_adam_thresholdTypeFEATURE_DIM_thres_init-15_sigmoid-1_l2_penalty0\n",
      "0\n",
      "\tNum of train records: 2400000\n",
      "\tNum of valid records: 300000\n",
      "\tNum of test records: 300000\n",
      "\tNum of fields: 39\n",
      "\tNum of features: 150032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pytorchEnv\\env\\Lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrain epoch 29984\n",
      "BackBone Embedding Parameters:  4801024\n",
      "--------------------------------------------------------------------------------\n",
      "[complete episode  starts!]\n",
      "Initializing ...\n",
      "Retrain epoch 29984\n",
      "BackBone Embedding Parameters:  4801024\n",
      "[Epoch 0|Step 0|Flag 0|Sparsity 0.9938|Params 29984| AUC [0, 0]]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.520813 | Logloss: 2.337095\n",
      "Evaluate Time 0.14400300000000002 minutes\n",
      "[Epoch    0] train MF loss: 2.25208998, valid loss: 0inf, time 0.15328797 minutes\n",
      "********************************************************************************\n",
      "[Epoch 0|Step 1000|Flag 0|Sparsity 0.9938|Params 29984| AUC [0.520813484224496, 0]]\n",
      "[Epoch 0|Step 2000|Flag 0|Sparsity 0.9938|Params 29984| AUC [0.520813484224496, 0]]\n",
      "[Epoch 1|Step 656|Flag 0|Sparsity 0.9938|Params 29984| AUC [0.520813484224496, 0]]\n",
      "[Epoch 1|Step 1656|Flag 0|Sparsity 0.9938|Params 29984| AUC [0.520813484224496, 0]]\n",
      "[Epoch 2|Step 312|Flag 0|Sparsity 0.9938|Params 29984| AUC [0.520813484224496, 0]]\n",
      "[Epoch 2|Step 1312|Flag 0|Sparsity 0.9938|Params 29984| AUC [0.520813484224496, 0]]\n",
      "[Epoch 2|Step 2312|Flag 0|Sparsity 0.9938|Params 29984| AUC [0.520813484224496, 0]]\n",
      "[Epoch 3|Step 968|Flag 0|Sparsity 0.9938|Params 29984| AUC [0.520813484224496, 0]]\n",
      "[Epoch 3|Step 1968|Flag 0|Sparsity 0.9938|Params 29984| AUC [0.520813484224496, 0]]\n",
      "[Epoch 4|Step 624|Flag 0|Sparsity 0.9938|Params 29984| AUC [0.520813484224496, 0]]\n",
      "[Epoch 4|Step 1624|Flag 0|Sparsity 0.9938|Params 29984| AUC [0.520813484224496, 0]]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.788571 | Logloss: 0.457510\n",
      "Evaluate Time 0.08696413333333333 minutes\n",
      "[Epoch    5] train MF loss: 0.46043128, valid loss: 0inf, time 6.59999232 minutes\n",
      "********************************************************************************\n",
      "[Epoch 5|Step 280|Flag 0|Sparsity 0.9938|Params 29984| AUC [0.78857121749222, 5]]\n",
      "[Epoch 5|Step 1280|Flag 0|Sparsity 0.9938|Params 29984| AUC [0.78857121749222, 5]]\n",
      "[Epoch 5|Step 2280|Flag 0|Sparsity 0.9938|Params 29984| AUC [0.78857121749222, 5]]\n",
      "[Epoch 6|Step 936|Flag 0|Sparsity 0.9938|Params 29984| AUC [0.78857121749222, 5]]\n",
      "[Epoch 6|Step 1936|Flag 0|Sparsity 0.9938|Params 29984| AUC [0.78857121749222, 5]]\n",
      "[Epoch 7|Step 592|Flag 0|Sparsity 0.9938|Params 29984| AUC [0.78857121749222, 5]]\n",
      "[Epoch 7|Step 1592|Flag 0|Sparsity 0.9938|Params 29984| AUC [0.78857121749222, 5]]\n",
      "[Epoch 8|Step 248|Flag 0|Sparsity 0.9938|Params 29984| AUC [0.78857121749222, 5]]\n",
      "[Epoch 8|Step 1248|Flag 0|Sparsity 0.9938|Params 29984| AUC [0.78857121749222, 5]]\n",
      "[Epoch 8|Step 2248|Flag 0|Sparsity 0.9938|Params 29984| AUC [0.78857121749222, 5]]\n",
      "[Epoch 9|Step 904|Flag 0|Sparsity 0.9938|Params 29984| AUC [0.78857121749222, 5]]\n",
      "[Epoch 9|Step 1904|Flag 0|Sparsity 0.9938|Params 29984| AUC [0.78857121749222, 5]]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.792587 | Logloss: 0.454452\n",
      "Evaluate Time 0.09106133333333334 minutes\n",
      "[Epoch   10] train MF loss: 0.43458354, valid loss: 0inf, time 6.27435032 minutes\n",
      "********************************************************************************\n",
      "[Epoch 10|Step 560|Flag 0|Sparsity 0.9938|Params 29984| AUC [0.7925868174875411, 10]]\n",
      "[Epoch 10|Step 1560|Flag 0|Sparsity 0.9938|Params 29984| AUC [0.7925868174875411, 10]]\n",
      "[Epoch 11|Step 216|Flag 0|Sparsity 0.9938|Params 29984| AUC [0.7925868174875411, 10]]\n",
      "[Epoch 11|Step 1216|Flag 0|Sparsity 0.9938|Params 29984| AUC [0.7925868174875411, 10]]\n",
      "[Epoch 11|Step 2216|Flag 0|Sparsity 0.9938|Params 29984| AUC [0.7925868174875411, 10]]\n",
      "[Epoch 12|Step 872|Flag 0|Sparsity 0.9938|Params 29984| AUC [0.7925868174875411, 10]]\n",
      "[Epoch 12|Step 1872|Flag 0|Sparsity 0.9938|Params 29984| AUC [0.7925868174875411, 10]]\n",
      "[Epoch 13|Step 528|Flag 0|Sparsity 0.9938|Params 29984| AUC [0.7925868174875411, 10]]\n",
      "[Epoch 13|Step 1528|Flag 0|Sparsity 0.9938|Params 29984| AUC [0.7925868174875411, 10]]\n",
      "[Epoch 14|Step 184|Flag 0|Sparsity 0.9938|Params 29984| AUC [0.7925868174875411, 10]]\n",
      "[Epoch 14|Step 1184|Flag 0|Sparsity 0.9938|Params 29984| AUC [0.7925868174875411, 10]]\n",
      "[Epoch 14|Step 2184|Flag 0|Sparsity 0.9938|Params 29984| AUC [0.7925868174875411, 10]]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.792177 | Logloss: 0.453229\n",
      "Evaluate Time 0.08811313333333333 minutes\n",
      "[Epoch   15] train MF loss: 0.41954151, valid loss: 0inf, time 6.34045737 minutes\n",
      "********************************************************************************\n",
      "[Epoch 15|Step 840|Flag 1|Sparsity 0.9938|Params 29984| AUC [0.7925868174875411, 10]]\n",
      "[Epoch 15|Step 1840|Flag 1|Sparsity 0.9938|Params 29984| AUC [0.7925868174875411, 10]]\n",
      "retrain for target parameters 49971\n",
      "DEEPFM_train_BaseDim32_bsz1024_lr_0.001_optim_adam_thresholdTypeFEATURE_DIM_thres_init-15_sigmoid-1_l2_penalty0\n",
      "0\n",
      "\tNum of train records: 2400000\n",
      "\tNum of valid records: 300000\n",
      "\tNum of test records: 300000\n",
      "\tNum of fields: 39\n",
      "\tNum of features: 150032\n",
      "Retrain epoch 49971\n",
      "BackBone Embedding Parameters:  4801024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pytorchEnv\\env\\Lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "[complete episode  starts!]\n",
      "Initializing ...\n",
      "Retrain epoch 49971\n",
      "BackBone Embedding Parameters:  4801024\n",
      "[Epoch 0|Step 0|Flag 0|Sparsity 0.9896|Params 49971| AUC [0, 0]]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.520813 | Logloss: 2.337081\n",
      "Evaluate Time 0.09948191666666667 minutes\n",
      "[Epoch    0] train MF loss: 2.24767923, valid loss: 0inf, time 0.10882253 minutes\n",
      "********************************************************************************\n",
      "[Epoch 0|Step 1000|Flag 0|Sparsity 0.9896|Params 49971| AUC [0.5208131448951006, 0]]\n",
      "[Epoch 0|Step 2000|Flag 0|Sparsity 0.9896|Params 49971| AUC [0.5208131448951006, 0]]\n",
      "[Epoch 1|Step 656|Flag 0|Sparsity 0.9896|Params 49971| AUC [0.5208131448951006, 0]]\n",
      "[Epoch 1|Step 1656|Flag 0|Sparsity 0.9896|Params 49971| AUC [0.5208131448951006, 0]]\n",
      "[Epoch 2|Step 312|Flag 0|Sparsity 0.9896|Params 49971| AUC [0.5208131448951006, 0]]\n",
      "[Epoch 2|Step 1312|Flag 0|Sparsity 0.9896|Params 49971| AUC [0.5208131448951006, 0]]\n",
      "[Epoch 2|Step 2312|Flag 0|Sparsity 0.9896|Params 49971| AUC [0.5208131448951006, 0]]\n",
      "[Epoch 3|Step 968|Flag 0|Sparsity 0.9896|Params 49971| AUC [0.5208131448951006, 0]]\n",
      "[Epoch 3|Step 1968|Flag 0|Sparsity 0.9896|Params 49971| AUC [0.5208131448951006, 0]]\n",
      "[Epoch 4|Step 624|Flag 0|Sparsity 0.9896|Params 49971| AUC [0.5208131448951006, 0]]\n",
      "[Epoch 4|Step 1624|Flag 0|Sparsity 0.9896|Params 49971| AUC [0.5208131448951006, 0]]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.788610 | Logloss: 0.457417\n",
      "Evaluate Time 0.08810668333333332 minutes\n",
      "[Epoch    5] train MF loss: 0.45643806, valid loss: 0inf, time 6.45466728 minutes\n",
      "********************************************************************************\n",
      "[Epoch 5|Step 280|Flag 0|Sparsity 0.9896|Params 49971| AUC [0.7886104765764013, 5]]\n",
      "[Epoch 5|Step 1280|Flag 0|Sparsity 0.9896|Params 49971| AUC [0.7886104765764013, 5]]\n",
      "[Epoch 5|Step 2280|Flag 0|Sparsity 0.9896|Params 49971| AUC [0.7886104765764013, 5]]\n",
      "[Epoch 6|Step 936|Flag 0|Sparsity 0.9896|Params 49971| AUC [0.7886104765764013, 5]]\n",
      "[Epoch 6|Step 1936|Flag 0|Sparsity 0.9896|Params 49971| AUC [0.7886104765764013, 5]]\n",
      "[Epoch 7|Step 592|Flag 0|Sparsity 0.9896|Params 49971| AUC [0.7886104765764013, 5]]\n",
      "[Epoch 7|Step 1592|Flag 0|Sparsity 0.9896|Params 49971| AUC [0.7886104765764013, 5]]\n",
      "[Epoch 8|Step 248|Flag 0|Sparsity 0.9896|Params 49971| AUC [0.7886104765764013, 5]]\n",
      "[Epoch 8|Step 1248|Flag 0|Sparsity 0.9896|Params 49971| AUC [0.7886104765764013, 5]]\n",
      "[Epoch 8|Step 2248|Flag 0|Sparsity 0.9896|Params 49971| AUC [0.7886104765764013, 5]]\n",
      "[Epoch 9|Step 904|Flag 0|Sparsity 0.9896|Params 49971| AUC [0.7886104765764013, 5]]\n",
      "[Epoch 9|Step 1904|Flag 0|Sparsity 0.9896|Params 49971| AUC [0.7886104765764013, 5]]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.791259 | Logloss: 0.456231\n",
      "Evaluate Time 0.0951743 minutes\n",
      "[Epoch   10] train MF loss: 0.43351987, valid loss: 0inf, time 6.54528688 minutes\n",
      "********************************************************************************\n",
      "[Epoch 10|Step 560|Flag 0|Sparsity 0.9896|Params 49971| AUC [0.791259010643252, 10]]\n",
      "[Epoch 10|Step 1560|Flag 0|Sparsity 0.9896|Params 49971| AUC [0.791259010643252, 10]]\n",
      "[Epoch 11|Step 216|Flag 0|Sparsity 0.9896|Params 49971| AUC [0.791259010643252, 10]]\n",
      "[Epoch 11|Step 1216|Flag 0|Sparsity 0.9896|Params 49971| AUC [0.791259010643252, 10]]\n",
      "[Epoch 11|Step 2216|Flag 0|Sparsity 0.9896|Params 49971| AUC [0.791259010643252, 10]]\n",
      "[Epoch 12|Step 872|Flag 0|Sparsity 0.9896|Params 49971| AUC [0.791259010643252, 10]]\n",
      "[Epoch 12|Step 1872|Flag 0|Sparsity 0.9896|Params 49971| AUC [0.791259010643252, 10]]\n",
      "[Epoch 13|Step 528|Flag 0|Sparsity 0.9896|Params 49971| AUC [0.791259010643252, 10]]\n",
      "[Epoch 13|Step 1528|Flag 0|Sparsity 0.9896|Params 49971| AUC [0.791259010643252, 10]]\n",
      "[Epoch 14|Step 184|Flag 0|Sparsity 0.9896|Params 49971| AUC [0.791259010643252, 10]]\n",
      "[Epoch 14|Step 1184|Flag 0|Sparsity 0.9896|Params 49971| AUC [0.791259010643252, 10]]\n",
      "[Epoch 14|Step 2184|Flag 0|Sparsity 0.9896|Params 49971| AUC [0.791259010643252, 10]]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.790461 | Logloss: 0.455189\n",
      "Evaluate Time 0.08889698333333333 minutes\n",
      "[Epoch   15] train MF loss: 0.41094154, valid loss: 0inf, time 6.63906932 minutes\n",
      "********************************************************************************\n",
      "[Epoch 15|Step 840|Flag 1|Sparsity 0.9896|Params 49971| AUC [0.791259010643252, 10]]\n",
      "[Epoch 15|Step 1840|Flag 1|Sparsity 0.9896|Params 49971| AUC [0.791259010643252, 10]]\n"
     ]
    }
   ],
   "source": [
    "target_parameters = [29989,49968]\n",
    "best_test_results_list = []\n",
    "for target_param in target_parameters:\n",
    "\n",
    "    print(f\"retrain for target parameters {target_param}\")\n",
    "    parser = setup_args()\n",
    "    parser.set_defaults(\n",
    "        alias='train',\n",
    "        tensorboard='./tmp/runs/{factorizer}/{data_type}',\n",
    "        ##########\n",
    "        ## data ##\n",
    "        ##########\n",
    "        data_type='avazu',\n",
    "        data_path='./data/{data_type}/',\n",
    "        load_in_queue=False,\n",
    "        category_only=False,\n",
    "        rebuild_cache=False,\n",
    "        eval_res_path='./tmp/res/{factorizer}/{data_type}/{alias}/{epoch_idx}.csv',\n",
    "        emb_save_path='./tmp/embedding/{factorizer}/{data_type}/{alias}/{num_parameter}',\n",
    "        ######################\n",
    "        ## train/test split ##\n",
    "        ######################\n",
    "        test_ratio=0.1,\n",
    "        valid_ratio=1/9,\n",
    "        ##########################\n",
    "        ## Devices & Efficiency ##\n",
    "        ##########################\n",
    "        use_cuda=True,\n",
    "        early_stop=40,\n",
    "        log_interval=1,\n",
    "        display_interval=1000,\n",
    "        eval_interval=5,  # 10 epochs between 2 evaluations\n",
    "        device_ids_test=[0],\n",
    "        device_id=0,\n",
    "        batch_size_train=1024,\n",
    "        batch_size_valid=1024,\n",
    "        batch_size_test=1024,\n",
    "        ###########\n",
    "        ## Model ##\n",
    "        ###########\n",
    "        factorizer='fm',\n",
    "        model='deepfm',\n",
    "        fm_lr=1e-3,\n",
    "        # Deep\n",
    "        mlp_dims=[100, 100],\n",
    "        # AutoInt\n",
    "        has_residual=True,\n",
    "        full_part=True,\n",
    "        num_heads=2,\n",
    "        num_layers=3,\n",
    "        att_dropout=0.4,\n",
    "        atten_embed_dim=64,\n",
    "        # optimizer setting\n",
    "        fm_optimizer='adam',\n",
    "        fm_amsgrad=False,\n",
    "        fm_eps=1e-8,\n",
    "        fm_l2_regularization=1e-5,\n",
    "        fm_betas=(0.9, 0.999),\n",
    "        fm_grad_clip=100,  # 0.1\n",
    "        fm_lr_exp_decay=1,\n",
    "        l2_penalty=0,\n",
    "        #########\n",
    "        ## PEP ##\n",
    "        #########\n",
    "        latent_dim=32,\n",
    "        threshold_type='feature_dim',\n",
    "        g_type='sigmoid',\n",
    "        gk=1,\n",
    "        threshold_init=-15,\n",
    "        retrain_emb_param=target_param,\n",
    "        re_init=False,\n",
    "    )\n",
    "\n",
    "    opt = parser.parse_args(args=[])\n",
    "    opt = vars(opt)\n",
    "    opt['alias'] = '{}_{}_BaseDim{}_bsz{}_lr_{}_optim_{}_thresholdType{}_thres_init{}_{}-{}_l2_penalty{}'.format(\n",
    "        opt['model'].upper(),\n",
    "        opt['alias'],\n",
    "        opt['latent_dim'],\n",
    "        opt['batch_size_train'],\n",
    "        opt['fm_lr'],\n",
    "        opt['fm_optimizer'],\n",
    "        opt['threshold_type'].upper(),\n",
    "        opt['threshold_init'],\n",
    "        opt['g_type'],\n",
    "        opt['gk'],\n",
    "        opt['l2_penalty']\n",
    "    )\n",
    "    print(opt['alias'])\n",
    "    random.seed(opt['seed'])\n",
    "    # np.random.seed(opt['seed'])\n",
    "    torch.manual_seed(opt['seed'])\n",
    "    torch.cuda.manual_seed_all(opt['seed'])\n",
    "    engine = Engine(opt)\n",
    "    best_result = engine.train()\n",
    "    best_test_results_list.append(best_result)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avazu_Deepfm_PEP_retrain = {}\n",
    "for i, param in enumerate(target_parameters):\n",
    "    avazu_Deepfm_PEP_retrain[param] =  best_test_results_list[i]\n",
    "\n",
    "pd.DataFrame(avazu_Deepfm_PEP_retrain).to_csv(\"avazu_Deepfm_PEP_retrain.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train For AutoInt, avazu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUTOINT_train_BaseDim32_bsz1024_lr_0.001_optim_adam_thresholdTypeFEATURE_DIM_thres_init-15_sigmoid-1_l2_penalty0\n",
      "\tNum of train records: 32343173\n",
      "\tNum of valid records: 4042896\n",
      "\tNum of test records: 4042898\n",
      "\tNum of fields: 22\n",
      "\tNum of features: 645394\n",
      "BackBone Embedding Parameters:  20652608\n",
      "--------------------------------------------------------------------------------\n",
      "[complete episode  starts!]\n",
      "Initializing ...\n",
      "BackBone Embedding Parameters:  20652608\n",
      "********************************************************************************\n",
      "Save the initial embedding table\n",
      "********************************************************************************\n",
      "[Epoch 0|Step 0|Flag 0|Sparsity 0.0001|Params 20650472| Best AUC 0]\n",
      "[Epoch 0|Step 1000|Flag 0|Sparsity 0.8931|Params 2208227| Best AUC 0]\n",
      "[Epoch 0|Step 2000|Flag 0|Sparsity 0.9038|Params 1986537| Best AUC 0]\n",
      "[Epoch 0|Step 3000|Flag 0|Sparsity 0.9156|Params 1743907| Best AUC 0]\n",
      "[Epoch 0|Step 4000|Flag 0|Sparsity 0.9267|Params 1514735| Best AUC 0]\n",
      "[Epoch 0|Step 5000|Flag 0|Sparsity 0.9364|Params 1313027| Best AUC 0]\n",
      "[Epoch 0|Step 6000|Flag 0|Sparsity 0.9453|Params 1128899| Best AUC 0]\n",
      "[Epoch 0|Step 7000|Flag 0|Sparsity 0.9545|Params 939608| Best AUC 0]\n",
      "[Epoch 0|Step 8000|Flag 0|Sparsity 0.9650|Params 723750| Best AUC 0]\n",
      "[Epoch 0|Step 9000|Flag 0|Sparsity 0.9768|Params 478831| Best AUC 0]\n",
      "[Epoch 0|Step 10000|Flag 0|Sparsity 0.9868|Params 272348| Best AUC 0]\n",
      "[Epoch 0|Step 11000|Flag 0|Sparsity 0.9929|Params 146347| Best AUC 0]\n",
      "[Epoch 0|Step 12000|Flag 0|Sparsity 0.9959|Params 83833| Best AUC 0]\n",
      "********************************************************************************\n",
      "Reach the target parameter: 50000, save embedding with size: 49982\n",
      "********************************************************************************\n",
      "[Epoch 0|Step 13000|Flag 0|Sparsity 0.9976|Params 48574| Best AUC 0]\n",
      "********************************************************************************\n",
      "Reach the target parameter: 30000, save embedding with size: 29988\n",
      "********************************************************************************\n",
      "Minimal target parameters achieved, stop pruning.\n",
      "********** {'AUC': [0, 0], 'LogLoss': [inf, 0]} **********\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "parser = setup_args()\n",
    "parser.set_defaults(\n",
    "    alias='train',\n",
    "    tensorboard='./tmp/runs/{factorizer}/{data_type}',\n",
    "    ##########\n",
    "    ## data ##\n",
    "    ##########\n",
    "    data_type='avazu',\n",
    "    data_path='./data/{data_type}/',\n",
    "    load_in_queue=False,\n",
    "    category_only=False,\n",
    "    rebuild_cache=False,\n",
    "    eval_res_path='./tmp/res/{factorizer}/{data_type}/{alias}/{epoch_idx}.csv',\n",
    "    emb_save_path='./tmp/embedding/{factorizer}/{data_type}/{alias}/{num_parameter}',\n",
    "    ######################\n",
    "    ## train/test split ##\n",
    "    ######################\n",
    "    test_ratio=0.1,\n",
    "    valid_ratio=1/9,\n",
    "    ##########################\n",
    "    ## Devices & Efficiency ##\n",
    "    ##########################\n",
    "    use_cuda=True,\n",
    "    early_stop=10,\n",
    "    log_interval=1,\n",
    "    display_interval=1000,\n",
    "    eval_interval=5,  # 10 epochs between 2 evaluations\n",
    "    device_ids_test=[0],\n",
    "    device_id=0,\n",
    "    batch_size_train=1024,\n",
    "    batch_size_valid=1024,\n",
    "    batch_size_test=1024,\n",
    "    ###########\n",
    "    ## Model ##\n",
    "    ###########\n",
    "    factorizer='fm',\n",
    "    model='autoint',\n",
    "    fm_lr=1e-3,\n",
    "    # Deep\n",
    "    mlp_dims=[100, 100],\n",
    "    # AutoInt\n",
    "    has_residual=True,\n",
    "    full_part=True,\n",
    "    num_heads=2,\n",
    "    num_layers=3,\n",
    "    att_dropout=0.4,\n",
    "    atten_embed_dim=64,\n",
    "    # optimizer setting\n",
    "    fm_optimizer='adam',\n",
    "    fm_amsgrad=False,\n",
    "    fm_eps=1e-8,\n",
    "    fm_l2_regularization=1e-5,\n",
    "    fm_betas=(0.9, 0.999),\n",
    "    fm_grad_clip=100,  # 0.1\n",
    "    fm_lr_exp_decay=1,\n",
    "    l2_penalty=0,\n",
    "    #########\n",
    "    ## Embeddings//PEP ##\n",
    "    #########\n",
    "    latent_dim=32,\n",
    "    threshold_type='feature_dim',\n",
    "    g_type='sigmoid',\n",
    "    gk=1,\n",
    "    threshold_init=-15,\n",
    "    candidate_p=[50000, 30000],\n",
    ")\n",
    "\n",
    "opt = parser.parse_args(args=[])\n",
    "opt = vars(opt)\n",
    "\n",
    "# rename alias\n",
    "\n",
    "opt['alias'] = '{}_{}_BaseDim{}_bsz{}_lr_{}_optim_{}_thresholdType{}_thres_init{}_{}-{}_l2_penalty{}'.format(\n",
    "    opt['model'].upper(),\n",
    "    opt['alias'],\n",
    "    opt['latent_dim'],\n",
    "    opt['batch_size_train'],\n",
    "    opt['fm_lr'],\n",
    "    opt['fm_optimizer'],\n",
    "    opt['threshold_type'].upper(),\n",
    "    opt['threshold_init'],\n",
    "    opt['g_type'],\n",
    "    opt['gk'],\n",
    "    opt['l2_penalty']\n",
    ")\n",
    "print(opt['alias'])\n",
    "random.seed(opt['seed'])\n",
    "# np.random.seed(opt['seed'])\n",
    "torch.manual_seed(opt['seed'])\n",
    "torch.cuda.manual_seed_all(opt['seed'])\n",
    "engine = Engine(opt)\n",
    "best_result = engine.train()\n",
    "print(\"*\"*10 , best_result, \"*\"*10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrain AutoInt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrain for target parameters 19992\n",
      "AUTOINT_train_BaseDim32_bsz1024_lr_0.001_optim_adam_thresholdTypeFEATURE_DIM_thres_init-15_sigmoid-1_l2_penalty0\n",
      "0\n",
      "\tNum of train records: 2400000\n",
      "\tNum of valid records: 300000\n",
      "\tNum of test records: 300000\n",
      "\tNum of fields: 39\n",
      "\tNum of features: 150032\n",
      "Retrain epoch 19992\n",
      "BackBone Embedding Parameters:  4801024\n",
      "--------------------------------------------------------------------------------\n",
      "[complete episode  starts!]\n",
      "Initializing ...\n",
      "Retrain epoch 19992\n",
      "BackBone Embedding Parameters:  4801024\n",
      "[Epoch 0|Step 0|Flag 0|Sparsity 0.9958|Params 19992| Best AUC 0]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.586419 | Logloss: 0.644200\n",
      "Evaluate Time 0.09862778333333333 minutes\n",
      "[Epoch    0] train MF loss: 0.63114500, valid loss: 0inf, time 0.10800515 minutes\n",
      "********************************************************************************\n",
      "[Epoch 0|Step 1000|Flag 0|Sparsity 0.9958|Params 19992| Best AUC 0.5864]\n",
      "[Epoch 0|Step 2000|Flag 0|Sparsity 0.9958|Params 19992| Best AUC 0.5864]\n",
      "[Epoch 1|Step 656|Flag 0|Sparsity 0.9958|Params 19992| Best AUC 0.5864]\n",
      "[Epoch 1|Step 1656|Flag 0|Sparsity 0.9958|Params 19992| Best AUC 0.5864]\n",
      "[Epoch 2|Step 312|Flag 0|Sparsity 0.9958|Params 19992| Best AUC 0.5864]\n",
      "[Epoch 2|Step 1312|Flag 0|Sparsity 0.9958|Params 19992| Best AUC 0.5864]\n",
      "[Epoch 2|Step 2312|Flag 0|Sparsity 0.9958|Params 19992| Best AUC 0.5864]\n",
      "[Epoch 3|Step 968|Flag 0|Sparsity 0.9958|Params 19992| Best AUC 0.5864]\n",
      "[Epoch 3|Step 1968|Flag 0|Sparsity 0.9958|Params 19992| Best AUC 0.5864]\n",
      "[Epoch 4|Step 624|Flag 0|Sparsity 0.9958|Params 19992| Best AUC 0.5864]\n",
      "[Epoch 4|Step 1624|Flag 0|Sparsity 0.9958|Params 19992| Best AUC 0.5864]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.793063 | Logloss: 0.451930\n",
      "Evaluate Time 0.11090801666666666 minutes\n",
      "[Epoch    5] train MF loss: 0.45013219, valid loss: 0inf, time 9.13125275 minutes\n",
      "********************************************************************************\n",
      "[Epoch 5|Step 280|Flag 0|Sparsity 0.9958|Params 19992| Best AUC 0.7931]\n",
      "[Epoch 5|Step 1280|Flag 0|Sparsity 0.9958|Params 19992| Best AUC 0.7931]\n",
      "[Epoch 5|Step 2280|Flag 0|Sparsity 0.9958|Params 19992| Best AUC 0.7931]\n",
      "[Epoch 6|Step 936|Flag 0|Sparsity 0.9958|Params 19992| Best AUC 0.7931]\n",
      "[Epoch 6|Step 1936|Flag 0|Sparsity 0.9958|Params 19992| Best AUC 0.7931]\n",
      "[Epoch 7|Step 592|Flag 0|Sparsity 0.9958|Params 19992| Best AUC 0.7931]\n",
      "[Epoch 7|Step 1592|Flag 0|Sparsity 0.9958|Params 19992| Best AUC 0.7931]\n",
      "[Epoch 8|Step 248|Flag 0|Sparsity 0.9958|Params 19992| Best AUC 0.7931]\n",
      "[Epoch 8|Step 1248|Flag 0|Sparsity 0.9958|Params 19992| Best AUC 0.7931]\n",
      "[Epoch 8|Step 2248|Flag 0|Sparsity 0.9958|Params 19992| Best AUC 0.7931]\n",
      "[Epoch 9|Step 904|Flag 0|Sparsity 0.9958|Params 19992| Best AUC 0.7931]\n",
      "[Epoch 9|Step 1904|Flag 0|Sparsity 0.9958|Params 19992| Best AUC 0.7931]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.793561 | Logloss: 0.451901\n",
      "Evaluate Time 0.10997921666666667 minutes\n",
      "[Epoch   10] train MF loss: 0.41635841, valid loss: 0inf, time 9.33028773 minutes\n",
      "********************************************************************************\n",
      "[Epoch 10|Step 560|Flag 0|Sparsity 0.9958|Params 19992| Best AUC 0.7936]\n",
      "[Epoch 10|Step 1560|Flag 0|Sparsity 0.9958|Params 19992| Best AUC 0.7936]\n",
      "[Epoch 11|Step 216|Flag 0|Sparsity 0.9958|Params 19992| Best AUC 0.7936]\n",
      "[Epoch 11|Step 1216|Flag 0|Sparsity 0.9958|Params 19992| Best AUC 0.7936]\n",
      "[Epoch 11|Step 2216|Flag 0|Sparsity 0.9958|Params 19992| Best AUC 0.7936]\n",
      "[Epoch 12|Step 872|Flag 0|Sparsity 0.9958|Params 19992| Best AUC 0.7936]\n",
      "[Epoch 12|Step 1872|Flag 0|Sparsity 0.9958|Params 19992| Best AUC 0.7936]\n",
      "[Epoch 13|Step 528|Flag 0|Sparsity 0.9958|Params 19992| Best AUC 0.7936]\n",
      "[Epoch 13|Step 1528|Flag 0|Sparsity 0.9958|Params 19992| Best AUC 0.7936]\n",
      "[Epoch 14|Step 184|Flag 0|Sparsity 0.9958|Params 19992| Best AUC 0.7936]\n",
      "[Epoch 14|Step 1184|Flag 0|Sparsity 0.9958|Params 19992| Best AUC 0.7936]\n",
      "[Epoch 14|Step 2184|Flag 0|Sparsity 0.9958|Params 19992| Best AUC 0.7936]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.792921 | Logloss: 0.452663\n",
      "Evaluate Time 0.11467105000000001 minutes\n",
      "[Epoch   15] train MF loss: 0.43298090, valid loss: 0inf, time 9.41467878 minutes\n",
      "********************************************************************************\n",
      "[Epoch 15|Step 840|Flag 1|Sparsity 0.9958|Params 19992| Best AUC 0.7936]\n",
      "[Epoch 15|Step 1840|Flag 1|Sparsity 0.9958|Params 19992| Best AUC 0.7936]\n",
      "retrain for target parameters 29984\n",
      "AUTOINT_train_BaseDim32_bsz1024_lr_0.001_optim_adam_thresholdTypeFEATURE_DIM_thres_init-15_sigmoid-1_l2_penalty0\n",
      "0\n",
      "\tNum of train records: 2400000\n",
      "\tNum of valid records: 300000\n",
      "\tNum of test records: 300000\n",
      "\tNum of fields: 39\n",
      "\tNum of features: 150032\n",
      "Retrain epoch 29984\n",
      "BackBone Embedding Parameters:  4801024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pytorchEnv\\env\\Lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "[complete episode  starts!]\n",
      "Initializing ...\n",
      "Retrain epoch 29984\n",
      "BackBone Embedding Parameters:  4801024\n",
      "[Epoch 0|Step 0|Flag 0|Sparsity 0.9938|Params 29984| Best AUC 0]\n",
      "Evaluate on test ...\n",
      "********************************************************************************\n",
      "Test AUC: 0.576816 | Logloss: 0.644201\n",
      "Evaluate Time 0.13949085 minutes\n",
      "[Epoch    0] train MF loss: 0.63404930, valid loss: 0inf, time 0.15089083 minutes\n",
      "********************************************************************************\n",
      "[Epoch 0|Step 1000|Flag 0|Sparsity 0.9938|Params 29984| Best AUC 0.5768]\n",
      "[Epoch 0|Step 2000|Flag 0|Sparsity 0.9938|Params 29984| Best AUC 0.5768]\n",
      "[Epoch 1|Step 656|Flag 0|Sparsity 0.9938|Params 29984| Best AUC 0.5768]\n",
      "[Epoch 1|Step 1656|Flag 0|Sparsity 0.9938|Params 29984| Best AUC 0.5768]\n",
      "[Epoch 2|Step 312|Flag 0|Sparsity 0.9938|Params 29984| Best AUC 0.5768]\n",
      "[Epoch 2|Step 1312|Flag 0|Sparsity 0.9938|Params 29984| Best AUC 0.5768]\n"
     ]
    }
   ],
   "source": [
    "target_parameters = [29988, 49982]\n",
    "best_test_results_list = []\n",
    "for target_param in target_parameters:\n",
    "\n",
    "    print(f\"retrain for target parameters {target_param}\")\n",
    "    parser = setup_args()\n",
    "    parser.set_defaults(\n",
    "        alias='train',\n",
    "        tensorboard='./tmp/runs/{factorizer}/{data_type}',\n",
    "        ##########\n",
    "        ## data ##\n",
    "        ##########\n",
    "        data_type='avazu',\n",
    "        data_path='./data/{data_type}/',\n",
    "        load_in_queue=False,\n",
    "        category_only=False,\n",
    "        rebuild_cache=False,\n",
    "        eval_res_path='./tmp/res/{factorizer}/{data_type}/{alias}/{epoch_idx}.csv',\n",
    "        emb_save_path='./tmp/embedding/{factorizer}/{data_type}/{alias}/{num_parameter}',\n",
    "        ######################\n",
    "        ## train/test split ##\n",
    "        ######################\n",
    "        test_ratio=0.1,\n",
    "        valid_ratio=1/9,\n",
    "        ##########################\n",
    "        ## Devices & Efficiency ##\n",
    "        ##########################\n",
    "        use_cuda=True,\n",
    "        early_stop=40,\n",
    "        log_interval=1,\n",
    "        display_interval=1000,\n",
    "        eval_interval=5,  # 10 epochs between 2 evaluations\n",
    "        device_ids_test=[0],\n",
    "        device_id=0,\n",
    "        batch_size_train=1024,\n",
    "        batch_size_valid=1024,\n",
    "        batch_size_test=1024,\n",
    "        ###########\n",
    "        ## Model ##\n",
    "        ###########\n",
    "        factorizer='fm',\n",
    "        model='autoint',\n",
    "        fm_lr=1e-3,\n",
    "        # Deep\n",
    "        mlp_dims=[100, 100],\n",
    "        # AutoInt\n",
    "        has_residual=True,\n",
    "        full_part=True,\n",
    "        num_heads=2,\n",
    "        num_layers=3,\n",
    "        att_dropout=0.4,\n",
    "        atten_embed_dim=64,\n",
    "        # optimizer setting\n",
    "        fm_optimizer='adam',\n",
    "        fm_amsgrad=False,\n",
    "        fm_eps=1e-8,\n",
    "        fm_l2_regularization=1e-5,\n",
    "        fm_betas=(0.9, 0.999),\n",
    "        fm_grad_clip=100,  # 0.1\n",
    "        fm_lr_exp_decay=1,\n",
    "        l2_penalty=0,\n",
    "        #########\n",
    "        ## PEP ##\n",
    "        #########\n",
    "        latent_dim=32,\n",
    "        threshold_type='feature_dim',\n",
    "        g_type='sigmoid',\n",
    "        gk=1,\n",
    "        threshold_init=-15,\n",
    "        retrain_emb_param=target_param,\n",
    "        re_init=False,\n",
    "    )\n",
    "\n",
    "    opt = parser.parse_args(args=[])\n",
    "    opt = vars(opt)\n",
    "    opt['alias'] = '{}_{}_BaseDim{}_bsz{}_lr_{}_optim_{}_thresholdType{}_thres_init{}_{}-{}_l2_penalty{}'.format(\n",
    "        opt['model'].upper(),\n",
    "        opt['alias'],\n",
    "        opt['latent_dim'],\n",
    "        opt['batch_size_train'],\n",
    "        opt['fm_lr'],\n",
    "        opt['fm_optimizer'],\n",
    "        opt['threshold_type'].upper(),\n",
    "        opt['threshold_init'],\n",
    "        opt['g_type'],\n",
    "        opt['gk'],\n",
    "        opt['l2_penalty']\n",
    "    )\n",
    "    print(opt['alias'])\n",
    "    random.seed(opt['seed'])\n",
    "    # np.random.seed(opt['seed'])\n",
    "    torch.manual_seed(opt['seed'])\n",
    "    torch.cuda.manual_seed_all(opt['seed'])\n",
    "    engine = Engine(opt)\n",
    "    best_result = engine.train()\n",
    "\n",
    "    best_test_results_list.append(best_result)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AutoInt_avazu_PEP_retrain = {}\n",
    "for i, param in enumerate(target_parameters):\n",
    "    AutoInt_avazu_PEP_retrain[param] =  best_test_results_list[i]\n",
    "\n",
    "pd.DataFrame(AutoInt_avazu_PEP_retrain).to_csv(\"AutoInt_avazu_PEP_retrain.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
